{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ed0a601e-dbad-4b37-bc1b-6562abacb46b",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-07-27'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - agents\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    "title: The Activeloop Deep Lake Vector Store for Agents & Large Language Models\n",
    "description: Activeloop Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps, and enables hybrid searches on these embeddings and their attributes for efficient data retrieval and integrates with LangChain and Agents \n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake4.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c2b08-47a3-4a00-98ed-c0609993d211",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Activeloop Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps . It enables hybrid searches on these embeddings and their attributes for efficient data retrieval. It also integrates with LangChain & Agents, facilitating the development and deployment of applications.\n",
    "\n",
    "## Deeplake v Other Vector Stores\n",
    "\n",
    "Deep Lake provides several advantages over the typical vector store:\n",
    "\n",
    "- It’s multimodal, which means that it can be used to store items of diverse modalities, such as texts, images, audio, and video, along with their vector representations. \n",
    "- It’s serverless, which means that we can create and manage cloud datasets without creating and managing a database instance. This aspect gives a great speedup to new projects.\n",
    "- Last, it’s possible to easily create a data loader out of the data loaded into a Deep Lake dataset. It is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow.\n",
    "\n",
    "In order to use Deep Lake, you first have to register on the Activeloop website and redeem your API token. Here are the steps for doing it:\n",
    "\n",
    "1. Sign up for an account on Activeloop's platform. You can sign up at [Activeloop's website](https://app.activeloop.ai/register). After specifying your username, click on the “Sign up” button. You should now see your homepage.\n",
    "2. You should now see a “Create API token” button at the top of your homepage. Click on it, and you’ll get redirected to the “API tokens” page. This is where you can generate, manage, and revoke your API keys for accessing Deep Lake.\n",
    "3. Click on the \"Create API token\" button. Then, you should see a popup asking for a token name and an expiration date. By default, the token expiration date is set so that the token expires after one day from its creation, but you can set it further in the future if you want to keep using the same token for the whole duration of the course. Once you’ve set the token name and its expiration date, click on the “Create API token” button.\n",
    "4. You should now see a green banner saying that the token has been successfully generated, along with your new API token, on the “API tokens” page. To copy your token to your clipboard, click on the square icon on its right.\n",
    "\n",
    "Now that you have your API token, you can conveniently store under the ACTIVELOOP_TOKEN key in the environment variable to retrieve it automatically by the Deep Lake libraries whenever needed.\n",
    "\n",
    "Let's demonsrate how it can be used.\n",
    "\n",
    "## Import Libs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9673df93-a817-4f3f-a02e-7e4721e45fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# We have loaded the environment vars using a .env file and have assigned os.environ[\"ACTIVELOOP_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a893c-8481-4f40-a119-f0e608e6d672",
   "metadata": {},
   "source": [
    "## Basic Deeplake Demo\n",
    "\n",
    "Lets demonstrate how we can use the Deeplake vector store. We will use Langchain as well as an OpenAI GPT-3.5 model as our LLM stack. We will set up a simple vector store with some birthdays, create an LLM based agent then ask a question about one of the birthdays - which will require the agent to find the details in the Deeplake.\n",
    "\n",
    "Let's first set up the Deeplake vector store and LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7858256-31a1-4876-9b2f-11254cdbe9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://pranath/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (2, 1536)  float32   None   \n",
      "    id        text      (2, 1)      str     None   \n",
      " metadata     json      (2, 1)      str     None   \n",
      "   text       text      (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['d9f49eb8-354b-11ee-9eb0-acde48001122',\n",
       " 'd9f4a034-354b-11ee-9eb0-acde48001122']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the LLM and embeddings models\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# Create Deep Lake dataset\n",
    "# Use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"pranath\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d174851-d597-4852-b464-f169dffdf9e0",
   "metadata": {},
   "source": [
    "Now, let's create a Langchain RetrievalQA chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb2d12a-2d44-40a8-b824-1d091a21f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad9dd4-0ad5-441e-9662-d697a5fa676d",
   "metadata": {},
   "source": [
    "Next, let's create an agent that uses the RetrievalQA chain as a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "065c0675-0043-479c-9a64-7a9ad3436c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb754a45-4f17-41b0-9cd0-fb91d8f83c42",
   "metadata": {},
   "source": [
    "Finally, we can use the agent to ask a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82fb97a9-926d-414a-8edd-36ca0a734c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Napoleone was born.\n",
      "Action: Retrieval QA System\n",
      "Action Input: When was Napoleone born?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Napoleon Bonaparte was born on 15 August 1769.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"When was Napoleone born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4147e-b0e0-4ca3-8cb2-9a2082209d37",
   "metadata": {},
   "source": [
    "Here, the agent used the “Retrieval QA System” tool with the query “When was Napoleone born?” which is then run on our new Deep Lake dataset, returning the most similar document (i.e., the document containing the date of birth of Napoleon). This document is eventually used to generate the final output.\n",
    "\n",
    "Note the Agent also made use of the [ReaCT framework](https://livingdatalab.com/posts/2023-07-20-llm-application-considerations-2.html) for LLM prompt structuring.\n",
    "\n",
    "This example shows how to utilise Deep Lake as a vector database and to develop an agent that uses a RetrievalQA chain as a tool to respond to queries depending on the provided content.\n",
    "\n",
    "## Adding more Data and Reloading Deeplake\n",
    "\n",
    "Let's add a case where more data is added and an existing vector storage is reloaded.\n",
    "\n",
    "We first reload a vector store from Deep Lake that is already there and is situated at a specific dataset path. After that, we import fresh text data and divide it into manageable portions. Last but not least, we include these chunks into the current dataset by producing and archiving matching embeddings for each additional text segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "950d2872-bc04-463d-a383-4e47a252ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://pranath/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://pranath/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b7931762-354d-11ee-9eb0-acde48001122',\n",
       " 'b79318e8-354d-11ee-9eb0-acde48001122']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the existing Deep Lake dataset and specify the embedding function\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# create new documents\n",
    "texts = [\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeeb672-68ac-407d-a5ff-337e30e628e2",
   "metadata": {},
   "source": [
    "Then, we replicate our prior agent and pose a query that can only be addressed by the most recent documents added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c867bd-d3e9-4ae2-9f65-8862c72c3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the wrapper class for GPT3\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# create a retriever from the db\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    " llm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# instantiate a tool that uses the retriever\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# create an agent that uses the tool\n",
    "agent = initialize_agent(\n",
    " tools,\n",
    " llm,\n",
    " agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    " verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438da6b1-84a8-4a61-8e89-1be09bd84b70",
   "metadata": {},
   "source": [
    "Let’s now test our agent with a new question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57768880-f064-4a59-8dfd-3cc68633532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Michael Jordan was born.\n",
      "Action: Retrieval QA System\n",
      "Action Input: When was Michael Jordan born?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Michael Jordan was born on 17 February 1963.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Michael Jordan was born on 17 February 1963.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Michael Jordan was born on 17 February 1963.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"When was Michael Jordan born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8becd5c-5754-4583-9042-d2efb510a3bb",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac5277-152e-4735-bcbb-5bfbff381d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
