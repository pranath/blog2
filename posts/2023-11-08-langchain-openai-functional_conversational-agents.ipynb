{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9e1476a0-20c8-46b9-b97e-6d9c23b83a1d",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-11-08'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - langchain\n",
    " - openai\n",
    " - agents\n",
    "title: Langchain and OpenAI Functions - Conversational agents\n",
    "description: In this post we will explore the advanced concept of conversational agents. Building upon our previous articles, we delve into creating a conversational future using Large Language Models (LLMs), blending tool usage with chat memory, akin to the workings of OpenAI's ChatGPT. This article serves as a comprehensive guide to understanding and implementing these agents.\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain2.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3f6a4-8348-4cd6-a336-10ecb9cfaaa3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this post we aim to explore the advanced concept of conversational agents. Building upon our previous articles, we delve into creating a conversational future using Large Language Models (LLMs), blending tool usage with chat memory, akin to the workings of OpenAI's ChatGPT. This article serves as a comprehensive guide to understanding and implementing these agents.\n",
    "\n",
    "## Understanding the Basics of Conversational Agents\n",
    "\n",
    "First, let's go over the fundamentals of agents, which are a combination of language models and code, with the language model dictating what actions to do or what the inputs to those actions should be. Then there's an agent loop, which simply utilises that agent to select the tool, then calls that tool, examines the result, and repeats this till it reaches criteria. Why this process? \n",
    "\n",
    "They can be a variety of reasons. The first and most common is to use a language model to identify where to stop. So, if you recall from the previous post, we discussed the concept of an agent's finish. We've modelled response at this point. This is one such talking point. Other options include hard coding rules such as the maximum number of iterations or anything similar. \n",
    "\n",
    "We'll be using the tools we created in the [last article](/posts/2023-11-07-langchain-openai-functions-tools-routing-apis.html). We'll take that concept of routing and selecting and then calling the tool and include it into our own agent loop using the language. We'll utilise the agent-executive class in language to demonstrate how it corresponds to the agent loop. We'll also include items like handling errors and early stopping.\n",
    "\n",
    "The first thing we'll do is set up the environment, followed by the identical tools we used in the prior article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36261434-a96c-49e8-ad41-918d14da089a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f44fbb-6708-4fce-bac4-55dc261e3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7406eb-334b-43d0-8110-31656a55b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "import datetime\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.utcnow()\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac0a1e7-567f-4775-a538-351a5ff099e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4856d233-0916-4b97-8a7c-b494d5f2b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf0d789-559b-42df-9d6d-77a0cd8888d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea75a7-6d75-4e66-a5fc-385f94780ec5",
   "metadata": {},
   "source": [
    "## Implementing Conversational Agents in Practice\n",
    "\n",
    "### Setting Up the Environment\n",
    "\n",
    "On our list of tools, we'll generate functions by calling format tool to open a function. We'll build our model with temperature equal to zero and bind it to functions. We previously linked everything together in a chain, adding to the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933ce7cb-099c-4f43-a548-b355a5b66511",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4281a2d8-7884-40e0-b34d-09c39eecbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"what is the weather is sf?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1dbc697-3e0a-4197-8ddc-2894763b4b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab829b59-803f-488f-bcf3-5119c224baa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 37.7749, 'longitude': -122.4194}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c184d-b0bb-48b2-859f-7202678b2830",
   "metadata": {},
   "source": [
    "Again, that's because the input here is specifically aimed at calling one of the tools. What we want to do is create this loop that determines what tools do is then calls that tool and passes it back in and looks at it and tells them something. If you look at the prompt, what that means is we need a place in the prompt that's back in this history of tools that are called and the corresponding output. We don't have to change the prompt a little bit to add that in. Specifically, we want to add in a place where we can pass in a list of messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b4d483-c498-4932-a373-bc2f4da17c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cefdb7-523b-4121-8dae-afee9359ef6c",
   "metadata": {},
   "source": [
    "That's because we'll convert this tool selection to a tool observation and a collection of messages before passing it back in. To serve as a placeholder for this series of messages, we'll utilise a message in placeholder. Then, on our prompt template, we'll have the same initial two components, but we'll put this even script add message in placeholder. Essentially, what is the same is that we'll have a system message, which is this, again, brief system message, where they'll have the user input. Following that, we'll pass in the action and observation pairings. That prompt is created by us. With this prompt, we're going to start a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66ac6ea2-0e20-424a-ba33-6f8d627379b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496fd16-7da8-466a-8a4e-5ce706831938",
   "metadata": {},
   "source": [
    "Let's try it again with the same input. We frequently need to pass an agent scratchpad at this point because this is the variable that we're going to feed in. We'll pass through this beam and move up the list because we haven't performed any actions yet. Then, as we've been doing, we can see which function result one wants us to call, and then we can call that function and return to observation. Looking at this observation, we can observe that it is made of string. We also have result one, which is this agent action message log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bada1de8-d7c4-4880-98c2-d47cc65f684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\",\n",
    "    \"agent_scratchpad\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d5f4e61-36a3-4d80-9c49-0b83cedb1592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11845c11-7d41-455a-8ac0-4b8161acaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = get_current_temperature(result1.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "810184c8-8495-4189-b83e-17220cf65bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 11.0°C'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c1b223d-ea2a-414c-924c-512c4c9da66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.agent.AgentActionMessageLog"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e18f6b7-e877-48c3-aab7-165830dbfbcd",
   "metadata": {},
   "source": [
    "So, how do we convert these two things back into a list of messages that we didn't pass into the agent scratchpad? To accomplish this, we'll utilise this function format to open a function from the langchain agents format_scratchpad. What happens behind the scenes in format to open a function? What will happen is that we will take the first result. We'll take the message log from result one in particular. So the message log is a collection of messages that contribute to the current agent action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61853679-75b5-4462-8059-f1214286a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ca4278-165a-4915-8ec6-09e696f29d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_temperature', 'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}'}})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.message_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27e079a7-d58c-41c0-8762-4e183e72a1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_temperature', 'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}'}}),\n",
       " FunctionMessage(content='The current temperature is 11.0°C', name='get_current_temperature')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_to_openai_functions([(result1, observation), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025e988-991d-4a83-b3cf-1227725d10a0",
   "metadata": {},
   "source": [
    "So, if we look at it, it contains the chat message with the extra coordinates, the function call that says obtain current temperature, and then the parameters with the JSON string, the precise response from open AI. So we're saving that in this message log so that when it comes time to build this agent scratchpad, we can just put it back in there. Then, given the observation, which is just a string, we'll use the function message type that we discussed in article one and feed that in.\n",
    "\n",
    "So we'll call this function, and we'll call it on a list of two, both of which correspond to the agent action and the observation. And the reason for the list of two is that we add extra steps for the agent to take. We can just keep passing and all of this of two both, and we will keep generating this listed message. So here we have an AI message, and the output is that only tomorrow and making the tape is agent activity. Then there's this function message with the content observation and the name obtain temperature.\n",
    "\n",
    "If we then take that and run the chain for the same input again, but this time with agent scratchpad. If we look at where we obtain that from result to agent finish with the output being current temperature, we can see that the temperature is still there. So let's combine all of that into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09735a9e-452d-472b-a4c0-35c315e9e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\", \n",
    "    \"agent_scratchpad\": format_to_openai_functions([(result1, observation)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9df61ecc-52fe-4fae-85a2-dbb61d68e2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 11.0°C.'}, log='The current temperature in San Francisco is 11.0°C.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08650c73-eccd-4c53-be5d-4d6fe886ba98",
   "metadata": {},
   "source": [
    "### Building the Agent Loop\n",
    "\n",
    "So what we're going to do is develop a run agent function that will accept user input. It will start with an empty list. This is the middle stage, and it will continue for a while. And while it is correct, it will call the chain with this user input and with this agent scratchpad every time we call format on this list of intermediate stages. And we'll see how that's put together later. If the outcome is an agent finish, we will return with that result. Otherwise, it indicates an agent action.\n",
    "\n",
    "So we'll look for the relevant tool to utilise, which I'll call with that tool input, and then we'll return to observation. And then we'll add that to the whole set of results in observation to intermediate phases and continue on our path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9dd139f-e06a-4148-a025-ae100851c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = chain.invoke({\n",
    "            \"input\": user_input, \n",
    "            \"agent_scratchpad\": format_to_openai_functions(intermediate_steps)\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e86c69-0c63-4bfe-b80f-6e686d4b211b",
   "metadata": {},
   "source": [
    "### Enhancing the Agent Loop\n",
    "\n",
    "We'll make one tiny change to this chain before proceeding to make it a genuine agent chain. And basically, we're going to use this style to open up the function logic and the chain itself. That will make it a little more portable. So it only has to accept user input and intermediate processes. We'll build this agent chain with a dependable pass through. A dependable pass through just takes the initial input and passes it through. What we're going to do is insert this assigned variable here.\n",
    "\n",
    "This assignment requires you to add a new parameter to the dictionary that will act as an agent scratchpad. And it is at this point that we refer to the format to open the air function on intermediate phases. We're using the pipe, and the end result is a dictionary into the chain. And we'll obtain a complete end-to-end chain that now includes the input and intermediate steps. This does the necessary intermediate step processing to build the scratchpad, then passes it to the prompt, the model, to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f6c1121-70a6-41ae-8c3d-41f7cfc756af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a05df88-4ad9-4c48-a7dd-6b42db91cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"input\": user_input, \n",
    "            \"intermediate_steps\": intermediate_steps\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8bf88b6-9992-4041-8344-ef4921bf3cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 11.0°C.'}, log='The current temperature in San Francisco is 11.0°C.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is the weather is sf?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d89c99-c65c-4efc-b558-73d4f2bfda16",
   "metadata": {},
   "source": [
    "With this update, we can now make a slightly simpler move here, where you'll note that we're simply invoking the agent chain on the input directly. Because pre-processing is now done within the agent chain. Let us now execute this agent. So let's call it based on what the weather is in asset. We return to the print chain. Let's go on to another question. \n",
    "\n",
    "What exactly is Langchain? We can see that we get a response when we run the agent now. The chain is then a framework designed to simplify the development of LLM based applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c94a493-b480-409a-968a-038c8895709c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for various tasks such as document analysis and summarization, chatbots, and code analysis. LangChain helps developers leverage the power of language models in their applications.'}, log='LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for various tasks such as document analysis and summarization, chatbots, and code analysis. LangChain helps developers leverage the power of language models in their applications.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c8c6fd7-7c8c-487a-81de-1dcff739a2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'Hello! How can I assist you today?'}, log='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b270a064-8cf1-44da-a59e-be622f9099b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87baebf4-2555-4e98-aece-a1c327644817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'langchain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "Page: Prompt engineering\n",
      "Summary: Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.A prompt for a text-to-text model can be a query such as \"what is Fermat's little theorem?\", a command such as \"write a poem about leaves falling\", a short statement of feedback (for example, \"too verbose\", \"too formal\", \"rephrase again\", \"omit this word\") or a longer statement including context, instructions, and input data. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as \"maison -> house, chat -> cat, chien -> dog\", an approach called few-shot learning.When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\n",
      "\n",
      "\n",
      "\n",
      "Page: Sentence embedding\n",
      "Summary: In natural language processing, a sentence embedding refers to a numeric representation of a sentence in the form of a vector of real numbers which encodes meaningful semantic information.State of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \n",
      "Other approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions. Though this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \n",
      "An alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for various tasks such as document analysis and summarization, chatbots, and code analysis. LangChain helps developers leverage the power of language models in their applications.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is langchain?',\n",
       " 'output': 'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for various tasks such as document analysis and summarization, chatbots, and code analysis. LangChain helps developers leverage the power of language models in their applications.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2da813c-47c3-487e-8c43-444c9b4d9821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob', 'output': 'Hello Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15ba8a9f-fcb2-44d0-878f-3017f607e955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry, but I don't have access to personal information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name',\n",
       " 'output': \"I'm sorry, but I don't have access to personal information.\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is my name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28900fc7-3457-430d-b338-cc9673bb057c",
   "metadata": {},
   "source": [
    "Agents also includes error handling for tools. So, if you call it tool and the tool errors, it will take care of it wonderfully. We catch that error. We return it to the language model. That is the proper answer we want - in this example when it doesn;t know the name. We may easily use this by importing an agent executor from the language. This can then be initialised with agent equals the agent chain. We pass in the tools because it requires them to be able to call it. Then we pass those tools in for some nice logging. We can view some logs of what's going on if we now call this with verbose set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d1df28a-27da-4f8e-ab03-a37a276db74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8598e87-de8a-4710-a90e-2a919adc00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3865c2c-cb80-4341-bcbe-3b5150749622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40b036fc-446f-4075-9a92-9a4bac6ecb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37f70acf-57ff-4ab9-a942-f0ad6e680c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?')],\n",
       " 'output': 'Hello Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1554b0c-8c72-4d8d-bbed-20afd94828c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYour name is Bob.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats my name',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?'),\n",
       "  HumanMessage(content='whats my name'),\n",
       "  AIMessage(content='Your name is Bob.')],\n",
       " 'output': 'Your name is Bob.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40887f61-6746-4e51-852d-5ff0457384b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe current temperature is 11.0°C\u001b[0m\u001b[32;1m\u001b[1;3mThe current temperature in San Francisco is 11.0°C.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats the weather in sf?',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?'),\n",
       "  HumanMessage(content='whats my name'),\n",
       "  AIMessage(content='Your name is Bob.'),\n",
       "  HumanMessage(content='whats the weather in sf?'),\n",
       "  AIMessage(content='The current temperature in San Francisco is 11.0°C.')],\n",
       " 'output': 'The current temperature in San Francisco is 11.0°C.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0b511-21d6-4c7e-b4e2-f886bc296997",
   "metadata": {},
   "source": [
    "This is the result that we get back as part of the agent chain. It's very similar to a lot of what's powering OpenAI's ChatGPT. So if you're interacted with a code interpreter there or the plug-in system there, you'll notice that the language model often thinks about what to do. Then it calls a tool that gets back the response, then it calls another one, they get back the response from that, and then it returns to the user. So this is a really powerful loop and technique that can be used to create something like ChatGPT.\n",
    "\n",
    "## Advanced Agent Loop Techniques\n",
    "\n",
    "Let's include another tool. We're planning to update our tool list. So we'll have the same or temperature tool, as well as the same Wikipedia search engine. Then there's this new tool that we made which just returns the reversed input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1cd4bd2-703e-4e4e-8f5c-ae3d491a41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_your_own(query: str) -> str:\n",
    "    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n",
    "    print(type(query))\n",
    "    return query[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67cb6fdc-ff55-4a7b-8a39-1871006aa990",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia, create_your_own]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54991a-79d0-48c1-84a8-322ddcc32e7d",
   "metadata": {},
   "source": [
    "So, taking a quick glance at this, we can see that we have our functions, which are calling the same function as previously, format tool to place the new functions. We have our model, which is the chat open and I model, and we'll be adding functions to it. We have memory, which is the same as before. We have our prompt, which is the same as before. So it has a system message, a conversation history, user input, and a scratchpad for the agent. \n",
    "\n",
    "Then, as before, we build the agent chain, and finally, as before, we build the agent-executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66a73e69-acfb-471e-b54d-8ac586235521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n",
       "      require([\"gridstack\"], function(GridStack) {\n",
       "\twindow.GridStack = GridStack\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"notyf\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 2;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n",
       "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) >= 0) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var css_urls = [\"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.css\", \"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/debugger.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/dataframe.css\"];\n",
       "  var inline_js = [    function(Bokeh) {\n",
       "      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n",
       "    },    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, js_modules, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.css\", \"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/debugger.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/dataframe.css\"];\n  var inline_js = [    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n    },    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.bk-root, .bk-root .bk:before, .bk-root .bk:after {\n",
       "  font-family: var(--jp-ui-font-size1);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    \n",
    "    def __init__(self, tools, **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are helpful but sassy assistant\"),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "        self.chain = RunnablePassthrough.assign(\n",
    "            agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "        ) | self.prompt | self.model | OpenAIFunctionsAgentOutputParser()\n",
    "        self.qa = AgentExecutor(agent=self.chain, tools=tools, verbose=False, memory=self.memory)\n",
    "    \n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return\n",
    "        inp.value = ''\n",
    "        result = self.qa.invoke({\"input\": query})\n",
    "        self.answer = result['output'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=450)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=450, styles={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        return pn.WidgetBox(*self.panels, scroll=True)\n",
    "\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "080c3827-87fc-4dbe-a566-ca95b3f9068e",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='1017'>\n",
       "  <div class=\"bk-root\" id=\"4bf853e8-bdc0-4b98-bdc4-8d008c776ad1\" data-root-id=\"1017\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"7bceb14e-18d7-4508-9263-a9fa1ea90c09\":{\"defs\":[{\"extends\":null,\"module\":null,\"name\":\"ReactiveHTML1\",\"overrides\":[],\"properties\":[]},{\"extends\":null,\"module\":null,\"name\":\"FlexBox1\",\"overrides\":[],\"properties\":[{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_content\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_items\"},{\"default\":\"row\",\"kind\":null,\"name\":\"flex_direction\"},{\"default\":\"wrap\",\"kind\":null,\"name\":\"flex_wrap\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"justify_content\"}]},{\"extends\":null,\"module\":null,\"name\":\"GridStack1\",\"overrides\":[],\"properties\":[{\"default\":\"warn\",\"kind\":null,\"name\":\"mode\"},{\"default\":null,\"kind\":null,\"name\":\"ncols\"},{\"default\":null,\"kind\":null,\"name\":\"nrows\"},{\"default\":true,\"kind\":null,\"name\":\"allow_resize\"},{\"default\":true,\"kind\":null,\"name\":\"allow_drag\"},{\"default\":[],\"kind\":null,\"name\":\"state\"}]},{\"extends\":null,\"module\":null,\"name\":\"click1\",\"overrides\":[],\"properties\":[{\"default\":\"\",\"kind\":null,\"name\":\"terminal_output\"},{\"default\":\"\",\"kind\":null,\"name\":\"debug_name\"},{\"default\":0,\"kind\":null,\"name\":\"clears\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationAreaBase1\",\"overrides\":[],\"properties\":[{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationArea1\",\"overrides\":[],\"properties\":[{\"default\":[],\"kind\":null,\"name\":\"notifications\"},{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"},{\"default\":[{\"background\":\"#ffc107\",\"icon\":{\"className\":\"fas fa-exclamation-triangle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"warning\"},{\"background\":\"#007bff\",\"icon\":{\"className\":\"fas fa-info-circle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"info\"}],\"kind\":null,\"name\":\"types\"}]},{\"extends\":null,\"module\":null,\"name\":\"Notification\",\"overrides\":[],\"properties\":[{\"default\":null,\"kind\":null,\"name\":\"background\"},{\"default\":3000,\"kind\":null,\"name\":\"duration\"},{\"default\":null,\"kind\":null,\"name\":\"icon\"},{\"default\":\"\",\"kind\":null,\"name\":\"message\"},{\"default\":null,\"kind\":null,\"name\":\"notification_type\"},{\"default\":false,\"kind\":null,\"name\":\"_destroyed\"}]},{\"extends\":null,\"module\":null,\"name\":\"TemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]},{\"extends\":null,\"module\":null,\"name\":\"MaterialTemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]}],\"roots\":{\"references\":[{\"attributes\":{\"args\":{\"bidirectional\":false,\"properties\":{\"value\":\"loading\"},\"source\":{\"id\":\"1023\"},\"target\":{\"id\":\"1025\"}},\"code\":\"\\n    if ('value'.startsWith('event:')) {\\n      var value = true\\n    } else {\\n      var value = source['value'];\\n      value = value;\\n    }\\n    if (typeof value !== 'boolean' || source.labels !== ['Loading']) {\\n      value = true\\n    }\\n    var css_classes = target.css_classes.slice()\\n    var loading_css = ['pn-loading', 'arcs']\\n    if (value) {\\n      for (var css of loading_css) {\\n        if (!(css in css_classes)) {\\n          css_classes.push(css)\\n        }\\n      }\\n    } else {\\n     for (var css of loading_css) {\\n        var index = css_classes.indexOf(css)\\n        if (index > -1) {\\n          css_classes.splice(index, 1)\\n        }\\n      }\\n    }\\n    target['css_classes'] = css_classes\\n    \",\"tags\":[[140512133348272,[null,\"value\"],[null,\"loading\"]]]},\"id\":\"1029\",\"type\":\"CustomJS\"},{\"attributes\":{\"reload\":false},\"id\":\"1031\",\"type\":\"panel.models.location.Location\"},{\"attributes\":{\"client_comm_id\":\"028641e4464c407a9e2d7ddb9e8410d8\",\"comm_id\":\"4fc485c1fd31460cab1e38c511fe2434\",\"plot_id\":\"1017\"},\"id\":\"1030\",\"type\":\"panel.models.comm_manager.CommManager\"},{\"attributes\":{\"children\":[{\"id\":\"1022\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1027\"}],\"margin\":[0,0,0,0],\"name\":\"Column00132\"},\"id\":\"1021\",\"type\":\"Column\"},{\"attributes\":{\"children\":[{\"id\":\"1019\"}],\"margin\":[0,0,0,0],\"name\":\"Row00135\"},\"id\":\"1018\",\"type\":\"Row\"},{\"attributes\":{\"margin\":[0,0,0,0],\"tabs\":[{\"id\":\"1028\"}]},\"id\":\"1020\",\"type\":\"panel.models.tabs.Tabs\"},{\"attributes\":{\"children\":[{\"id\":\"1023\"}],\"margin\":[0,0,0,0],\"name\":\"Row00123\"},\"id\":\"1022\",\"type\":\"Row\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"Divider00124\",\"style\":{\"height\":\"100%\",\"width\":\"100%\"},\"text\":\"<hr style=\\\"margin: 0px\\\">\",\"width_policy\":\"fit\"},\"id\":\"1024\",\"type\":\"Div\"},{\"attributes\":{\"children\":[{\"id\":\"1026\"}],\"height\":400,\"margin\":[0,0,0,0],\"min_height\":400,\"name\":\"Row00129\"},\"id\":\"1025\",\"type\":\"Row\"},{\"attributes\":{\"children\":[{\"id\":\"1018\"},{\"id\":\"1020\"}],\"margin\":[0,0,0,0],\"name\":\"Column00137\"},\"id\":\"1017\",\"type\":\"Column\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"Divider00131\",\"style\":{\"height\":\"100%\",\"width\":\"100%\"},\"text\":\"<hr style=\\\"margin: 0px\\\">\",\"width_policy\":\"fit\"},\"id\":\"1027\",\"type\":\"Div\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"Str00127\",\"text\":\"&lt;pre&gt; &lt;/pre&gt;\"},\"id\":\"1026\",\"type\":\"panel.models.markup.HTML\"},{\"attributes\":{\"js_property_callbacks\":{\"change:value\":[{\"id\":\"1029\"}]},\"margin\":[5,10,5,10],\"max_length\":5000,\"placeholder\":\"Enter text here\\u2026\"},\"id\":\"1023\",\"type\":\"TextInput\"},{\"attributes\":{\"css_classes\":[\"markdown\"],\"margin\":[5,5,5,5],\"name\":\"Markdown00133\",\"text\":\"&lt;h1&gt;QnA_Bot&lt;/h1&gt;\"},\"id\":\"1019\",\"type\":\"panel.models.markup.HTML\"},{\"attributes\":{\"child\":{\"id\":\"1021\"},\"name\":\"Column00132\",\"title\":\"Conversation\"},\"id\":\"1028\",\"type\":\"Panel\"}],\"root_ids\":[\"1017\",\"1030\",\"1031\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.3\"}};\n",
       "    var render_items = [{\"docid\":\"7bceb14e-18d7-4508-9263-a9fa1ea90c09\",\"root_ids\":[\"1017\"],\"roots\":{\"1017\":\"4bf853e8-bdc0-4b98-bdc4-8d008c776ad1\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "Column\n",
       "    [0] Row\n",
       "        [0] Markdown(str)\n",
       "    [1] Tabs\n",
       "        [0] Column\n",
       "            [0] Row\n",
       "                [0] TextInput(placeholder='Enter text here…')\n",
       "            [1] Divider()\n",
       "            [2] ParamFunction(function, height=400, loading_indicator=True)\n",
       "            [3] Divider()"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "1017"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb = cbfs(tools)\n",
    "\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=400),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# QnA_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1))\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72566e7c-8136-456c-bce0-42d75265f5ea",
   "metadata": {},
   "source": [
    "## Conclusion: The Future of Conversational Agents\n",
    "\n",
    "In conclusion, this exploration into conversational agents reveals a vast potential for creating intelligent, interactive systems based on LLM based agent applications. By understanding the basics, implementing an effective agent loop, and integrating advanced techniques, we can build sophisticated agents capable of dynamic interactions very similar to OpenAI's ChatGPT. The future of conversational agents is promising, with endless possibilities for innovation and practical applications. \n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [Functions, Tools and Agents with LangChain](https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/) by DeepLearning.ai - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05a004-b26d-4f2c-bedd-c1b62482606d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
