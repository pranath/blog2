{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d38019f1-99ad-4e90-9d29-5f0118da94e4",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-18'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    "title: Chat with a GitHub Repository\n",
    "description: This project will focus on using the language models for generating embeddings from corpora this will power a chat application that can answer questions from a GitHub repositorys text files like .md and .txt. \n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake1.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2517a-7627-447b-b707-35c370332d1c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Large Linguistic Models (LLMs) achieve a superior level of language understanding during their training. It allows them to create human-like text and create powerful representations from text data. We covered using LangChain to use LLM to write content with hands-on projects.\n",
    "\n",
    "This post will focus on using language models to create seamless vector representations from a corpus. The representation in question will power a chat application that can answer questions from any text by finding the closest data point to the request. This project focuses on finding answers from text files in GitHub repositories, such as .md and .txt. So we'll start by collecting data from a GitHub repository and converting that data into embeds. These integrations will be saved to Activeloop's Deep Lake vector database for quick and easy access. The Deep Lake recovery object will find related files based on user queries and provide them as context to the template. Finally, the model uses the information provided to best answer the question. \n",
    "\n",
    "## What is Deep Lake?\n",
    "\n",
    "It is a vector database that provides multimodal storage for all data types (including but not limited to PDF, audio and video files) as well as vectorized representations of data. they. This service eliminates the need to build data infrastructure while handling high-dimensional tensors. Furthermore, it offers a wide range of features such as visualization, parallel computing, data versioning, integration with major AI frameworks, and most importantly, search integration. Supported vector operations like cosine_similarity allow us to find related points in an integration space.\n",
    "\n",
    "\n",
    "The rest of the article is based on code from [\"Chat with Github Repo\"](https://github.com/peterw/Chat-with-Github-Repo/) and is organized as follows:\n",
    "\n",
    "1) Processing the Files \n",
    "\n",
    "2) Saving the Embedding \n",
    "\n",
    "3) Retrieving from Database \n",
    "\n",
    "4) Creating an Interface.\n",
    "\n",
    "## Processing the Repository Files\n",
    "\n",
    "To access files from the target repository, the script will clone the desired repository to your computer, placing the files in a folder named \"repos\". Once we've uploaded the file, just browse through the directory to create a list of files. It is possible to filter out specific extensions or environmental factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf513a07-09de-4ebc-bc06-4eaa9efeb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./path/to/cloned/repository\"\n",
    "docs = []\n",
    "file_extensions = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "\t\n",
    "\tfor file in filenames:\n",
    "\t  file_path = os.path.join(dirpath, file)\n",
    "\t\n",
    "\t  if file_extensions and os.path.splitext(file)[1] not in file_extensions:\n",
    "      continue\n",
    "\t\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    docs.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080e26d-ea28-4f29-8002-d20376b92d0f",
   "metadata": {},
   "source": [
    "The sample code above generates a list of all the files in the archive. It is possible to filter each item by extension types like file_extensions=['.md', '.txt'] with focus only on markdown and text files. The original implementation had more filters and an unsafe approach; Please refer to the [complete code](https://github.com/peterw/Chat-with-Github-Repo/blob/main/src/utils/process.py#L20).\n",
    "\n",
    "Now that the list of files has been generated, the split_documents method of the CharacterTextSplitter class of the LangChain library will read the files and split their contents into blocks of 1000 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd2feb-afe1-48b9-bdb9-cbcdea8738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splitted_text = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b9dc49-1339-4e2d-8396-9e1aa276c6d4",
   "metadata": {},
   "source": [
    "The splitted_text variable holds the textual content which is ready to be converted to embedding representations.\n",
    "\n",
    "## Saving the Embeddings\n",
    "\n",
    "Let's create the database before doing the text-to-embed conversion. This is where the integration between LangChain and Deep Lake comes in handy! We initialize the database in the cloud using the hub:\n",
    "//... format and LangChain's OpenAIEmbeddings() as an embedded function. The Deep Lake Library will collect the content and automatically generate the embedded content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93974e72-ade8-40d2-b62a-5e23416f2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_chat_with_gh\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(splitted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c96ca-3ed3-4b9b-a0e6-a00495406419",
   "metadata": {},
   "source": [
    "## Retrieving from Database\n",
    "\n",
    "The final step is to code the process to answer user questions based on information from the database. Again, the integration of LangChain and Deep Lake greatly simplifies the process, making it extremely easy. We need 1) an object that accesses the Deep Lake database using the .as_retriever() method and 2) a conversational model like ChatGPT using the ChatOpenAI() class.\n",
    "\n",
    "Finally, LangChain's RetrievalQA layer ties everything together! It uses user input as prompt while including database results as context. So the ChatGPT model can find the correct model from the provided context. It should be noted that the database fetcher is configured to collect instances that are closely related to the user's query using cosine similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82230bc0-2cf6-4b7c-be53-dbf21ab2972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the DeepLake instance\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Set the search parameters for the retriever\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "# Create a ChatOpenAI model instance\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a RetrievalQA instance from the model and retriever\n",
    "qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# Return the result of the query\n",
    "qa.run(\"What is the repository's name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f11cf3-875e-4069-ac6a-d6451644d62b",
   "metadata": {},
   "source": [
    "## Create an Interface\n",
    "\n",
    "Creating a user interface (UI) that is accessible to the bot through a web browser is an optional but very important step. This addition will take your ideas to the next level, allowing users to interact with the application easily even without any programming expertise. This repository uses the Streamlit platform, a quick and easy way to build and deploy an application instantly and for free - and i have used before on [earlier projects](https://livingdatalab.com/projects.html). It provides a wide range of utilities to eliminate the need to use backend frameworks or frontends to build a web application.\n",
    "\n",
    "We need to install the library and its chat component using the pip command. We strongly recommend that you install the latest version of each library. In addition, the provided codes have been tested using streamlit and streamlit-chat versions of 2023.6.21 and 20230314 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f602f5-ea14-4a95-8abc-3186cd14cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit streamlit_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9d864-fd89-423c-8c57-6d867951d4af",
   "metadata": {},
   "source": [
    "The [API documentation page](https://seldonia.notion.site/Chat-with-a-GitHub-Repository-26669f0e8d634ecf9ece0fb7d2cf2b78) provides a complete list of available extensions that can be used in your application. We need a simple UI that accepts user input and displays the chat in a chat-like interface. Fortunately, Streamlit offers both.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00f54c-4db2-42f6-b2d2-7dc8176b18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "# Set the title for the Streamlit app\n",
    "st.title(f\"Chat with GitHub Repository\")\n",
    "\n",
    "# Initialize the session state for placeholder messages.\n",
    "if \"generated\" not in st.session_state:\n",
    "\tst.session_state[\"generated\"] = [\"i am ready to help you ser\"]\n",
    "\n",
    "if \"past\" not in st.session_state:\n",
    "\tst.session_state[\"past\"] = [\"hello\"]\n",
    "\n",
    "# A field input to receive user queries\n",
    "input_text = st.text_input(\"\", key=\"input\")\n",
    "\n",
    "# Search the databse and add the responses to state\n",
    "if user_input:\n",
    "\toutput = qa.run(user_input)\n",
    "\tst.session_state.past.append(user_input)\n",
    "\tst.session_state.generated.append(output)\n",
    "\n",
    "# Create the conversational UI using the previous states\n",
    "if st.session_state[\"generated\"]:\n",
    "\tfor i in range(len(st.session_state[\"generated\"])):\n",
    "\t\tmessage(st.session_state[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "\t\tmessage(st.session_state[\"generated\"][i], key=str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b513f2-7e30-42e2-abc8-d4dba21d8bd9",
   "metadata": {},
   "source": [
    "The above code is very simple. We call st.text_input() to generate text input for user queries. The query will be passed to the previously declared RetrievalQA object and the results will be displayed using the message component. You should store the code in question in a Python file ( for example chat.py) and run the following command to see the local interface. \n",
    "\n",
    "**streamlit run ./chat.py**\n",
    "\n",
    "## Putting Everything Together\n",
    "\n",
    "As we mentioned before, the codes for this lesson are available in [\"Chat with GitHub Repo\"](https://github.com/peterw/Chat-with-Github-Repo), you can easily fork and get it up and running in 3 easy steps. First, fork the repository and install the required libraries using pip.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d63371-c870-4675-90a9-cb2ec47da269",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/peterw/Chat-with-Git-Repo.git\n",
    "!cd Chat-with-Git-Repo\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc19859-de71-4aa6-af1b-ecfc55e6c232",
   "metadata": {},
   "source": [
    "Second, rename the environment file from .env.example to .env and fill in the API keys. You must have accounts in both OpenAI and Activeloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99707bf3-7b66-495a-a15e-772d18b1b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp .env.example .env\n",
    "\n",
    "# OPENAI_API_KEY=your_openai_api_key\n",
    "# ACTIVELOOP_TOKEN=your_activeloop_api_token\n",
    "# ACTIVELOOP_USERNAME=your_activeloop_username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c1764-9503-4e32-9196-ce2f5ffcc57b",
   "metadata": {},
   "source": [
    "Lastly, use the process command to read and store the contents of any repository on the Deep Lake by passing the repository URL to the --repo-url argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a73521-aee1-41a5-abad-629a86c19f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/main.py process --repo-url https://github.com/username/repo_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875ac11-275f-44ad-a3ac-ffa0536a048d",
   "metadata": {},
   "source": [
    "**Be aware of the costs associated with generating embeddings using the OpenAI API. Using a smaller repository that needs fewer resources and faster processing is better.**\n",
    "\n",
    "And run the chat interface by using the chat command followed by the database name. It is the same as repo_name from the above sample. You can also see the database name by logging in to the Deep Lake dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b5110-f823-4048-b8d7-7c57414afef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/main.py chat --activeloop-dataset-name <dataset_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be38463-68e3-4204-918d-dca450340d44",
   "metadata": {},
   "source": [
    "The application will be accessible using a browser on the http://localhost:8501 URL or the next available port. (as demonstrated in the image below) Please read the [complete instruction](https://github.com/peterw/Chat-with-Github-Repo/tree/main#setup) for more information, like filtering a repository content by file extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b26e45-260c-414f-9504-0cf605e7163d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have broken down the important sections of the \"Chat with GitHub Repo\" repository to learn how to build chatbots using the user interface. You learned how to use the Deep Lake database to store high-dimensional embedded assets and query them using functions similar to cosine. Their integration with the LangChain library has provided easy-to-use APIs for storing and retrieving data. Finally, we created the user interface using the Streamlit library to make the bot accessible to everyone.  \n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b69781-662a-4d62-ad2a-f94557a809c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
