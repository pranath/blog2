{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1855468e-b4a5-43f3-a252-cee26ab49e54",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-06-21'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - openai\n",
    " - prompt-engineering\n",
    "title: Using Chain of Thought Reasoning with ChatGPT\n",
    "description: In this article we will focus on large language model tasks to process a series of inputs i.e. the tasks that take the input and generate a useful output often through a series of steps - using ChatGPT\n",
    "image: https://github.com/pranath/blog/raw/master/images/chatgpt1.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613f6af-ce1c-49ea-ae99-0d2e3fa3fae1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Large language models such as [ChatGPT](https://openai.com/blog/chatgpt) can generate text responses based on a given prompt or input. Writing prompts allow users to guide the language model's output by providing a specific context or topic for the response. This feature has many practical applications, such as generating creative writing prompts, assisting in content creation, and even aiding in customer service chatbots. \n",
    "\n",
    "In [earlier articles](/#category=openai) i've looked at how you can use ChatGPT to solve some of these tasks with simple prompts. But in many use cases, what is required is not just one prompt but a sequence of prompts where we need to also consider the outputs at each stage, before providing a final output - for example with a customer service chatbot.\n",
    "\n",
    "In this article we will focus on large language model tasks to process a series of inputs i.e. the tasks that take the input and generate a useful output often through a series of steps - using ChatGPT.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Load the API key and relevant Python libaries.\n",
    "\n",
    "First we need to load certain python libs and connect the OpenAi api.\n",
    "\n",
    "The OpenAi api library needs to be configured with an account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
    " ```\n",
    " !export OPENAI_API_KEY='sk-...'\n",
    " ```\n",
    "\n",
    "Or, set `openai.api_key` to its value:\n",
    "\n",
    "```\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df1c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98a05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d273f-df72-47e2-a9a6-a8994d742aec",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting\n",
    "\n",
    "As we have seen in [earlier articles](/#category=openai) of mine, an LLM may occasionally find it necessary to reason in depth about an issue before responding to a particular inquiry. In order to allow the model to consider the issue more thoroughly and for a longer period of time before providing a final answer, we can reframe the question to ask for a sequence of pertinent reasoning steps before the model responds. This way, the model will be less likely to make reasoning mistakes by jumping to the wrong conclusion.\n",
    "\n",
    "And in general, we call this strategy of \n",
    "asking the model to reason about a problem in steps, \n",
    "**\"Chain of Thought Reasoning\"**.\n",
    "\n",
    "For some applications, it would be improper to reveal to the user the thought process a model goes through to arrive at a solution. In tutoring applications, for instance, we might want to encourage students to work on their own solutions, but a model's analysis of the student's solution may give away the solution to the student. \n",
    "Inner monologue, which is merely a fancy way of stating concealing the model's thinking from the user, is a strategy that can be employed to mitigate this. \n",
    "\n",
    "The purpose of *inner monologue* is to give the model instructions on how to organise elements of the output that are intended to be passed easily but should be hidden from the user. \n",
    " \n",
    "The output is then passed and only a portion of it is made available to the user before the output is finally presented to them. So keep in mind the classification issue from a [previous article](/posts/2023-06-19-evaluating-classification-inputs-large-language-models.html) when we requested the model to divide a client question into a primary and secondary group. Therefore, we might wish to follow different steps depending on that classification.\n",
    "\n",
    "## Use-case: A customer enquiry\n",
    "\n",
    "Imagine that the customer's inquiry had been placed under the category for product information. \n",
    "We'll want to give details about the things we offer in the following instructions. \n",
    "Therefore, the classification in this instance would have been main, general inquiry, secondary, and product information. \n",
    "So, starting there, let's look at an example. \n",
    " \n",
    "Let's now review the message from our system. \n",
    "So, what we're asking the model to do here is consider the answer before drawing a conclusion. \n",
    "\n",
    "So, the \n",
    "instruction is:\n",
    "\n",
    ">\"Follow these steps to answer \n",
    "the customer queries. The customer query will be \n",
    "delimited with four hashtags.\"\n",
    "\n",
    "So, then we've split this up into steps. \n",
    "\n",
    "So, the first step is to:\n",
    "\n",
    ">\"Decide whether the user is asking \n",
    "a question about a specific product or products. \n",
    "And a product category doesn't count.\". \n",
    "\n",
    "Step two:\n",
    "\n",
    ">\"If the user is asking about specific products, \n",
    "identify whether the products are in the following list.\". \n",
    "\n",
    "And now we've included a list of available products. So, here \n",
    "we have five available products. \n",
    "They're all varieties of laptops and these are all \n",
    "made up products. They were actually generated by GPT-4. \n",
    " \n",
    "And step three:\n",
    "\n",
    ">\"If the message contains \n",
    "products in the list above, list any assumptions that the user \n",
    "is making in their message. For example, \n",
    "that laptop X is bigger than laptop Y or that \n",
    "laptop Z has a 2 year warranty.\" \n",
    " \n",
    "Step four is:\n",
    "\n",
    ">\"If the user made any assumptions, \n",
    "figure out whether the assumption is true based \n",
    "on your product information.\". \n",
    "\n",
    "And step five is:\n",
    "\n",
    ">\"First, politely correct the customer's \n",
    "incorrect assumptions, if applicable. Only \n",
    "mention or reference products in the list of \n",
    "5 available products, as these are the only five \n",
    "products that the store sells. And answer the customer in \n",
    "a friendly tone.\" \n",
    "\n",
    "And these kind of very pedantic instructions are probably \n",
    "unnecessary for a more advanced language \n",
    "model like GPT-4. Then we'll ask the model to use the following format. \n",
    "So, step one, delimiter, it's reasoning. Step two, delimiter, reasoning \n",
    "and so on. \n",
    "\n",
    "Using the delimiters will mean that it \n",
    "will be easier for us later to get just this response to the customer, and kind of cut off everything before. \n",
    "\n",
    "So, now let's try an example user message. \n",
    "So, our message is:\n",
    "\n",
    ">\"by how much is the BlueWave Chromebook more \n",
    "expensive than the TechPro desktop?\" \n",
    "\n",
    "So, let's take a look at these two products. \n",
    "The BlueWave Chromebook is 249.99. \n",
    "And the TechPro desktop is actually 999.99. This is not actually true. \n",
    "And so, let's see how the model handles this user request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e66beb-8fb5-4c7b-afa7-13d20ded1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to answer the customer queries.\n",
    "The customer query will be delimited with four hashtags,\\\n",
    "i.e. {delimiter}. \n",
    "\n",
    "Step 1:{delimiter} First decide whether the user is \\\n",
    "asking a question about a specific product or products. \\\n",
    "Product cateogry doesn't count. \n",
    "\n",
    "Step 2:{delimiter} If the user is asking about \\\n",
    "specific products, identify whether \\\n",
    "the products are in the following list.\n",
    "All available products: \n",
    "1. Product: TechPro Ultrabook\n",
    "   Category: Computers and Laptops\n",
    "   Brand: TechPro\n",
    "   Model Number: TP-UB100\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.5\n",
    "   Features: 13.3-inch display, 8GB RAM, 256GB SSD, Intel Core i5 processor\n",
    "   Description: A sleek and lightweight ultrabook for everyday use.\n",
    "   Price: $799.99\n",
    "\n",
    "2. Product: BlueWave Gaming Laptop\n",
    "   Category: Computers and Laptops\n",
    "   Brand: BlueWave\n",
    "   Model Number: BW-GL200\n",
    "   Warranty: 2 years\n",
    "   Rating: 4.7\n",
    "   Features: 15.6-inch display, 16GB RAM, 512GB SSD, NVIDIA GeForce RTX 3060\n",
    "   Description: A high-performance gaming laptop for an immersive experience.\n",
    "   Price: $1199.99\n",
    "\n",
    "3. Product: PowerLite Convertible\n",
    "   Category: Computers and Laptops\n",
    "   Brand: PowerLite\n",
    "   Model Number: PL-CV300\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.3\n",
    "   Features: 14-inch touchscreen, 8GB RAM, 256GB SSD, 360-degree hinge\n",
    "   Description: A versatile convertible laptop with a responsive touchscreen.\n",
    "   Price: $699.99\n",
    "\n",
    "4. Product: TechPro Desktop\n",
    "   Category: Computers and Laptops\n",
    "   Brand: TechPro\n",
    "   Model Number: TP-DT500\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.4\n",
    "   Features: Intel Core i7 processor, 16GB RAM, 1TB HDD, NVIDIA GeForce GTX 1660\n",
    "   Description: A powerful desktop computer for work and play.\n",
    "   Price: $999.99\n",
    "\n",
    "5. Product: BlueWave Chromebook\n",
    "   Category: Computers and Laptops\n",
    "   Brand: BlueWave\n",
    "   Model Number: BW-CB100\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.1\n",
    "   Features: 11.6-inch display, 4GB RAM, 32GB eMMC, Chrome OS\n",
    "   Description: A compact and affordable Chromebook for everyday tasks.\n",
    "   Price: $249.99\n",
    "\n",
    "Step 3:{delimiter} If the message contains products \\\n",
    "in the list above, list any assumptions that the \\\n",
    "user is making in their \\\n",
    "message e.g. that Laptop X is bigger than \\\n",
    "Laptop Y, or that Laptop Z has a 2 year warranty.\n",
    "\n",
    "Step 4:{delimiter}: If the user made any assumptions, \\\n",
    "figure out whether the assumption is true based on your \\\n",
    "product information. \n",
    "\n",
    "Step 5:{delimiter}: First, politely correct the \\\n",
    "customer's incorrect assumptions if applicable. \\\n",
    "Only mention or reference products in the list of \\\n",
    "5 available products, as these are the only 5 \\\n",
    "products that the store sells. \\\n",
    "Answer the customer in a friendly tone.\n",
    "\n",
    "Use the following format:\n",
    "Step 1:{delimiter} <step 1 reasoning>\n",
    "Step 2:{delimiter} <step 2 reasoning>\n",
    "Step 3:{delimiter} <step 3 reasoning>\n",
    "Step 4:{delimiter} <step 4 reasoning>\n",
    "Response to user:{delimiter} <response to customer>\n",
    "\n",
    "Make sure to include {delimiter} to separate every step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772a3b3-4230-4cfc-99a3-467ff186aa47",
   "metadata": {},
   "source": [
    "We will now format our messages array. And we will receive a response and we'll print it after that. \n",
    "We are hoping that after going through all of these phases, the model will recognise that the user has made a false assumption and then proceed to the final step to correct the user. \n",
    "So, we've actually kept track of a variety of complex states that the system might be in inside the confines of this one request. We also know the output from the preceding phase could change at any time, and we would want to take a different action. \n",
    "We wouldn't have any output in step 4 for instance, if the user hadn't made any assumptions in step 3. \n",
    "\n",
    "So this is a pretty complicated instruction for \n",
    "the model. So let's see if it did it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be1ea0a-a816-4694-8a79-77d985f2e274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### The user is asking a question about two specific products, the BlueWave Chromebook and the TechPro Desktop.\n",
      "Step 2:#### The prices of the two products are as follows:\n",
      "- BlueWave Chromebook: $249.99\n",
      "- TechPro Desktop: $999.99\n",
      "Step 3:#### The user is assuming that the BlueWave Chromebook is more expensive than the TechPro Desktop.\n",
      "Step 4:#### The assumption is incorrect. The TechPro Desktop is actually more expensive than the BlueWave Chromebook.\n",
      "Response to user:#### The BlueWave Chromebook is actually less expensive than the TechPro Desktop. The BlueWave Chromebook costs $249.99, while the TechPro Desktop costs $999.99.\n"
     ]
    }
   ],
   "source": [
    "user_message = f\"\"\"\n",
    "by how much is the BlueWave Chromebook more expensive \\\n",
    "than the TechPro Desktop\"\"\"\n",
    "\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4da134-3d47-4704-8346-1f6c3bc8bd29",
   "metadata": {},
   "source": [
    "In the first stage, the user is requesting information about particular products. They want to know how much these two products cost different from one another. \n",
    " \n",
    "It is wrong for the user to infer that the BlueWave Chromebook is more expensive than the TechBook Pro. The process of thinking through the issue is taking longer. \n",
    "The model performs better if it has time to reflect, much as a human would spend some time to consider an answer to any given topic. \n",
    "The BlueWave Chromebook is really less expensive than the TechBook Pro, which is the user's final comment. The BlueWave Chromebook is $249.99 cheaper than the TechBook Pro desktop, which costs $999.99. \n",
    " \n",
    "So let's look at yet another user message sample. \n",
    "\n",
    "So, let's format this user message. The query is:\n",
    "\n",
    ">\"Do you sell TVs?\" \n",
    "\n",
    "And if you recall, we merely included various PCs in our product list. Let's see what the model suggests. \n",
    "Since TVs aren't listed among the products available, the user in this scenario is enquiring if the store sells them in step one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51afe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### The user is asking about a specific product category, TVs.\n",
      "\n",
      "Step 2:#### The list of available products does not include any TVs.\n",
      "\n",
      "Response to user:#### I'm sorry, but we do not sell TVs at this time. Our store specializes in computers and laptops. However, if you are interested in purchasing a computer or laptop, please let me know and I would be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "user_message = f\"\"\"\n",
    "do you sell tvs\"\"\"\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552a4f6-5e65-4d85-9579-5263f720aa10",
   "metadata": {},
   "source": [
    "As you can see, the model then proceeds directly to the phase where it responds to the user after realising that the intermediary processes are not truly required. \n",
    "We did request the output in this particular format. So, in a strict sense, the model has not complied with our request. \n",
    "Once more, more sophisticated models like GPT4 will be more adept at doing that. \n",
    "We apologise, but we do not sell TVs at the store, is our response to the user in this instance. \n",
    " \n",
    "The products that are available are then listed. And so now, we only really want this part of the response. \n",
    "\n",
    "## Inner Monologue\n",
    "\n",
    "The user wouldn't want to see the earlier sections. In order to only print the final portion of the model output, we can simply cut the string at the last instance of this delimiter token or string of four hashtags. \n",
    "So, let's develop some code to only retrieve the last portion of this text. So, we'll use a try unless block. \n",
    " \n",
    "In case the model produces unexpected results and doesn't actually need these characters, to gracefully handle failures. \n",
    "We will state that our final response is the response before splitting the string at the delimiter string. \n",
    "\n",
    "We only want the final item in the output list because we are looking for the last instance, after that, we'll remove any blank space - since the characters may be followed by white space, as you can see. Then we're going to catch any errors and have a \n",
    "fallback response which is, \"Sorry, I'm having trouble right now. Please try \n",
    "asking another question.\". \n",
    "\n",
    "Since we asked the LLM to separate its reasoning steps by a delimiter, we can hide the chain-of-thought reasoning from the final output that the user sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a825237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but we do not sell TVs at this time. Our store specializes in computers and laptops. However, if you are interested in purchasing a computer or laptop, please let me know and I would be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    final_response = response.split(delimiter)[-1].strip()\n",
    "except Exception as e:\n",
    "    final_response = \"Sorry, I'm having trouble right now, please try asking another question.\"\n",
    "    \n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736fbdb-573d-4b5d-b35c-cd49f22c53ad",
   "metadata": {},
   "source": [
    "As you can see, we just cut the string to produce this output. And so, if we were to incorporate this into an application, this is what the user would see. \n",
    "Also, this task's request may be a little too complicated overall. It's possible that none of these intermediary stages are actually necessary. \n",
    "And in general, considerable trial and error testing is needed to identify the best trade-off in prompt complexity. \n",
    " \n",
    "So, it is wise to experiment with a variety of prompts before choosing one. \n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [Building Systems with the ChatGPT API Course](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/) by DeepLearning.ai and OpenAI - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde5a55-b3fb-4653-811e-a5832fc45346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
