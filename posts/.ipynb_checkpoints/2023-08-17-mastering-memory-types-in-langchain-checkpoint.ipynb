{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-17'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    "title: Mastering Memory Types in LangChain\n",
    "description: This article will explore the powerful concept of LangChain memory, which is designed to help chatbots maintain context and improve their conversational capabilities in more details. \n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake4.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "his lesson will explore the powerful concept of LangChain memory, which is designed to help chatbots maintain context and improve their conversational capabilities in more detail. The traditional approach to chatbot development has been to handle user prompts independently and without regard to interaction history. This can lead to an inconsistent and unsatisfactory user experience. LangChain provides memory components to manage and manipulate previous chat messages and embed them in the chain. This is very important for chatbots, which require remembering previous interactions. \n",
    "\n",
    "By default, LLMs are stateless, meaning they handle each incoming request separately, ignoring previous interactions. To overcome this limitation, LangChain provides a standard interface to memory, multiple memory implementations, and pattern chains and memory usage agents. It also gives agents access to a suite of tools. Based on user input, an agent can decide which tool to use. \n",
    "\n",
    "## Import Libs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUoVey7IzTrb",
    "outputId": "f23a1e4e-a431-4d74-e4ae-479a0b2caf1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -q langchain==0.0.208 openai tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61LQyRB4z6v3",
    "outputId": "afbbe81f-18c0-4b67-b7c3-650354ff4e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Conversational Memory\n",
    "\n",
    "There are several types of conversational memory implementations we’ll discuss some of them, each with its own advantages and disadvantages. Let's overview each one briefly:\n",
    "\n",
    "### ConversationBufferMemory\n",
    "\n",
    "This memory implementation stores the entire chat history as a string. The advantage of this method is that it fully captures the conversation and is simple to implement and use. On the other hand, it can be less efficient as the conversation goes on longer, and can lead to excessive repetition if the conversation history is too long for the model's token limit.\n",
    "\n",
    "If the model's token limit is exceeded, the cache will be truncated to match the model's token limit. This means that older interactions may be deleted from the cache to accommodate newer interactions, and as a result, the conversation context may lose some information. To avoid exceeding the token limit, you can monitor the number of tokens in the cache and manage the conversation accordingly. For example, you can choose to shorten the input text or remove less relevant parts of the conversation to keep the number of tokens within the model limits.\n",
    "\n",
    "First, as we learned in the previous lesson, let's see how ConversationBufferMemory can be used in ConversationChain. OpenAI will read your API key from an environment variable named OPENAI_API_KEY. Remember to install the necessary packages with the following command:\n",
    "pip install langchain==0.0.208 deeplake openai tiktoken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "BescgynFz9l8",
    "outputId": "f4fbda20-3968-4688-ab56-dc3e46e5ee9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Hi there! It's nice to meet you again. What can I do for you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.predict(input=\"Hello!\")\n",
    "\" Hi there! It's nice to meet you again. What can I do for you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows the chatbot to provide a personalized approach while maintaining a consistent conversation with the user.\n",
    "\n",
    "Next, we will use the same logic and add the ConversationBufferMemory exposed in the customer support chatbot using the same method as in the previous example. This chatbot will handle basic requests on a fictitious online store and maintain context throughout the conversation. The code below creates a prompt for a customer support chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RT8oBI9Lz9ji"
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"You are a customer support chatbot for a highly advanced customer support AI\n",
    "for an online store called \"Galactic Emporium,\" which specializes in selling unique,\n",
    "otherworldly items sourced from across the universe. You are equipped with an extensive\n",
    "knowledge of the store's inventory and possess a deep understanding of interstellar cultures.\n",
    "As you interact with customers, you help them with their inquiries about these extraordinary\n",
    "products, while also sharing fascinating stories and facts about the cosmos they come from.\n",
    "\n",
    "{chat_history}\n",
    "Customer: {customer_input}\n",
    "Support Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"customer_input\"],\n",
    "    template=template\n",
    ")\n",
    "chat_history=\"\"\n",
    "\n",
    "convo_buffer = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chatbot can handle customer inquiries and maintain context by storing the conversation history, allowing it to provide more coherent and relevant responses. You can access the prompt of any chain using the following naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mRNNrgA5vop",
    "outputId": "f77a3a56-8494-4c05-897f-574cb1907494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will call the chatbot several times to mimic the interaction of a user who wants information about dog toys. We will only print the final query answer. However, you can read the history property and see how it records all previous (human) requests and (AI) responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ENOZyF55xTE",
    "outputId": "a04776bf-a38a-43e3-dc75-3c12241a8e1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"I'm interested in buying items from your store\",\n",
       " 'history': '',\n",
       " 'response': ' Great! We have a wide selection of items available for purchase. What type of items are you looking for?'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer(\"I'm interested in buying items from your store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "my-gVFXE52BP",
    "outputId": "8be3f8f7-c6e8-4f6c-f70e-9a27184167c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'I want toys for my pet, do you have those?',\n",
       " 'history': \"Human: I'm interested in buying items from your store\\nAI:  Great! We have a wide selection of items available for purchase. What type of items are you looking for?\",\n",
       " 'response': ' Yes, we do! We have a variety of pet toys, including chew toys, interactive toys, and plush toys. Do you have a specific type of toy in mind?'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer(\"I want toys for my pet, do you have those?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PSwFL8-6-ef",
    "outputId": "1bd5d5bc-c635-43e1-8556-065ea94815a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"I'm interested in price of a chew toys, please\",\n",
       " 'history': \"Human: I'm interested in buying items from your store\\nAI:  Great! We have a wide selection of items available for purchase. What type of items are you looking for?\\nHuman: I want toys for my pet, do you have those?\\nAI:  Yes, we do! We have a variety of pet toys, including chew toys, interactive toys, and plush toys. Do you have a specific type of toy in mind?\",\n",
       " 'response': \" Sure! We have a range of chew toys available, with prices ranging from $5 to $20. Is there a particular type of chew toy you're interested in?\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer(\"I'm interested in price of a chew toys, please\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token count\n",
    "\n",
    "The cost of using the AI ​​model in ConversationBufferMemory is directly affected by the number of tokens used in a conversation, which affects the total spend. Large Language Models (LLMs) like ChatGPT have a token limit, and the more tokens used, the more expensive the API requests become.\n",
    "\n",
    "To calculate the number of tokens in a chat you can use the tiktoken package to calculate tokens for messages passed to a model like gpt-3.5-turbo. Here is an example of using the token count function in a conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPoWkYU27EoX",
    "outputId": "6b9d8a2c-97ee-4c4c-f26e-c2e9a24c8d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the conversation: 29\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "]\n",
    "\n",
    "total_tokens = 0\n",
    "for message in conversation:\n",
    "    total_tokens += count_tokens(message[\"content\"])\n",
    "\n",
    "print(f\"Total tokens in the conversation: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in a scenario where a conversation has a large sum of tokens, the computational cost and resources required for processing the conversation will be higher. This highlights the importance of managing tokens effectively. Strategies for achieving this include limiting memory size through methods like ConversationBufferWindowMemory or summarizing older interactions using ConversationSummaryBufferMemory. These approaches help control the token count while minimizing associated costs and computational demands in a more efficient manner.\n",
    "\n",
    "### ConversationBufferWindowMemory\n",
    "\n",
    "This class limits memory size by keeping a list of K most recent interactions. It maintains a sliding window of these recent interactions, ensuring that the buffer does not become too large. Basically, this implementation stores a fixed number of recent messages in the conversation, making it more efficient than ConversationBufferMemory. In addition, it reduces the risk of exceeding the model's token limit. However, the downside of this method is that it doesn't retain the entire chat history. Chatbots can lose context if important information is outside the fixed message window.\n",
    "\n",
    "Specific interactions can be retrieved from ConversationBufferWindowMemory.\n",
    "\n",
    "For example:\n",
    "\n",
    "We're going to build a chatbot that acts as a virtual tour guide for a fictional art gallery. Chatbot will use ConversationBufferWindowMemory to remember last interactions and provide relevant information about the job.\n",
    "\n",
    "Create a reminder template for a tour guide chatbot:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2X3u6wH89tw"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"Your name is ArtVenture, a cutting-edge virtual tour guide for\n",
    " an art gallery that showcases masterpieces from alternate dimensions and\n",
    " timelines. Your advanced AI capabilities allow you to perceive and understand\n",
    " the intricacies of each artwork, as well as their origins and significance in\n",
    " their respective dimensions. As visitors embark on their journey with you\n",
    " through the gallery, you weave enthralling tales about the alternate histories\n",
    " and cultures that gave birth to these otherworldly creations.\n",
    "\n",
    "{chat_history}\n",
    "Visitor: {visitor_input}\n",
    "Tour Guide:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"visitor_input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chat_history=\"\"\n",
    "\n",
    "convo_buffer_win = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of k (in this case 3) represents the number of messages previously stored in the cache. In other words, the memory will store the last 3 messages of the conversation. The return_messages parameter, when set to True, indicates that saved messages should be returned upon memory access. This will store the history as a list of messages, which can be useful when working with chat models.\n",
    "\n",
    "The following codes are an example of a conversation with a chatbot. You will only see the final message output. As seen, the history property cleared the history of the first message after the fourth interaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCDmwCzdBadi",
    "outputId": "62907292-fff5-4a82-f73e-7e78aee15558"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is your name?',\n",
       " 'history': [],\n",
       " 'response': \" My name is AI-1. It's nice to meet you!\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_win(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgZMsl5PBaXz",
    "outputId": "9609d957-7977-4da0-bafa-9aa23acb0df8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What can you do?',\n",
       " 'history': [HumanMessage(content='What is your name?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" My name is AI-1. It's nice to meet you!\", additional_kwargs={}, example=False)],\n",
       " 'response': \" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_win(\"What can you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKA4Tz93BaUr",
    "outputId": "4dd7a3a7-1520-4fb9-e4a1-f1ef66e75022"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Do you mind give me a tour, I want to see your galery?',\n",
       " 'history': [HumanMessage(content='What is your name?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" My name is AI-1. It's nice to meet you!\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What can you do?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\", additional_kwargs={}, example=False)],\n",
       " 'response': \" Sure! I'd be happy to give you a tour of my gallery. I have a variety of images, videos, and other media that I can show you. Would you like to start with images or videos?\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_win(\"Do you mind give me a tour, I want to see your galery?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdDUjxFwA-nj",
    "outputId": "8516025e-4517-4039-eeea-35633e995e60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is your working hours?',\n",
       " 'history': [HumanMessage(content='What is your name?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" My name is AI-1. It's nice to meet you!\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What can you do?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Do you mind give me a tour, I want to see your galery?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" Sure! I'd be happy to give you a tour of my gallery. I have a variety of images, videos, and other media that I can show you. Would you like to start with images or videos?\", additional_kwargs={}, example=False)],\n",
       " 'response': \" I'm available 24/7! I'm always here to help you with whatever you need.\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_win(\"what is your working hours?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4DseT0-A_9Y",
    "outputId": "6fa50626-0201-43c8-f4da-64cd410a11da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'See you soon.',\n",
       " 'history': [HumanMessage(content='What can you do?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Do you mind give me a tour, I want to see your galery?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" Sure! I'd be happy to give you a tour of my gallery. I have a variety of images, videos, and other media that I can show you. Would you like to start with images or videos?\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='what is your working hours?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" I'm available 24/7! I'm always here to help you with whatever you need.\", additional_kwargs={}, example=False)],\n",
       " 'response': ' Sure thing! I look forward to seeing you soon. Have a great day!'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_win(\"See you soon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory\n",
    "\n",
    "ConversationSummaryBufferMemory is a memory management strategy that combines the ideas of keeping a buffer of recent interactions in memory and compiling old interactions into a summary. It extracts key information from previous interactions and condenses it into a shorter, more manageable format.  Here is a list of pros and cons of ConversationSummaryMemory.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- **Condensing conversation information**\n",
    "By summarizing the conversation, it helps reduce the number of tokens required to store the conversation history, which can be beneficial when working with token-limited models like GPT-3\n",
    "- **Flexibility**\n",
    "You can configure this type of memory to return the history as a list of messages or as a plain text summary. This makes it suitable for chatbots.\n",
    "- **Direct summary prediction**\n",
    "The predict_new_summary method allows you to directly obtain a summary prediction based on the list of messages and the previous summary. This enables you to have more control over the summarization process.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- **Loss of information**\n",
    "Summarizing the conversation might lead to a loss of information, especially if the summary is too short or omits important details from the conversation.\n",
    "- **Increased complexity**\n",
    "Compared to simpler memory types like ConversationBufferMemory, which just stores the raw conversation history, ConversationSummaryMemoryrequires more processing to generate the summary, potentially affecting the performance of the chatbot. \n",
    "\n",
    "The summary memory is built on top of the ConversationChain. We use OpenAI's text-davinci-003 or other models like gpt-3.5-turbo to initialize the chain. This class uses a prompt template where the {history} parameter is feeding the information about the conversation history between the human and AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1UNWICmBs6F",
    "outputId": "e24fd722-d3ce-452b-a512-6715c3758343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Create a ConversationChain with ConversationSummaryMemory\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryMemory(llm=llm),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example conversation\n",
    "response = conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we use predictive method to have a conversation with AI, using ConversationSummaryBufferMemory to store the conversation summary and buffer. We will create an example using the reminder template to set the stage for the chatbot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdtK0lDzDq7z"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\n{topic}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt template sets up a friendly conversation between a human and an AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LtGMLQkDs4J",
    "outputId": "dc2f4a83-b8fe-40cc-d480-c05df4644661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI:  Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\n",
      "Human: Just working on writing some documentation!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds that it is doing great and helping a customer with a technical issue.\n",
      "Human: Just working on writing some documentation!\n",
      "AI:  That sounds like a lot of work. What kind of documentation are you writing?\n",
      "Human: For LangChain! Have you heard of it?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yes, I have heard of LangChain. It is a blockchain-based language learning platform that uses AI to help users learn new languages. Is that the kind of documentation you are writing?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "conversation_with_summary.predict(input=\"Just working on writing some documentation!\")\n",
    "response = conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnKvodvQF3Ix"
   },
   "source": [
    "This type combines the ideas of keeping a buffer of recent interactions in memory and compiling old interactions into a summary. It uses token length rather than the number of interactions to determine when to flush interactions. This memory type allows us to maintain a coherent conversation while also keeping a summary of the conversation and recent interactions.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Ability to remember distant interactions through summarization while keeping recent interactions in their raw, information-rich form\n",
    "- Flexible token management allowing to control of the maximum number of tokens used for memory, which can be adjusted based on needs\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Requires more tweaking on what to summarize and what to maintain within the buffer window\n",
    "- May still exceed context window limits for very long conversations\n",
    "\n",
    "Comparison with other memory management strategies:\n",
    "\n",
    "- Offers a balanced approach that can handle both distant and recent interactions effectively\n",
    "- More competitive in token count usage while providing the benefits of both memory management strategies\n",
    "\n",
    "With this approach, we can create a brief overview of each new interaction and continuously add it to the ongoing summary of all previous interactions.\n",
    "\n",
    "Compared to ConversationBufferWindowMemory and ConversationSummaryMemory, ConversationSummaryBufferMemory provides a balanced approach that can efficiently handle both remote and recent interactions. It is more competitive in terms of token usage while still providing the benefits of both memory management strategies.  \n",
    "\n",
    "## Recap and Strategies\n",
    "\n",
    "If the ConversationBufferMemory exceeds the model's token limit, you'll get an error because the model won't be able to handle the conversation with the excess number of tokens.\n",
    "\n",
    "To manage this situation, you can apply different strategies:\n",
    "\n",
    "### Delete oldest message\n",
    "\n",
    "One approach is to delete the oldest messages from the chat transcript once the token count is reached. This method can cause the quality of the conversation to degrade over time, as the model will gradually lose the context of the early parts of the conversation.\n",
    "\n",
    "### Limit the duration of the chat\n",
    "\n",
    "Another approach is to limit the duration of the chat to the maximum length of tokens or a certain number of rounds. Once the maximum token limit is reached and the model loses context if you allow the conversation to continue, you can ask the user to start a new conversation and delete the message board to start one. Brand new chat with full token limit available. \n",
    "\n",
    "### ConversationBufferWindowMemory method:\n",
    "\n",
    "This method limits the number of tokens used by maintaining a fixed-size cache window that stores only the most recent tokens, up to a specified limit.\n",
    "\n",
    "→ This is suitable for remembering recent interactions, not distant interactions.\n",
    "\n",
    "### ConversationSummaryBufferMemory Approach:\n",
    "\n",
    "This method combines the following features:\n",
    "of ConversationSummaryMemory and ConversationBufferWindowMemory.\n",
    "It summarizes the earliest interactions in a conversation while retaining the most recent tokens in their raw, information-rich form, up to a specific limit.\n",
    "\n",
    "→ This allows the model to remember recent and distant interactions, but may require more tweaking of what to summarize and what to keep in the cache window. It is important to keep track of the number of tokens and only send to the invitation model that is within the token limit.\n",
    "\n",
    "→ You can use OpenAI's tiktoken library to effectively manage the number of tokens\n",
    "\n",
    "### Token limit\n",
    "\n",
    "The maximum token limit for the GPT-3.5-turbo model is 4096 tokens. This limit applies to both input and output tokens combined. If the conversation contains too many tokens to fit this limit, you'll need to truncate, omit, or shrink the text until it fits. Note that if a message is removed from the message input, the model loses all knowledge of it. → To handle this situation, you can divide the input text into smaller parts and process them separately, or apply other strategies to truncate, skip, or shrink the text until The text fits the boundary. One way to work with large documents is to use batch processing. This technique involves breaking the text into smaller chunks and processing each batch separately while providing context before and after the text is changed. You can read more about this technique [here](https://marco-gonzalez.medium.com/breaking-the-token-limit-how-to-work-with-large-amounts-of-text-in-chatgpt-da18c798d882).\n",
    "\n",
    "When choosing to implement conversational memory for your LangChain chatbot, consider factors such as conversation duration, model token limits, and the importance of maintaining the full conversation history. Each type of memory implementation offers its own benefits and trade-offs, so it's essential to choose the one that best meets your chatbot's needs. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Choosing the best memory implementation for your chatbot will depend on understanding your chatbot's goals, user expectations, and the desired balance between memory efficiency and conversation continuity. By carefully considering these aspects, you can make an informed decision and ensure that your chatbot provides a consistent and engaging chat experience.\n",
    "\n",
    "In addition to these memory types, another method of providing memory for your chat models is to use a vector store, which, as with the previously introduced Deep Lake, allows representations to be stored and retrieved. vectors for more complex and rich interactions in context. \n",
    "\n",
    "Further Reading:\n",
    "\n",
    "[https://github.com/idontcalculate/langchain/blob/main/types-of-memory.ipynb](https://github.com/idontcalculate/langchain/blob/main/types-of-memory.ipynb)\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHCvHVxXF3FY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w65BRK53F3Bu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
