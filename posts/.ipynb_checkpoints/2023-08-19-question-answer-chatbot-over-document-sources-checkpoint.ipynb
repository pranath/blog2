{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-19'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    "title: Build a Question Answering Chatbot over Documents with Sources\n",
    "description: In this project we will look into the more advanced application of building a Question Answering (QA) Chatbot that works over documents and also provides credible sources of information for its answers\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake2.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's explore a more advanced application of artificial intelligence - creating a question-answering (QA) chatbot that works on a document and feeds its answers. Our QA chatbot uses a chain (specifically, RetrievalQAWithSourcesChain) and leverages it to sift through a set of documents, extracting relevant information to answer queries.\n",
    "\n",
    "The thread sends a structured prompt to the underlying language model to generate a response. These prompts are designed to guide language modeling, thereby improving the quality and relevance of responses. In addition, the recovery chain is designed to keep track of the sources of information it fetches to provide answers, providing the ability to back up its answers with trusted references. As we go, we will learn how to:\n",
    "\n",
    "1. Scan online articles and store the text content and URL of each article.\n",
    "2. Use the embedding model to calculate the embeddings of these documents and store them in Deep Lake, a vector database.\n",
    "3. Divide the article text into smaller sections, keeping track of the origin of each section.\n",
    "4. Use RetrievalQAWithSourcesChain to create a chatbot that retrieves responses and tracks their source.\n",
    "5. Generate a response to the query using a string and display the response with its source. \n",
    "\n",
    "This knowledge is transformative, allowing you to create intelligent chatbots that can answer questions with derived information, increasing the reliability and usefulness of the chatbot. \n",
    "\n",
    "## Import Libs & Setup\n",
    "\n",
    "Remember to install the required packages with the following command: pip install langchain==0.0.208 deeplake openai tiktoken. Additionally, install the newspaper3k package with version 0.2.8.\n",
    "\n",
    "Then, you need to add your OpenAI and Deep Lake API keys to the environment variables. The LangChain library will read the tokens and use them in the integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15BW5LkgIv0T",
    "outputId": "1161b679-2ffb-41a9-83fa-c3bbb4803821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -q langchain==0.0.208 deeplake openai tiktoken python-dotenv newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSFtehNoLP8C",
    "outputId": "68bd354a-9484-487d-8bad-54c9033e9859"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
    "!echo \"ACTIVELOOP_TOKEN='<ACTIVELOOP_TOKEN>'\" >> .env\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping for the News\n",
    "\n",
    "Now, let's begin by fetching some articles related to AI news. We're particularly interested in the text content of each article and the URL where it was published.\n",
    "\n",
    "In the code, you’ll see the following:\n",
    "\n",
    "- **Imports:** We begin by importing necessary Python libraries. requests are used to send HTTP requests, the newspaper is a fantastic tool for extracting and curating articles from a webpage, and time will help us introduce pauses during our web scraping task.\n",
    "- **Headers:** Some websites may block requests without a proper User-Agent header as they may consider it as a bot's action. Here we define a User-Agent string to mimic a real browser's request.\n",
    "- **Article URLs:** We have a list of URLs for online articles related to artificial intelligence news that we wish to scrape.\n",
    "- **Web Scraping:** We create an HTTP session using requests.Session() allows us to make multiple requests within the same session. We also define an empty list of pages_content to store our scraped articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11eE6zWCL0Xf"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from newspaper import Article # https://github.com/codelucas/newspaper\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_urls = [\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/16/openai-ceo-ai-regulation-is-essential/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/15/jay-migliaccio-ibm-watson-on-leveraging-ai-to-improve-productivity/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/15/iurii-milovanov-softserve-how-ai-ml-is-helping-boost-innovation-and-personalisation/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/11/ai-and-big-data-expo-north-america-begins-in-less-than-one-week/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/02/ai-godfather-warns-dangers-and-quits-google/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/04/28/palantir-demos-how-ai-can-used-military/\"\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "pages_content = [] # where we save the scraped articles\n",
    "\n",
    "for url in article_urls:\n",
    "    try:\n",
    "        time.sleep(2) # sleep two seconds for gentle scraping\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            article = Article(url)\n",
    "            article.download() # download HTML of webpage\n",
    "            article.parse() # parse HTML to extract the article text\n",
    "            pages_content.append({ \"url\": url, \"text\": article.text })\n",
    "        else:\n",
    "            print(f\"Failed to fetch article at {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching article at {url}: {e}\")\n",
    "\n",
    "#If an error occurs while fetching an article, we catch the exception and print\n",
    "#an error message. This ensures that even if one article fails to download,\n",
    "#the rest of the articles can still be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the embeddings of the document using the embedding model and store them in Deep Lake, a multimodal vector database. OpenAIEmbeddings will be used to create vector representations of our documents. These embeddings are height vectors that capture the semantic content of the document. When we create an instance of the Deep Lake class, we provide a path starting with the center: hub://... Specifies the name of the database to be stored in the cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFEMWNhjMFIA",
    "outputId": "b2ed20e4-ca1b-499d-dad0-180bfcd56b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "The dataset is private so make sure you are logged in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/ala/langchain_course_qabot_with_source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://ala/langchain_course_qabot_with_source loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "my_activeloop_org_id = \"<Your_Organization_Id>\" # TODO: use your organization id here\n",
    "my_activeloop_dataset_name = \"langchain_course_qabot_with_source\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important part of the setup process as it prepares the system to store and retrieve documents based on their semantic content. This functionality is essential for the next steps where we will find the most relevant documents to answer the user's question.\n",
    "\n",
    "Next, we'll split these articles into smaller sections, and for each section, we'll save its corresponding URL as the source. This division makes data processing efficient, makes retrieval tasks more manageable, and focuses on the most relevant passages of text when answering questions.\n",
    "\n",
    "Recursive CharacterTextSplitter is created with block size of 1000 and 100 characters overlap between blocks. The chunk_size parameter specifies the length of each block of text, while chunk_overlap determines the number of characters shared by contiguous blocks. For each document in pages_content, the text will be split into sections using the .split_text() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCHYtEdeNATo"
   },
   "outputs": [],
   "source": [
    "# We split the article texts into small chunks. While doing so, we keep track of each\n",
    "# chunk metadata (i.e. the URL where it comes from). Each metadata is a dictionary and\n",
    "# we need to use the \"source\" key for the document source so that we can then use the\n",
    "# RetrievalQAWithSourcesChain class which will automatically retrieve the \"source\" item\n",
    "# from the metadata dictionary.\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "all_texts, all_metadatas = [], []\n",
    "for d in pages_content:\n",
    "    chunks = text_splitter.split_text(d[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        all_texts.append(chunk)\n",
    "        all_metadatas.append({ \"source\": d[\"url\"] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"source\" key is used in the metadata dictionary to match the expectations of the RetrievalQAWithSourcesChain class, which will automatically retrieve this \"source\" item from the metadata. We then add these blocks to the Deep Lake database along with their respective metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jt7SGYYcNlsS",
    "outputId": "06762267-cca6-4489-c064-2abae4583330"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|██████████| 1/1 [00:21<00:00\n",
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://ala/langchain_course_qabot_with_source', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape      dtype  compression\n",
      "  -------   -------   -------    -------  ------- \n",
      " embedding  generic  (49, 1536)  float32   None   \n",
      "    ids      text     (49, 1)      str     None   \n",
      " metadata    json     (49, 1)      str     None   \n",
      "   text      text     (49, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a9ac1d0c-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac1ece-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac1faa-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2054-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac20f4-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2180-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2216-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac22a2-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2338-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac23ce-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac245a-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac24e6-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2572-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2608-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2694-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac272a-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac27c0-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac284c-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac28ce-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2964-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac29e6-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2a72-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2b08-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2b94-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2c20-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2cc0-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2d4c-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2dd8-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2e64-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2efa-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac2f86-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac301c-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac30ee-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac318e-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac3224-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac32b0-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac3346-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac344a-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac34f4-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac3580-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac3602-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac368e-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac371a-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac37a6-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac3832-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac38b4-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac3940-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac39cc-ffe6-11ed-8434-0242ac1c000c',\n",
       " 'a9ac3a4e-ffe6-11ed-8434-0242ac1c000c']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we add all the chunks to the deep lake, along with their metadata\n",
    "db.add_texts(all_texts, all_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the fun part - building the QA Chatbot. We'll create a RetrievalQAWithSourcesChain chain that not only retrieves relevant document snippets to answer the questions but also keeps track of the sources of these documents.\n",
    "\n",
    "## Setting up the Chain \n",
    "\n",
    "We then create an instance of RetrievalQAWithSourcesChain using the from_chain_type method. This method takes the following parameters:\n",
    "\n",
    "- **LLM:** This argument expects to receive an instance of a model (GPT-3, in this case) with a temperature of 0. The temperature controls the randomness of the model's outputs - a higher temperature results in more randomness, while a lower temperature makes the outputs more deterministic.\n",
    "- **chain_type=\"stuff\":** This defines the type of chain being used, which influences how the model processes the retrieved documents and generates responses. \n",
    "- **retriever=db.as_retriever():** This sets up the retriever that will fetch the relevant documents from the Deep Lake database. Here, the Deep Lake database instance db is converted into a retriever using its as_retriever method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcVe2PI_No7b"
   },
   "outputs": [],
   "source": [
    "# we create a RetrievalQAWithSourcesChain chain, which is very similar to a\n",
    "# standard retrieval QA chain but it also keeps track of the sources of the\n",
    "# retrieved documents\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm,\n",
    "                                                    chain_type=\"stuff\",\n",
    "                                                    retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll generate a response to a question using the chain. The response includes the answer and its corresponding sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_KGRTNkOT6I",
    "outputId": "96f3e90d-7f92-4aca-a6ee-9d68a655dd3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Geoffrey Hinton has expressed concerns about the potential dangers of AI, such as false text, images, and videos created by AI, and the impact of AI on the job market. He believes that AI has the potential to replace humans as the dominant species on Earth.\n",
      "\n",
      "Sources:\n",
      "- https://www.artificialintelligence-news.com/2023/05/02/ai-godfather-warns-dangers-and-quits-google/\n",
      "- https://www.artificialintelligence-news.com/2023/05/15/iurii-milovanov-softserve-how-ai-ml-is-helping-boost-innovation-and-personalisation/\n"
     ]
    }
   ],
   "source": [
    "# We generate a response to a query using the chain. The response object is a dictionary containing\n",
    "# an \"answer\" field with the textual answer to the query, and a \"sources\" field containing a string made\n",
    "# of the concatenation of the metadata[\"source\"] strings of the retrieved documents.\n",
    "d_response = chain({\"question\": \"What does Geoffrey Hinton think about recent trends in AI?\"})\n",
    "\n",
    "print(\"Response:\")\n",
    "print(d_response[\"answer\"])\n",
    "print(\"Sources:\")\n",
    "for source in d_response[\"sources\"].split(\", \"):\n",
    "    print(\"- \" + source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEcsDsp6OkbE"
   },
   "source": [
    "That's it! You have now built a Q&A chatbot that can provide answers from a collection of documents and where it gets information. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The chatbot was able to answer the question, giving a brief insight into Geoffrey Hinton's views on recent AI trends. Sources provided and feedback go back to original articles expressing these views. This process adds an extra layer of reliability and traceability to the chatbot response. The presence of multiple sources also shows that the chatbot can pull information from various documents to give a complete answer, proving the effectiveness of RetrievalQAWithSourcesChain in retrieving information. \n",
    "\n",
    "Further Reading:\n",
    "\n",
    "[https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa)\n",
    "\n",
    "[https://python.langchain.com/docs/integrations/vectorstores/activeloop_deeplake](https://python.langchain.com/docs/integrations/vectorstores/activeloop_deeplake)\n",
    "\n",
    "[https://docs.activeloop.ai/quickstart](https://docs.activeloop.ai/quickstart)\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
