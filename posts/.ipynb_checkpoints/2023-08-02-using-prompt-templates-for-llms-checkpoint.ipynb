{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-02'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    " - prompt-engineering\n",
    "title: Using Prompt Templates with Large Language Models\n",
    "description: This article explores the subtleties of PromptTemplates and efficient ways to use them. A PromptTemplate is a pre-established pattern or framework used to create efficient and dependable prompts for extensive language models - it serves as a guide to make sure the input text or prompt is formatted correctly\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake2.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We can do a variety of jobs thanks to large language models. These models work on the simple premise that they take a text input sequence and produce a text output sequence. The prompt or input text is the most important element in this process.\n",
    "\n",
    "For anyone working with large language models, creating appropriate prompts is essential since poorly created prompts result in poor outputs while well formulated prompts produce effective outcomes. The LangChain library has created a complete collection of objects specifically for prompts since it understands how important they are.\n",
    "\n",
    "This article explores the subtleties of Prompt Templates and efficient ways to use them. A Prompt Template is a pre-established pattern or framework used to create efficient and dependable prompts for extensive language models. It serves as a guide to make sure the input text or prompt is formatted correctly.\n",
    "\n",
    "## Import Libs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STjlSS-7kCoz",
    "outputId": "254c04c4-5aa3-4f92-9b84-dbef85d7e11f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.6/934.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -q langchain==0.0.208 openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsV8LDNckFNY",
    "outputId": "04814f16-7f2a-4134-8bdf-e151a43e466f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with Prompt Templates\n",
    "\n",
    "Using a PromptTemplate with a single dynamic input for a user inquiry is demonstrated here. Ensure that your OPEN AI key is used to define the OPENAI_API_KEY in your environment variables. The following command should be used to install the necessary packages: install langchain==0.0.208 deeplake openai tiktoken using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfBouz-GkHpo",
    "outputId": "de8f21da-8a18-4281-ca4e-d77089127cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main advantage of quantum computing over classical computing?\n",
      "Answer:  The main advantage of quantum computing over classical computing is its ability to solve complex problems faster.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided, answer\n",
    "with \"I don't know\".\n",
    "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
    "...\n",
    "Question: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Set the query you want to ask\n",
    "input_data = {\"query\": \"What is the main advantage of quantum computing over classical computing?\"}\n",
    "\n",
    "# Run the LLMChain to get the AI-generated answer\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Question:\", input_data[\"query\"])\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With any additional question, you can modify the input_data dictionary.\n",
    "\n",
    "The template is a prepared string that contains a placeholder for the word \"query\" that, when used, will be replaced by an actual inquiry. Two arguments are needed to build a PromptTemplate object:\n",
    "\n",
    "1. input_variables: A list of the template's variable names; in this instance, it simply contains the query.\n",
    "2. template: A string of placeholders and prepared text used as a template.\n",
    "\n",
    "By giving input data, the PromptTemplate object can be used to generate prompts with customised questions. The input data is a dictionary with the variable name in the template's name as the key. A language model can then be used to create answers from the resulting prompt.\n",
    "\n",
    "To choose a subset of examples that will be the most instructive for the language model, you can develop a FewShotPromptTemplate with an ExampleSelector for use in more complex applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4nUPXHskR39",
    "outputId": "8ec568ca-0ad5-490e-c98a-2837c562a871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tropical forests and mangrove swamps\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\"animal\": \"lion\", \"habitat\": \"savanna\"},\n",
    "    {\"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
    "    {\"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Animal: {animal}\n",
    "Habitat: {habitat}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"animal\", \"habitat\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Identify the habitat of the given animal\",\n",
    "    suffix=\"Animal: {input}\\nHabitat:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the dynamic_prompt\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"input\": \"tiger\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can also save your PromptTemplate to a file in your local filesystem in JSON or YAML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4djEqXz2kuVw"
   },
   "outputs": [],
   "source": [
    "prompt_template.save(\"awesome_prompt.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load it back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDEcazXml-W_"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "loaded_prompt = load_prompt(\"awesome_prompt.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of Prompt Templates\n",
    "\n",
    "Let's examine other examples using various Prompt Template types. The following example shows how to teach the LLM using a few short prompts by giving examples of how to answer ironically to inquiries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0ufnnNyl_6q",
    "outputId": "0234aff7-a038-49bf-a58e-1cb1a66ebbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Start by studying Schrödinger's cat. That should get you off to a good start.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
    "    }, {\n",
    "        \"query\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative and funny responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the few_shot_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example's FewShotPromptTemplate shows how effective dynamic prompts can be. This method uses instances of prior encounters rather than employing a static template, enabling the AI to better comprehend the context and style of the desired answer.\n",
    "\n",
    "Several benefits of dynamic prompts versus static templates include:\n",
    "    \n",
    "- Improved context understanding: By providing examples, the AI can grasp the context and style of responses more effectively, enabling it to generate responses that are more in line with the desired output.\n",
    "- Flexibility: Dynamic prompts can be easily customized and adapted to specific use cases, allowing developers to experiment with different prompt structures and find the most effective format for their application.\n",
    "- Better results: As a result of the improved context understanding and flexibility, dynamic prompts often yield higher-quality outputs that better match user expectations.\n",
    "\n",
    "By providing examples and context that direct the AI towards producing more precise, contextually relevant, and stylistically consistent responses, this enables us to fully utilise the model's capabilities.\n",
    "\n",
    "Additionally, prompt templates work nicely with other LangChain features like chains and let you manage the number of examples supplied based on query length. This aids in regulating the balance between the quantity of instances and prompt size and optimising token usage.\n",
    "\n",
    "Giving the model as many relevant instances as you can without going over the maximum context window or slowing down processing is essential to maximising the performance of few-shot learning. We can strike a compromise between providing enough background and upholding the model's operational efficiency by dynamically including or excluding examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0qYx377mGlR"
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do you feel today?\",\n",
    "        \"answer\": \"As an AI, I don't have feelings, but I've got jokes!\"\n",
    "    }, {\n",
    "        \"query\": \"What is the speed of light?\",\n",
    "        \"answer\": \"Fast enough to make a round trip around Earth 7.5 times in one second!\"\n",
    "    }, {\n",
    "        \"query\": \"What is a quantum computer?\",\n",
    "        \"answer\": \"A magical box that harnesses the power of subatomic particles to solve complex problems.\"\n",
    "    }, {\n",
    "        \"query\": \"Who invented the telephone?\",\n",
    "        \"answer\": \"Alexander Graham Bell, the original 'ringmaster'.\"\n",
    "    }, {\n",
    "        \"query\": \"What programming language is best for AI development?\",\n",
    "        \"answer\": \"Python, because it's the only snake that won't bite.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris, the city of love and baguettes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is photosynthesis?\",\n",
    "        \"answer\": \"A plant's way of saying 'I'll turn this sunlight into food. You're welcome, Earth.'\"\n",
    "    }, {\n",
    "        \"query\": \"What is the tallest mountain on Earth?\",\n",
    "        \"answer\": \"Mount Everest, Earth's most impressive bump.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the most abundant element in the universe?\",\n",
    "        \"answer\": \"Hydrogen, the basic building block of cosmic smoothies.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the largest mammal on Earth?\",\n",
    "        \"answer\": \"The blue whale, the original heavyweight champion of the world.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the fastest land animal?\",\n",
    "        \"answer\": \"The cheetah, the ultimate sprinter of the animal kingdom.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the square root of 144?\",\n",
    "        \"answer\": \"12, the number of eggs you need for a really big omelette.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the average temperature on Mars?\",\n",
    "        \"answer\": \"Cold enough to make a Martian wish for a sweater and a hot cocoa.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of utilizing the examples list of dictionaries directly, we implement a LengthBasedExampleSelector like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdui3d7Amxgr"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final prompt is kept under the intended token limit by the code's dynamic selection and inclusion of examples based on their length using the LengthBasedExampleSelector. The dynamic_prompt_template is initialised using the selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyiUSvXHmye2"
   },
   "outputs": [],
   "source": [
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, rather than using a set list of examples, the dynamic_prompt_template makes use of the example_selector. This enables the FewShotPromptTemplate to modify the amount of examples included in accordance with the length of the input query. By doing this, it makes the best use possible of the available context window and guarantees that the language model receives an adequate quantity of contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BT34c4BkmzqD",
    "outputId": "3f17f8d8-9249-47b5-a193-f8b9ec9315c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Alexander Graham Bell, the man who made it possible to talk to people from miles away!\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Existing example and prompt definitions, and dynamic_prompt_template initialization\n",
    "\n",
    "# Create the LLMChain for the dynamic_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"Who invented the telephone?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut0T3AsBm4lr"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "For creating efficient prompts for extensive language models, prompt templates are crucial because they offer an organised and standardised framework that maximises accuracy and relevance. Dynamic prompt integration improves context comprehension, adaptability, and outcomes, making them an important tool for language model development.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
