{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-12'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    "title: Chains and why they are used in Langchain\n",
    "description: In this post we delve deeper into the concept of chains, which provide an end-to-end pipeline for utilizing language models. These chains seamlessly integrate models, prompts, memory, parsing output, and debugging capabilities, offering a user-friendly interface.\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake3.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Because it allows for natural language querying, prompting is regarded the most effective means of communicating with language models. We talked over prompting tactics and briefly used chains earlier. The chains will be explained in greater depth in this lesson.\n",
    "\n",
    "The chains are in charge of building an end-to-end pipeline for utilising the language models. They will integrate the model, prompt, memory, parsing output, and debugging capacity into a user-friendly interface. A chain will 1) take the user's question as input, 2) process the LLM's response, and 3) deliver the result to the user.\n",
    "\n",
    "By inheriting the Chain class, you can create your own pipeline. The LLMChain, for example, is the most basic type of chain in LangChain, descended from the Chain parent class. We'll start by looking at how to invoke this class and then move on to adding other functionalities.\n",
    "\n",
    "## Import Libs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZgUJsmpUCUi",
    "outputId": "7c27b0d4-5fad-4618-e063-68e87de1e81d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.7/823.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -q langchain==0.0.208 openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAsSxpAyUOnF",
    "outputId": "89d0a63f-9c88-4a8f-958a-e97c7451a9f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44NeYe0GXe32"
   },
   "source": [
    "## LLMChain\n",
    "\n",
    "There are several ways to use a chain, each with a different output format. This section's example is of constructing a bot that can suggest a replacement term based on context. The code sample below shows how to use the GPT-3 model with the OpenAI API. It then constructs a prompt using LangChain's PromptTemplate, and the LLMChain class connects everything together. It is also critical to establish the OPENAI_API_KEY environment variable with your OpenAI API credentials. \n",
    "\n",
    "The most straightforward approach uses the chain class __call__ method. It means passing the input directly to the object while initializing it. It will return the input variable and the model’s response under the text key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQn4558HUPvI",
    "outputId": "35edbf54-a44e-41d7-806f-2d2e0f374970"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "prompt_template = \"What is a word to replace the following: {word}?\"\n",
    "\n",
    "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "llm_chain(\"artificial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lGTqvJxXjMZ"
   },
   "source": [
    "It is also possible to pass numerous inputs at once and receive a list for each input by using the.apply() method. The only distinction is that inputs are not included in the returning list. Regardless, the returned list will be in the same order as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhrI8CggVtJo",
    "outputId": "f2673059-65b2-4ba4-df2c-063688277c88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n\\nSynthetic'}, {'text': '\\n\\nWisdom'}, {'text': '\\n\\nAutomaton'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5jB5LBJXk9s"
   },
   "source": [
    "The.generate() method returns an instance of LLMResult, which has more information. For example, the finish_reason key shows the reason for the generation process's termination. It could be stopped, indicating that the model has opted to finish or has reached the length limit. Other self-explanatory information includes the total amount of spent tokens and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYi0o5KqV68n",
    "outputId": "e656827f-2299-423e-f4a8-af09a1402fae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWisdom', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAutomaton', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 33, 'completion_tokens': 13, 'total_tokens': 46}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q1BvtRiXuLg"
   },
   "source": [
    "The next function we'll look at is.predict(). (which might be interchanged with.run()) Its most common application is to pass several inputs for a single prompt. However, it is also feasible to utilise it with a single input variable. The following command will send both the word to be replaced and the context for the model to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OX4RrRrlXvWm",
    "outputId": "c9cc9d8a-1585-4f6d-ca5f-e752c04c1fd2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nVentilator'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "\n",
    "llm_chain.predict(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the domain of objects, the model accurately recommended that a Ventilator would be an appropriate alternative for the word fan. Furthermore, when we perform the experiment in a new context, people, the outcome will alter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NpdjaLWwYQQ1",
    "outputId": "5ebe6455-95a2-4167-e235-d49982f9fe0b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nAdmirer'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"humans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9P5gGwxClJeL",
    "outputId": "560ea2b2-b540-42a8-c18a-19fcca6f0e70"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nVentilator'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_chain.run(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNPOT6iAbt1l"
   },
   "source": [
    "The sample codes above demonstrate how to feed single or multiple inputs to a chain and retrieve the outputs. However, as we discussed in the \"Managing Outputs with Output Parsers\" course, we prefer to receive formatted output in most circumstances.\n",
    "\n",
    "As shown below, we can give a prompt as a string to a Chain and initialise it with the.from_string() function.\n",
    "from_string(llm=llm, template=template)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6T5_9k2bx_N"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\"\"\n",
    "llm_chain = LLMChain.from_string(llm=llm, template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AkE6wx8Vb9Ns",
    "outputId": "dbedb888-49d7-43da-88a5-05472fbea85d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nVentilator'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRIaIXSKZu6U"
   },
   "source": [
    "## Parsers\n",
    "\n",
    "As previously stated, the output parsers can establish a data schema to provide suitably structured replies. It wouldn't be an end-to-end pipeline unless parsers were used to extract information from the LLM textual output. The following example shows how to use the CommaSeparatedListOutputParser class in conjunction with the PromptTemplate to ensure that the results are in a list format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aIEZWDQtZwKw",
    "outputId": "aa45ef77-7ea6-42ea-c613-94ac0accfb88"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nSynthetic, Manufactured, Imitation, Fabricated, Fake, Simulated, Artificial Intelligence, Automated, Constructed, Programmed, Mechanical, Processed, Algorithmic, Generated.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[], output_parser=output_parser))\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJ18G38-aXcE",
    "outputId": "248f1533-6948-4f94-be8d-1ceb88433c20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthetic',\n",
       " 'Manufactured',\n",
       " 'Imitation',\n",
       " 'Fabricated',\n",
       " 'Fake',\n",
       " 'Simulated',\n",
       " 'Artificial Intelligence',\n",
       " 'Automated',\n",
       " 'Constructed',\n",
       " 'Programmed',\n",
       " 'Processed',\n",
       " 'Mechanical',\n",
       " 'Man-Made',\n",
       " 'Lab-Created',\n",
       " 'Artificial Neural Network.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict_and_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b8_oGlAcx3F"
   },
   "source": [
    "## Conversational Chain (Memory)\n",
    "\n",
    "Memory is the next component that will complete a chain, depending on the application. Using the ConversationalBufferMemory class, LangChain provides a ConversationalChain to track past prompts and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTYpcUktae6w",
    "outputId": "3c7c83dd-5ba6-44f6-bbe7-baf153c97232"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthetic', 'Manufactured', 'Imitation']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory())\n",
    "\n",
    "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can ask it to return the following four replacement words. It uses the memory to find the next options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7_vgaBgeWWG",
    "outputId": "0e9c50d3-a48f-4083-bafe-4944a53dda26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fabricated', 'Simulated', 'Automated', 'Constructed']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict_and_parse(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Kz12V6FhjC3"
   },
   "source": [
    "## Debug\n",
    "\n",
    "By setting the verbose option to True, you can inspect the inner workings of any chain. As shown in the code below, the chain will return the initial prompt and the output. The output is determined by the application. If there are more steps, it may provide more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGRoPJ1xhtTE",
    "outputId": "4bfd9840-47a9-4967-8c9d-e6a9ff0fbaa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "Answer briefly. write the first 3 options.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Synthetic', 'Manufactured', 'Imitation']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True)\n",
    "\n",
    "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XI9e40ui1yX"
   },
   "source": [
    "## Sequential Chain\n",
    "\n",
    "Another helpful feature is using a sequential chain that concatenates multiple chains into one. The following code shows a sample usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A16wajt2hxLE"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fs4Chc0iKaj3"
   },
   "source": [
    "The SimpleSequentialChain will start running each chain from the first index and pass its response to the next one in the list.\n",
    "\n",
    "## Custom Chain\n",
    "\n",
    "The LangChain library includes multiple preconfigured chains for various purposes, such as Transformation Chain, LLMCheckerChain, LLMSummarizationCheckerChain, and OpenAPI Chain, all of which have the properties indicated in previous sections. You can also define your chain for any custom task. In this section, we will build a chain that returns the meaning of a term and then offers a substitute.\n",
    "\n",
    "It begins by developing a class that derives the majority of its functionality from the Chain class. Then, depending on the use case, the following three methods must be defined. The input_keys and output_keys methods tell the model what to expect, and the _call method executes each chain and merges its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tCjI4DtKbTG"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will declare each chain individually using the LLMChain class. Lastly, we call our custom chain ConcatenateChain to merge the results of the chain_1 and chain_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-W3ZqALLbwP",
    "outputId": "83f65c01-5573-403f-9180-7a2b60a41b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output:\n",
      "\n",
      "\n",
      "Artificial means something that is not natural or made by humans, but rather created or produced by artificial means.\n",
      "\n",
      "Synthetic\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"artificial\")\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This post introduced us to LangChain and its powerful feature, chains, which integrate several components to form a cohesive application. The post began by demonstrating the use of numerous premade chains from the LangChain package. Then we added more functionality like parsers, memory, and debugging. Finally, the technique of creating bespoke chains was described.\n",
    "\n",
    "More articles on Langchain can be found [here](https://livingdatalab.com/#category=langchain).\n",
    "\n",
    "Further Reading:\n",
    "\n",
    "[https://python.langchain.com/docs/modules/chains/](https://python.langchain.com/docs/modules/chains/)\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
