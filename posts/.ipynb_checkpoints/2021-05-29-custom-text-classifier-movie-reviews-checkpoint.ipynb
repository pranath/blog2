{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /projects/natural-language-processing/2021/05/29/custom-text-classifier-movie-reviews\n",
    "categories:\n",
    "- projects\n",
    "- natural-language-processing\n",
    "- deep-learning\n",
    "- fastai\n",
    "date: '2021-05-29'\n",
    "description: In this article we are going to create a deep learning text classifier\n",
    "  using the fastai library, and the ULMFit approach.\n",
    "image: https://github.com/pranath/blog/raw/master/images/ulmfit.png\n",
    "output-file: 2021-05-29-custom-text-classifier-movie-reviews.html\n",
    "title: Creating a custom text classifier for movie reviews\n",
    "toc: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghxHyEVnvawH"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this article we are going to train a deep learning text classifier using the fastai library. We will do this for the [IMDB](http://ai.stanford.edu/~amaas/data/sentiment/) movie reviews dataset. In particular, we will look at fastai's ULMFit approach which involves fine tuning a language model more with specialised text before using this language model as a basis for a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpRF8lqy0qy-",
    "outputId": "81fca051-5db6-4b76-f9c1-e76e1812eb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 727kB 27.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.2MB 41.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 53.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 9.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
      "\u001b[?25hMounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastbook import *\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUchinELjFEm"
   },
   "source": [
    "## Text Pre-processing\n",
    "\n",
    "So how might we proceed with building a language model, that we can then use for clasisifcation? Consider with one of the simplest neural networks, a [collaberative filtering model](https://livingdatalab.com/deep-learning-theory/2021/05/25/collaberative-filtering-from-scratch.html). This uses embedding matrices to encode different items (such as films) and users, combine these using dot products to calculate a value, which we test against known ratings - and use gradient descent to learn the correct embedding matrices to best predict these ratings. \n",
    "\n",
    "Optionally, we can create instead a deep learning model from this by concatinating the embedding matrices instead of the dot product, then putting the result through an activtion function, and more layers etc.\n",
    "\n",
    "So we could use a similar approach, where we put a sequence of words through a neural network via encoding them in an embedding martix for words. However a significant difference from the collaberative filtering approach here is the idea of a sequence.\n",
    "\n",
    "We can proceed with these 5 steps:\n",
    "\n",
    "1. **Tokenisation**: convert words to recognised units\n",
    "2. **Numericalisation**: convert tokens to numbers\n",
    "3. **Create data loader**: Create a data loader to train the language model which creates a target variable offset by one word from the input variable from the text data\n",
    "4. **Train language model**: We need to train a model that can take an amount of text data of variable length, and be able to predict the next word for any word in the sequence.\n",
    "5. **Train classifier model**: Using what the language model has learned about the text as a basis, we can build on top of this to create and train a language model.\n",
    "\n",
    "This is an approach pioneered by fastai called the Universal Langauage Model Fine-tuining (ULMFit) approach.\n",
    "\n",
    "![](https://github.com/pranath/blog/raw/master/images/ulmfit.png \"The ULMFit methodology\")\n",
    "\n",
    "### Tokenisation\n",
    "\n",
    "Lets get the data and tokenise it using the fastai library tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "BZx7E6Zd1Gh0",
    "outputId": "c249e55e-a06d-41ac-ef57-9438b8166a90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I caught up with this movie on TV after 30 years or more. Several aspects o'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download data\n",
    "path = untar_data(URLs.IMDB)\n",
    "\n",
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])\n",
    "# Show example text data\n",
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygkYEJF31zM3"
   },
   "source": [
    "Fastai has an english word tokeniser, lets see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CxCRqSOo-gh",
    "outputId": "cfd57ad8-7254-49fe-ce76-bfaff703886f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#626) ['I','caught','up','with','this','movie','on','TV','after','30','years','or','more','.','Several','aspects','of','the','film','stood','out','even','when','viewing','it','so','many','years','after','it'...]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test word tokeniser function\n",
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApCibh5ZpJUD",
    "outputId": "30388d51-522e-4d22-f4df-209cdd133e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#699) ['xxbos','i','caught','up','with','this','movie','on','xxup','tv','after','30','years','or','more','.','xxmaj','several','aspects','of','the','film','stood','out','even','when','viewing','it','so','many','years'...]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test word tokeniser class\n",
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEBtqXCh2Syr"
   },
   "source": [
    "The class goes beyond just converting the text to tokens for words, for example it creates tokens like 'xxbos' which is a special token to indicate the beginning of a new text sequence i.e. 'beggining of stream' standard NLP concept.\n",
    "\n",
    "The class applies a series fo rules and transformations to the text, here is a list of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ey-I-Acq2-mZ",
    "outputId": "68ff04ab-836d-461a-a647-172f83a62b18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html>,\n",
       " <function fastai.text.core.replace_rep>,\n",
       " <function fastai.text.core.replace_wrep>,\n",
       " <function fastai.text.core.spec_add_spaces>,\n",
       " <function fastai.text.core.rm_useless_spaces>,\n",
       " <function fastai.text.core.replace_all_caps>,\n",
       " <function fastai.text.core.replace_maj>,\n",
       " <function fastai.text.core.lowercase>]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23Funcgn1Uc6"
   },
   "source": [
    "\n",
    "### Numericalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "t2SjTIOR3uEB",
    "outputId": "939d2b5e-9cee-4ef7-df78-18d00975aaae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"(#2096) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','of','to','is','in','it','i'...]\""
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get first 2000 reviews to test\n",
    "txts = L(o.open().read() for o in files[:2000])\n",
    "# Tokenise\n",
    "toks = tkn(txt)\n",
    "# Select subset of tokenised reviews\n",
    "toks200 = txts[:200].map(tkn)\n",
    "num = Numericalize()\n",
    "# Numericalise tokens - create a vocab\n",
    "num.setup(toks200)\n",
    "# Show first 20 tokens of vocab\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdUDa7-N5O5A",
    "outputId": "b653bcf3-686e-4ee8-ff8a-b8f55eb8aa91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,   19,  726,   79,   29,   21,   32,   31,    7,  314,  112, 1195,  138,   63,   71,   10,    8,  393, 1524,   14])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Now we can convert tokens to numbers for example\n",
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQRO0o9G6u_W"
   },
   "source": [
    "### Create data loader\n",
    "\n",
    "So we need to join all the text together, and then divide it into specific sized batches of multiple lines of text of fixed length, which maintain the correct order of the text within each batch. At every epoch the order of the reviews is shuffled, but we then join these all together and construct mini-batches in order, which our model will process and learn from. This is all done automatically by the fastai library tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQ_aKEAc70yb",
    "outputId": "2662c2d1-08f5-4e39-9eaa-286759a0f9fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get some example numericalised tokens\n",
    "nums200 = toks200.map(num)\n",
    "# Pass to dataloader\n",
    "dl = LMDataLoader(nums200)\n",
    "# Get first batch of data and check sizes\n",
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "4P_9OHEh8N6E",
    "outputId": "8933a617-288a-4fe2-fa2c-8479b08a85de"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Examine example input variable should be start of a text\n",
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "D2bZsEV68eZC",
    "outputId": "099af10a-7f01-452b-ac13-4456ccd43547"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Examine example target variable which is the same plus added next word - this is what we want to predict\n",
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoKYcmo9jPvV"
   },
   "source": [
    "## Training a text classifier\n",
    "\n",
    "### Fine tune language model\n",
    "We can further simplify the text preparation for training our language model by combining the tokenisation, numericalisation and dataloader creation into one step by creating a TextBlock and then a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV05gUpoG3UE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create text dataloader for language model training\n",
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=128, seq_len=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMRECeJ___n6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a language model learner, by default will use x-entropy loss\n",
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()\n",
    "# Train model\n",
    "learn.fit_one_cycle(1, 2e-2)\n",
    "# Save model encoder\n",
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6w6qGNmAwz_"
   },
   "source": [
    "### Fine tune classifier model\n",
    "\n",
    "To fine tune the classifier model we create the data loader in a slightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MMvQWHWA7gI"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create text dataloader for classifier model training - using lm vocab\n",
    "dls_clas = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=128, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-g5XqUxrBNcX"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create classifier learner\n",
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy).to_fp16()\n",
    "# Load encoder from language model\n",
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7VnbYrGCHZI"
   },
   "source": [
    "When fine tuning the classifier, it is found to be best if we gradually unfreeze layers to train, and this is best done in manual steps. The first fit will just train the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_Om7VfHCDbK"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train model - last layer only\n",
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtX1jmbJCZfQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Unfreeze a few more layers and train some more with discriminative learning rates\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXDm8MVoConk"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Unfreeze more layers and train more\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSzcAhm9CxcP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Unfreeze whole model and train more\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s9utSpUC59J"
   },
   "source": [
    "On this IMDB dataset we can achieve a classification accuracy of around 95% using this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_My_poWS3wBI"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this article we have looked in more detail at how we can train a text classifier using the 3 step ULMFit fastai approach, and achieve a good level of accuracy. We also saw in more detail what the fastai library does under the hood to make this process much easier."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
