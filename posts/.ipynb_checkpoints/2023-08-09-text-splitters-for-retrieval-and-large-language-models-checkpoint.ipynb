{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-09'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    " - retrievers\n",
    "title: Text Splitters for Retrieval and Large Language Models\n",
    "description: Giving documents to the LLM as information sources and asking it to produce an answer based on the information it extracts from the document is one strategy for reducing hallucinations - in this article we will look at how text splitters can help with this\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake1.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Large Language Models are known for producing text that looks and reads like human beings, but they may also \"hallucinate\" and produce information that is both accurate and illogical. It's interesting to note that this inclination can be helpful while undertaking creative work because it produces a variety of original and inventive thoughts, opening up fresh viewpoints and promoting the creative process. This presents a problem, though, in circumstances where accuracy is crucial, including code reviews, duties involving insurance, or answers to research-related questions.\n",
    "\n",
    "Giving documents to the LLM as information sources and asking it to produce an answer based on the information it extracts from the document is one strategy for reducing hallucinations. Users can check the information with the source document and this can lessen the risk of hallucinations.\n",
    "\n",
    "Let's discuss the pros and cons of this approach:\n",
    "\n",
    "Pros:\n",
    "\n",
    "1. **Reduced hallucination:** By providing a source document, the LLM is more likely to generate content based on the given information, reducing the chances of creating false or irrelevant information.\n",
    "2. **Increased accuracy:** With a reliable source document, the LLM can generate more accurate answers, especially in use cases where accuracy is crucial.\n",
    "3. **Verifiable information:** Users can cross-check the generated content with the source document to ensure the information is accurate and reliable.\n",
    "\n",
    "Cons:\n",
    "\n",
    "1. **Limited scope:** Relying on a single document may limit the scope of the generated content, as the LLM will only have access to the information provided in the document.\n",
    "2. **Dependence on document quality:** The accuracy of the generated content heavily depends on the quality and reliability of the source document. The LLM will likely generate incorrect or misleading content if the document contains inaccurate or biased information.\n",
    "3. **Inability to eliminate hallucination completely:** Although providing a document as a base reduces the chances of hallucination, it does not guarantee that the LLM will never generate false or irrelevant information.\n",
    "\n",
    "Another issue is that LLMs are unable to feed complete documents due to a limit prompt size. Because of this, it is essential to break documents into smaller pieces, and Text Splitters are quite helpful in doing so. Text splitters make it easier for language models to process huge text documents by dividing them into smaller, more manageable portions.\n",
    "\n",
    "As smaller segments may be more likely to match a query, using a Text Splitter can also enhance the performance of vector store searches. It can be helpful to experiment with various chunk sizes and overlaps in order to adapt the results to your particular requirements.\n",
    "\n",
    "## Import Libs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRhvwNoLQr1l",
    "outputId": "d33e5bc9-bf03-4707-96d8-3988f468b8d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -q langchain==0.0.208 tiktoken pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VxeKBo6x55sQ",
    "outputId": "4e6575be-79da-487f-cbee-f704026f6877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-25 16:35:36--  https://www.cheat-sheets.org/saved-copy/The%20One%20Page%20Linux%20Manual.pdf\n",
      "Resolving www.cheat-sheets.org (www.cheat-sheets.org)... 90.156.201.26, 90.156.201.79, 90.156.201.114, ...\n",
      "Connecting to www.cheat-sheets.org (www.cheat-sheets.org)|90.156.201.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 96538 (94K) [application/pdf]\n",
      "Saving to: ‘The One Page Linux Manual.pdf’\n",
      "\n",
      "The One Page Linux  100%[===================>]  94.28K   348KB/s    in 0.3s    \n",
      "\n",
      "2023-07-25 16:35:38 (348 KB/s) - ‘The One Page Linux Manual.pdf’ saved [96538/96538]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!wget https://www.cheat-sheets.org/saved-copy/The%20One%20Page%20Linux%20Manual.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing a Text Splitter\n",
    "\n",
    "It's critical to divide large texts into manageable sections while processing them. Due to the importance of preserving semantically connected text fragments, this initially straightforward process can quickly become complicated. Depending on the sort of writing, \"semantically related\" may mean several things. We'll look at a number of approaches in this post to do this.\n",
    "\n",
    "Text splitters often go through the following steps:\n",
    "\n",
    "1. Divide the text into small, semantically meaningful chunks (often sentences).\n",
    "2. Combine these small chunks into a larger one until a specific size is reached (determined by a particular function).\n",
    "3. Once the desired size is attained, separate that chunk as an individual piece of text, then start forming a new chunk with some overlap to maintain context between segments.\n",
    "\n",
    "Consequently, there are two primary dimensions to consider when customizing your text splitter:\n",
    "\n",
    "- The method used to split the text\n",
    "- The approach for measuring chunk size\n",
    "\n",
    "## Character Text Splitter\n",
    "\n",
    "This kind of splitter can be used in a variety of situations when it is necessary to break up lengthy passages of text into more manageable, semantically sound portions. For simpler processing or analysis, you might utilise it to divide a lengthy article into manageable bits. To balance the trade-offs between dividing the text into digestible chunks and maintaining semantic context between chunks, the splitter gives you the option to customise the chunking process along two axes: chunk size and chunk overlap.\n",
    "\n",
    "Use the PyPDFLoader class to load the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCN2p_HlQuSX"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"The One Page Linux Manual.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By loading the text file, we can ask more specific questions related to the subject, which helps minimize the likelihood of LLM hallucinations and ensures more accurate, context-driven responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mW9ujW6KQ5ma",
    "outputId": "42e6a7c8-2926-432f-f726-336171737cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\\nVersion 3.0 May 1999 squadron@powerup.com.au\\nStarting & Stopping\\nshutdown -h now Shutdown the system now and do not\\nreboot\\nhalt Stop all processes - same as above\\nshutdown -r 5 Shutdown the system in 5 minutes and\\nreboot\\nshutdown -r now Shutdown the system now and reboot\\nreboot Stop all processes and then reboot - same\\nas above\\nstartx Start the X system\\nAccessing & mounting file systems\\nmount -t iso9660 /dev/cdrom\\n/mnt/cdromMount the device cdrom\\nand call it cdrom under the\\n/mnt directory\\nmount -t msdos /dev/hdd\\n/mnt/ddriveMount hard disk “d” as a\\nmsdos file system and call\\nit ddrive under the /mnt\\ndirectory\\nmount -t vfat /dev/hda1\\n/mnt/cdriveMount hard disk “a” as a\\nVFAT file system and call it\\ncdrive under the /mnt\\ndirectory\\numount /mnt/cdrom Unmount the cdrom\\nFinding files and text within files\\nfind / -name  fname Starting with the root directory, look\\nfor the file called fname\\nfind / -name ”*fname* ” Starting with the root directory, look\\nfor the file containing the string fname\\nlocate missingfilename Find a file called missingfilename\\nusing the locate command - this\\nassumes you have already used the\\ncommand updatedb (see next)\\nupdatedb Create or update the database of files\\non all file systems attached to the linux\\nroot directory\\nwhich missingfilename Show the subdirectory containing the\\nexecutable file  called missingfilename\\ngrep textstringtofind\\n/dirStarting with the directory called dir ,\\nlook for and list all files containing\\ntextstringtofind\\nThe X Window System\\nxvidtune Run the X graphics tuning utility\\nXF86Setup Run the X configuration menu with\\nautomatic probing of graphics cards\\nXconfigurator Run another X configuration menu with\\nautomatic probing of graphics cards\\nxf86config Run a text based X configuration menu\\nMoving, copying, deleting & viewing files\\nls -l List files in current directory using\\nlong format\\nls -F List files in current directory and\\nindicate the file type\\nls -laC List all files in current directory in\\nlong format and display in columnsrm name Remove a file or directory called\\nname\\nrm -rf name Kill off an entire directory and all it’s\\nincludes files and subdirectories\\ncp filename\\n/home/dirnameCopy the file called filename to the\\n/home/dirname directory\\nmv filename\\n/home/dirnameMove the file called filename to the\\n/home/dirname directory\\ncat filetoview Display the file called filetoview\\nman -k keyword Display man pages containing\\nkeyword\\nmore filetoview Display the file called filetoview one\\npage at a time, proceed to next page\\nusing the spacebar\\nhead filetoview Display the first 10 lines of the file\\ncalled filetoview\\nhead -20 filetoview Display the first 20 lines of the file\\ncalled filetoview\\ntail filetoview Display the last 10 lines of the file\\ncalled filetoview\\ntail -20 filetoview Display the last 20 lines of the file\\ncalled filetoview\\nInstalling software for Linux\\nrpm -ihv name.rpm Install the rpm package called name\\nrpm -Uhv name.rpm Upgrade the rpm package called\\nname\\nrpm -e package Delete the rpm package called\\npackage\\nrpm -l package List the files in the package called\\npackage\\nrpm -ql package List the files and state the installed\\nversion of the package called\\npackage\\nrpm -i --force package Reinstall the rpm package called\\nname having deleted parts of it (not\\ndeleting using rpm -e)\\ntar -zxvf archive.tar.gz or\\ntar -zxvf archive.tgzDecompress the files contained in\\nthe zipped and tarred archive called\\narchive\\n./configure Execute the script preparing the\\ninstalled files for compiling\\nUser Administration\\nadduser accountname Create a new user call accountname\\npasswd accountname Give accountname a new password\\nsu Log in as superuser from current login\\nexit Stop being superuser and revert to\\nnormal user\\nLittle known tips and tricks\\nifconfig List ip addresses for all devices on\\nthe machine\\napropos subject List manual pages for subject\\nusermount Executes graphical application for\\nmounting and unmounting file\\nsystems' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no single method for chunking text that works in all situations; what works in one situation may not work in another. Going through a few procedures is necessary to determine the ideal chunk size for your project. First, purge any unnecessary information from your data, such as HTML elements from websites. Select a couple other chunk sizes to test after that. The type of data you're working with and the model you're using will determine the optimal size.  Finally, execute various queries and compare the results to see how well each size performs. Before choosing the ideal size, you might have to try a few different ones. Even if it could take some time, the best outcomes are worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5wpgZI0RKBJ",
    "outputId": "b44ee37c-9821-4d62-95f2-f07c0bc287d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 2 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rw4Wzt7N7F7k",
    "outputId": "d3d8eb0e-ecd1-4731-8d3e-e758ba442c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\n",
      "Version 3.0 May 1999 squadron@powerup.com.au\n",
      "Starting & Stopping\n",
      "shutdown -h now Shutdown the system now and do not\n",
      "reboot\n",
      "halt Stop all processes - same as above\n",
      "shutdown -r 5 Shutdown the system in 5 minutes and\n",
      "reboot\n",
      "shutdown -r now Shutdown the system now and reboot\n",
      "reboot Stop all processes and then reboot - same\n",
      "as above\n",
      "startx Start the X system\n",
      "Accessing & mounting file systems\n",
      "mount -t iso9660 /dev/cdrom\n",
      "/mnt/cdromMount the device cdrom\n",
      "and call it cdrom under the\n",
      "/mnt directory\n",
      "mount -t msdos /dev/hdd\n",
      "/mnt/ddriveMount hard disk “d” as a\n",
      "msdos file system and call\n",
      "it ddrive under the /mnt\n",
      "directory\n",
      "mount -t vfat /dev/hda1\n",
      "/mnt/cdriveMount hard disk “a” as a\n",
      "VFAT file system and call it\n",
      "cdrive under the /mnt\n",
      "directory\n",
      "umount /mnt/cdrom Unmount the cdrom\n",
      "Finding files and text within files\n",
      "find / -name  fname Starting with the root directory, look\n",
      "for the file called fname\n",
      "find / -name ”*fname* ” Starting with the root directory, look\n",
      "for the file containing the string fname\n",
      "locate missingfilename Find a file called missingfilename\n",
      "using the locate command - this\n",
      "assumes you have already used the\n",
      "command updatedb (see next)\n",
      "updatedb Create or update the database of files\n",
      "on all file systems attached to the linux\n",
      "root directory\n",
      "which missingfilename Show the subdirectory containing the\n",
      "executable file  called missingfilename\n",
      "grep textstringtofind\n",
      "/dirStarting with the directory called dir ,\n",
      "look for and list all files containing\n",
      "textstringtofind\n",
      "The X Window System\n",
      "xvidtune Run the X graphics tuning utility\n",
      "XF86Setup Run the X configuration menu with\n",
      "automatic probing of graphics cards\n",
      "Xconfigurator Run another X configuration menu with\n",
      "automatic probing of graphics cards\n",
      "xf86config Run a text based X configuration menu\n",
      "Moving, copying, deleting & viewing files\n",
      "ls -l List files in current directory using\n",
      "long format\n",
      "ls -F List files in current directory and\n",
      "indicate the file type\n",
      "ls -laC List all files in current directory in\n",
      "long format and display in columnsrm name Remove a file or directory called\n",
      "name\n",
      "rm -rf name Kill off an entire directory and all it’s\n",
      "includes files and subdirectories\n",
      "cp filename\n",
      "/home/dirnameCopy the file called filename to the\n",
      "/home/dirname directory\n",
      "mv filename\n",
      "/home/dirnameMove the file called filename to the\n",
      "/home/dirname directory\n",
      "cat filetoview Display the file called filetoview\n",
      "man -k keyword Display man pages containing\n",
      "keyword\n",
      "more filetoview Display the file called filetoview one\n",
      "page at a time, proceed to next page\n",
      "using the spacebar\n",
      "head filetoview Display the first 10 lines of the file\n",
      "called filetoview\n",
      "head -20 filetoview Display the first 20 lines of the file\n",
      "called filetoview\n",
      "tail filetoview Display the last 10 lines of the file\n",
      "called filetoview\n",
      "tail -20 filetoview Display the last 20 lines of the file\n",
      "called filetoview\n",
      "Installing software for Linux\n",
      "rpm -ihv name.rpm Install the rpm package called name\n",
      "rpm -Uhv name.rpm Upgrade the rpm package called\n",
      "name\n",
      "rpm -e package Delete the rpm package called\n",
      "package\n",
      "rpm -l package List the files in the package called\n",
      "package\n",
      "rpm -ql package List the files and state the installed\n",
      "version of the package called\n",
      "package\n",
      "rpm -i --force package Reinstall the rpm package called\n",
      "name having deleted parts of it (not\n",
      "deleting using rpm -e)\n",
      "tar -zxvf archive.tar.gz or\n",
      "tar -zxvf archive.tgzDecompress the files contained in\n",
      "the zipped and tarred archive called\n",
      "archive\n",
      "./configure Execute the script preparing the\n",
      "installed files for compiling\n",
      "User Administration\n",
      "adduser accountname Create a new user call accountname\n",
      "passwd accountname Give accountname a new password\n",
      "su Log in as superuser from current login\n",
      "exit Stop being superuser and revert to\n",
      "normal user\n",
      "Little known tips and tricks\n",
      "ifconfig List ip addresses for all devices on\n",
      "the machine\n",
      "apropos subject List manual pages for subject\n",
      "usermount Executes graphical application for\n",
      "mounting and unmounting file\n",
      "systems\n"
     ]
    }
   ],
   "source": [
    "print (\"Preview:\")\n",
    "print (texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY72xoXgRo8n"
   },
   "source": [
    "#====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Character Text Splitter\n",
    "\n",
    "A text splitter called the Recursive Character Text Splitter divides the text into sections based on a list of characters that is provided. Up until the resulting chunks are small enough, it makes an effort to separate text using the characters from a list in order. As paragraphs, sentences, and words are typically the most semantically linked units of text, the default list of characters used for splitting is [\"nn\", \"n\", \" \", \"], which aims to keep them together for as long as possible. This indicates that the text is first split into two new-line characters by the class. \n",
    "\n",
    "The output will then be split by a single new-line character, followed by a space character, and so on, until the appropriate chunk size is reached if the resulting chunks are still greater than the desired chunk size.\n",
    "\n",
    "You can make an instance of the RecursiveCharacterTextSplitter and supply the following parameters to use it:\n",
    "\n",
    "- **chunk_size :** The maximum size of the chunks, as measured by the length_function (default is 100).\n",
    "- **chunk_overlap:** The maximum overlap between chunks to maintain continuity between them (default is 20).\n",
    "- **length_function:** parameter is used to calculate the length of the chunks. By default, it is set to len, which counts the number of characters in a chunk. However, you can also pass a token counter or any other function that calculates the length of a chunk based on your specific requirements.\n",
    "\n",
    "In some circumstances, such as when dealing with language models with token restrictions, it may be advantageous to use a token counter rather than the default len function. To better control and optimise your requests, you might choose to count tokens rather than characters since OpenAI's GPT-3 has a token restriction of 4096 tokens per request.\n",
    "\n",
    "Here is a demonstration of RecursiveCharacterTextSplitter in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVro99tsRtYH"
   },
   "outputs": [],
   "source": [
    "!echo \"Helllo, my name is Ala\\n Hello again\\n\\ntesting newline.\" > LLM.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d131J86RLtb"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('LLM.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbD9oy4sRqKV"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCLacGWZRzJ1",
    "outputId": "011c0607-91a5-4240-a53c-0f51d5d63666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Helllo, my name is Ala\\n Hello again', metadata={}), Document(page_content='testing newline.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([sample_text])\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdbQYMtpSVdQ"
   },
   "source": [
    "With the needed parameters, we constructed a RecursiveCharacterTextSplitter class instance. The predetermined list of characters is [\"n\", \"n\", \" \", \"\"].\n",
    "\n",
    "Two new-line characters (nn) are used to first separate the text. The class then tries to divide the output by a single new-line character (n) because the chunks are still bigger than the required chunk size (50).\n",
    "\n",
    "The RecursiveCharacterTextSplitter is used in this example to split the text into chunks with a maximum size of 50 characters and an overlap of 10 characters. A list of papers with the divided text will be the output.\n",
    "\n",
    "You can build a custom function that determines the number of tokens in a given text and supply it as the length_function parameter to use a token counter. By doing this, you can make sure that your text splitter determines the length of chunks using the number of tokens rather than the number of letters. \n",
    "\n",
    "## NLTK Text Splitter\n",
    "\n",
    "The Natural Language Toolkit (NLTK) TextSplitter in LangChain is an implementation of a text splitter that splits text based on tokenizers using the NLTK library. The objective is to break up lengthy texts into manageable pieces while maintaining the sentences' and paragraphs' natural order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iX4fHM6uSkoy",
    "outputId": "bb6e8a8d-35c4-4e8c-cf16-89bac2141758"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4-grTQDR0QY",
    "outputId": "efa3aee6-f08b-407e-d752-d9b221d333fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Helllo, my name is Ala\\n Hello again\\n\\ntesting newline.']\n"
     ]
    }
   ],
   "source": [
    "# Load a long document\n",
    "with open('LLM.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "\n",
    "\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5nzGYeLTbNm"
   },
   "source": [
    "The NLTKTextSplitter is not, as you noted in your context, specifically made to handle word segmentation in English sentences without spaces. Alternative libraries like pyenchant or word segment can be used for this.\n",
    "\n",
    "## SpacyTextSplitter\n",
    "\n",
    "The SpacyTextSplitter assists in breaking up huge text documents into pieces of a predefined size. For better control of massive text inputs, this is helpful. The SpacyTextSplitter is an alternative to NLTK-based sentence splitting, it is crucial to highlight. By specifying the chunk_size option, which is determined by a length function supplied to it and defaults to the number of characters, you can create a SpacyTextSplitter object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xooCnKxQSWnf",
    "outputId": "f6246710-1e99-4ae8-a009-4ffe5e87a44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Helllo, my name is Ala\\n \\n\\nHello again\\n\\ntesting newline.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "\n",
    "# Load a long document\n",
    "with open('LLM.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# Instantiate the SpacyTextSplitter with the desired chunk size\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "\n",
    "# Split the text using SpacyTextSplitter\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "\n",
    "# Print the first chunk\n",
    "print(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTOsrDcQYjza"
   },
   "source": [
    "## MarkdownTextSplitter\n",
    "\n",
    "The MarkdownTextSplitter is intended to separate text into headers, code blocks, and dividers that are written in Markdown. It is constructed as a straightforward RecursiveCharacterSplitter subclass with separators unique to Markdown. These separators are set by the Markdown syntax by default, but they can be changed by giving a list of characters when the MarkdownTextSplitter object is initialised. The length function that is provided in calculates the chunk size, which is initially set to the amount of characters. When initialising an instance, include an integer value to specify the chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rknT3Bk1Tb3z",
    "outputId": "f2cee5b7-4d85-4503-e84a-22c3264b08df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# \\n\\n# Welcome to My Blog!', metadata={}), Document(page_content='Introduction', metadata={}), Document(page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,', metadata={}), Document(page_content='Java, and JavaScript.', metadata={}), Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. JavaScript\\n3. Java\", metadata={}), Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}), Document(page_content='About this Blog', metadata={}), Document(page_content=\"In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on\", metadata={}), Document(page_content='the latest technology trends, and occasional book reviews.', metadata={}), Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}), Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"John\")\\n\\\\', metadata={}), Document(page_content='Stay tuned for more updates!', metadata={}), Document(page_content='Contact Me', metadata={}), Document(page_content='Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at', metadata={}), Document(page_content='johndoe@email.com.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "markdown_text = \"\"\"\n",
    "#\n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\"\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ2g0KI2Zb-s"
   },
   "source": [
    "The MarkdownTextSplitter provides a useful way to split text while keeping the organisation and meaning that Markdown style affords. The information can be intelligently divided into parts that are more semantically cohesive by recognising the Markdown syntax (such as headings, lists, and code blocks). When handling lengthy Markdown documents, this splitter is extremely helpful.\n",
    "\n",
    "## TokenTextSplitter\n",
    "\n",
    "Utilising TokenTextSplitter over other text splitters, such as CharacterTextSplitter, has the key benefit of respecting token bounds, preventing token splits in the middle of chunks. When using language models and embeddings, this can be very useful in preserving the text's semantic integrity.\n",
    "\n",
    "By first encoding the text as BPE (Byte Pair Encoding) tokens and then breaking these tokens into chunks, this kind of splitter breaks down raw text strings into manageable portions. The tokens included in each chunk are then put back together to form text. Using this class requires the tiktoken Python package. Pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrZxTi2IYl94",
    "outputId": "3353aec4-d822-4752-c621-40b30d303b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helllo, my name is Ala\n",
      " Hello again\n",
      "\n",
      "testing newline.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('LLM.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "# Split into smaller chunks\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GggTVv6oZeIh"
   },
   "source": [
    "The chunk_overlap parameter specifies the number of overlapping tokens between consecutive chunks, and chunk_size specifies the maximum number of BPE tokens in each chunk. You can fine-tune the granularity of the text pieces by changing these parameters.\n",
    "\n",
    "When converting text to BPE tokens and back, TokenTextSplitter may require additional computation, which could be a disadvantage. CharacterTextSplitter is a text segmentation tool that splits text into segments based on character count, making it an option if you require a quicker and easier text segmentation method.\n",
    "\n",
    "## Recap\n",
    "\n",
    "Text splitters are crucial for handling lengthy text, increasing the effectiveness of language model processing, and improving vector store search results. Choosing the splitting technique and determining the chunk size are necessary when customising text splitters. \n",
    "\n",
    "A tool that helps strike a compromise between digestible chunks and semantic context preservation is CharacterTextSplitter. The outcomes are tailored for certain use cases by experimentation with various chunk sizes and overlaps.\n",
    "\n",
    "While giving a range of adjustable chunk sizes and overlaps, RecursiveCharacterTextSplitter places a strong emphasis on maintaining semantic linkages. \n",
    "\n",
    "The Natural Language Toolkit library is used by NLTKTextSplitter to split text more precisely. Utilising the well-known SpaCy library, SpacyTextSplitter divides texts based on linguistic characteristics. Specifically designed for Markdown-formatted texts, MarkdownTextSplitter makes sure that content is divided in a way that makes sense given the grammar. TokenTextSplitter offers a fine-grained method for text segmentation by using BPE tokens for splitting.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "To get the best results for your text processing jobs, choose the right text splitter based on your unique needs and the type of text you are dealing with.\n",
    "\n",
    "Further Reading:\n",
    "\n",
    "[https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter)\n",
    "\n",
    "[https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter)\n",
    "\n",
    "[https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
