{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /deep-learning-theory/2021/06/17/neural-network-from-foundations\n",
    "categories:\n",
    "- deep-learning\n",
    "date: '2021-06-17'\n",
    "description: In this article we will cover building a basic neural network from the\n",
    "  most basic elements.\n",
    "image: https://github.com/pranath/blog/raw/master/images/deep1.jpg\n",
    "output-file: 2021-06-17-neural-network-from-foundations.html\n",
    "title: Building a Neural Network from the Foundations\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o19LFUoXbLV3",
    "outputId": "8cc197bf-8705-4d60-bdf8-17680d708583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 727kB 7.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.2MB 14.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 194kB 33.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 10.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 10.9MB/s \n",
      "\u001b[?25hMounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVC4qp68b3Mq"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this article we will cover building a basic neural network from the most basic elements (arrays and Pytorch modules). We will also cover some of the key theory required for this.\n",
    "\n",
    "This article and it's content is based on the [fastai deep learning course, chapter 17.](https://github.com/fastai/fastbook/blob/master/17_foundations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49vKZ2yHcRs9"
   },
   "source": [
    "## Building a Neural Network from basic elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hQAe7Fcchmy"
   },
   "source": [
    "### Creating a neuron\n",
    "\n",
    "A neuron takes a series of inputs, each of which is multipled by a weight, summing up all those inputs, and adding a bias - this input is then put thorugh an activation function. We could represent these as:\n",
    "\n",
    "output = sum([x*w for x,w in zip(inputs,weights)]) + bias\n",
    "\n",
    "def relu(x): return x if x >= 0 else 0\n",
    "\n",
    "A deep learning model stacks many of these neurons in layers. So for the output of an entire layer, using matrices we would have:\n",
    "\n",
    "y = x @ w.t() + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv1fybdUcn3U"
   },
   "source": [
    "### Matrix multiplication\n",
    "\n",
    "So we can define a function to manually do a matrix product using loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PxDWvYjZQ9G8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "def matmul(a,b):\n",
    "    ar,ac = a.shape # n_rows * n_cols\n",
    "    br,bc = b.shape\n",
    "    assert ac==br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVcGUordREMK"
   },
   "source": [
    "However this is hugely slower than we can do using Pytorch matrix multiplciation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWD2NQYicvH2"
   },
   "source": [
    "### Elementwise calculations\n",
    "\n",
    "We can do element wise operations on tensors - as long as they are the same shape, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVrAnTAZRaOc",
    "outputId": "b5072471-0a64-44d7-c976-e6dfacf2e540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 14.,  3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tensor([10., 6, -4])\n",
    "b = tensor([2., 8, 7])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb860XBwc0MF"
   },
   "source": [
    "### Broadcasting\n",
    "\n",
    "Broadcasting allows 2 arrays of different sizes to be compatible for arthimetic operations, by repeating the smaller array so it matches the size of the larger one.\n",
    "\n",
    "For example we can use *unsqeeze* in Pytorch to add extra dimensions explictly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sXGggFAS5ic",
    "outputId": "a40814e7-86dc-4fb7-927a-1cdc9572b7ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tensor([10.,20,30])\n",
    "c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsq31tn2TCP3"
   },
   "source": [
    "We can now replace our matrix multiplication with 3 loops with a broadcasting equivilent much shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m9-UM_bATJlM"
   },
   "outputs": [],
   "source": [
    "def matmul(a,b):\n",
    "    ar,ac = a.shape\n",
    "    br,bc = b.shape\n",
    "    assert ac==br\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n",
    "        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c__DCCKEc82C"
   },
   "source": [
    "## Forward and Backward passes of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_Bh5o0dENc"
   },
   "source": [
    "### Defining and initialising a layer\n",
    "\n",
    "So we can define a basic linear layer in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UgOof6qjUWKy"
   },
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBgyfa5pUgvc"
   },
   "source": [
    "Let's create some dummy data, and some simple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eM4o65KUpND",
    "outputId": "84004a19-9f7e-4e5a-af47-fcdbc18a0fb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200, 100)\n",
    "y = torch.randn(200)\n",
    "\n",
    "w1 = torch.randn(100,50)\n",
    "b1 = torch.zeros(50)\n",
    "w2 = torch.randn(50,1)\n",
    "b2 = torch.zeros(1)\n",
    "\n",
    "l1 = lin(x, w1, b1)\n",
    "l1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqSMRHKEU6SQ"
   },
   "source": [
    "But we have a problem to do with how the parameters are initialised consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCIu3z2TU_9l",
    "outputId": "42e8ab22-bfc2-4552-d80c-ba1931545fc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.2733), tensor(10.1770))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.mean(), l1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hitDYwCmVC6t"
   },
   "source": [
    "The std dev is 10, consider how if this is one layer which multiples by 10 how many layers could generate huge numbers that would be unmanagable and be a network hard to train. So we want our std dev to be close to one, and there is an equation for scaling our weights to this is so. \n",
    "\n",
    "$1/\\sqrt{n_{in}}$\n",
    "\n",
    "where $n_{in}$ represents the number of inputs. This is known as *Xavier initialization (or Glorot initialization)*.\n",
    "\n",
    "For example if we have 100 inputs, we should scale our weights by 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPaJHnreVywZ",
    "outputId": "166617af-7cb8-49b8-8aa1-d9727e259457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6374, -0.3009,  0.4669, -0.7221,  0.1983],\n",
      "        [-1.0054,  0.0244,  0.3540, -1.0580,  0.2675],\n",
      "        [ 0.0789,  0.6670,  0.2132,  0.2511, -1.3466],\n",
      "        [ 0.7786, -0.2874, -1.2391,  0.4132,  1.9071],\n",
      "        [ 2.1194,  0.0046, -1.7749,  1.5797,  1.4981]])\n",
      "tensor(1.1794)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(200, 100)\n",
    "for i in range(50): x = x @ (torch.randn(100,100) * 0.1)\n",
    "print(x[0:5,0:5])\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ui5MUoMxWXH0"
   },
   "source": [
    "Re-working our model with this in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOSn-ecpWX8A",
    "outputId": "dda41507-b8be-45c3-ac2f-907525afff20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0135), tensor(1.0176))"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200, 100)\n",
    "y = torch.randn(200)\n",
    "\n",
    "from math import sqrt\n",
    "w1 = torch.randn(100,50) / sqrt(100)\n",
    "b1 = torch.zeros(50)\n",
    "w2 = torch.randn(50,1) / sqrt(50)\n",
    "b2 = torch.zeros(1)\n",
    "\n",
    "l1 = lin(x, w1, b1)\n",
    "l1.mean(),l1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcQ2RM8-Wrsq"
   },
   "source": [
    "Now we need to define an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSzQCSS0Wvx0",
    "outputId": "c96fefb9-fd65-416b-dc1f-1feb09f3ee37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3988), tensor(0.5892))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relu(x): return x.clamp_min(0.)\n",
    "\n",
    "l2 = relu(l1)\n",
    "l2.mean(),l2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "296t5K_oW24y"
   },
   "source": [
    "So now the mean is no longer zero and our std dev is less like 1. So the Glorot method is not intended to be used with Relu and was invented before.\n",
    "\n",
    "A newer initialisation by Kaiming He et al workes better with Relu. It's formula is:\n",
    "\n",
    "$\\sqrt{2 / n_{in}}$\n",
    "\n",
    "where $n_{in}$ is the number of inputs of our model.\n",
    "\n",
    "Applying this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2iNdtAtXcUE",
    "outputId": "5bddbe31-eff7-405f-dbf5-5b9c3092f81b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5710), tensor(0.8222))"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200, 100)\n",
    "y = torch.randn(200)\n",
    "\n",
    "w1 = torch.randn(100,50) * sqrt(2 / 100)\n",
    "b1 = torch.zeros(50)\n",
    "w2 = torch.randn(50,1) * sqrt(2 / 50)\n",
    "b2 = torch.zeros(1)\n",
    "\n",
    "l1 = lin(x, w1, b1)\n",
    "l2 = relu(l1)\n",
    "l2.mean(), l2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiXnogBcXs6h"
   },
   "source": [
    "Now we can define a whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXlIcXW_XvKE",
    "outputId": "3a1fa358-599e-4e25-fdb2-45d5c69ff191"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(x):\n",
    "    l1 = lin(x, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    l3 = lin(l2, w2, b2)\n",
    "    return l3\n",
    "\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHuragp7X0-i"
   },
   "source": [
    "So we don't want this unit dimension. We can define a loss function and also get rid of this unit dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2VKHR9rAX-1g"
   },
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\n",
    "\n",
    "loss = mse(out, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "artz0Y_7dHyM"
   },
   "source": [
    "### Gradients and the Backwards Pass\n",
    "\n",
    "So PyTorch computes the gradients for us with *loss.backward* but behind the scenes is a bit of calculus. Given the whole network is a huge function, with each part a sub-function, lets start with the final part the loss function.\n",
    "\n",
    "We can calculate the loss with the loss function. If we take the derivative of the loss function with respect to the final weights, we can calculate the loss with respect to these weights. We can then use the chain rule to propagate these values backward, and calculate the loss with respect to every parameter in the model.\n",
    "\n",
    "Lets define a function to calculate the gradients of the loss function with respect to the final weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3S_ANByeZWKt"
   },
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ): \n",
    "    # grad of loss with respect to output of previous layer\n",
    "    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjVH7uUOZZ-7"
   },
   "source": [
    "Let's now define functions to calculate the gradients for the activation functions and also the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Wjk6CJviZl8w"
   },
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    # grad of relu with respect to input activations\n",
    "    inp.g = (inp>0).float() * out.g\n",
    "\n",
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = inp.t() @ out.g\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kn4Wzv8FdW_t"
   },
   "source": [
    "### Model refactoring\n",
    "\n",
    "Let's now put together everything: the model, the forward and backward pass methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0Rx1-gQaqbl",
    "outputId": "499de426-9687-48a7-e564-755f15208dc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7466)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g\n",
    "\n",
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp@self.w + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)\n",
    "\n",
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n",
    "        self.inp.g = 2.*x/self.targ.shape[0]\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()\n",
    "\n",
    "# Create model\n",
    "model = Model(w1, b1, w2, b2)\n",
    "\n",
    "# Forward pass\n",
    "loss = model(x, y)\n",
    "\n",
    "# Backward pass\n",
    "model.backward()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn3K1o2xdZzr"
   },
   "source": [
    "### Converting the model to Pytorch\n",
    "\n",
    "We could build this more simply using Pytorch methods, and in fact fastai methods built on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DP9fq3GGbjln"
   },
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n",
    "        self.loss = mse\n",
    "        \n",
    "    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkOfbBcNdi6f"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this article we have build a neural network from the most basic elements."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "neural_network_from_foundations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
