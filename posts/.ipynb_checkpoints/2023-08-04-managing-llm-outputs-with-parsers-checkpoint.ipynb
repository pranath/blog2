{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-04'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    " - prompt-engineering\n",
    "title: Managing Large Language Model Outputs with Parsers\n",
    "description: This article covers the different types of parsing objects used for LLMs and the troubleshooting processing\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake4.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In a production setting, a predictable data structure is always desired even when language models can only produce textual outputs. Imagine, for instance, that you are developing a thesaurus application and want to provide a list of potential synonyms depending on the context. The LLMs are strong enough to produce a lot of proposals quickly.\n",
    "\n",
    "Lack of a dynamic mechanism to retrieve relevant data from the stated string is the issue. You might argue that we can ignore the first two lines and break the response up by a new line. However, there is no assurance that the format of the response will always be the same. There may or may not be an introduction line, depending on the list.\n",
    "\n",
    "The Output Parsers assist in building a data structure that properly defines what should be expected from the output. In the instance of the word recommendation application, we can request a list of words or a combination of multiple characteristics, such as a word and an explanation of why it fits. The expected data can be extracted for you by the parser.\n",
    "\n",
    "The various categories of object parsing and troubleshooting processing are covered in this article.\n",
    "\n",
    "## Import Libs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aU3Ywhwcyu1m",
    "outputId": "44f0e8bc-c01c-46a2-d5c4-28baedd08dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m745.7/745.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -q langchain==0.0.208 openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzQIJO7t1SPK"
   },
   "outputs": [],
   "source": [
    "!echo \"OPENAI_API_KEY='<API_KEY>'\" > .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOgAy2kF00W8",
    "outputId": "3c02ea78-b318-496c-910d-23721f1b6131"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo1f1Z6fUxJv"
   },
   "source": [
    "## Output Parsers\n",
    "\n",
    "In this section, we'll give you an introduction to three classes. Even if the Pydrantic parser is the most potent and adaptable wrapper, it is still useful to be aware of the alternatives for simpler issues. To further comprehend the specifics of each strategy, we shall use the thesaurus application in each segment.\n",
    "\n",
    "### PydanticOutputParser\n",
    "\n",
    "This class instructs the model to generate its output in a JSON format and then extract the information from the response. You will be able to treat the parser’s output as a list, meaning it will be possible to index through the results without worrying about formatting.\n",
    "\n",
    "**It is important to note that not all models have the same capability in terms of generating JSON outputs. So, it would be best to use a more powerful model (like OpenAI’s DaVinci instead of Curie) to get the most satisfactory result.**\n",
    "\n",
    "The Pydantic package, which aids in the creation and validation of data structures in Python, is used by this class. It allows us to give the anticipated output a name, type, and description. In the case of the thesaurus, we require a variable that can hold numerous suggestions. By creating a class that derives from the Pydantic's BaseModel class, it is simple to accomplish. The following command should be used to install the necessary packages: Installing the langchain==0.0.208 deeplake openai tiktoken package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "\n",
    "    # Throw error in case of receiving a numbered-list from API\n",
    "    @validator('words')\n",
    "    def not_start_with_number(cls, field):\n",
    "        for item in field:\n",
    "            if item[0].isnumeric():\n",
    "                raise ValueError(\"The word can not start with numbers!\")\n",
    "        return field\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By building the Suggestions schema class, we always import and reference the required libraries. This class has the following two crucial components:\n",
    "    \n",
    "1. Expected Outputs: Each output is defined by declaring a variable with desired type, like a list of strings (: List[str]) in the sample code, or it could be a single string (: str) if you are expecting just one word/sentence as the response. Also, It is required to write a simple explanation using the Field function’s description attribute to help the model during inference. (We will see an example of having multiple outputs later in the lesson)\n",
    "2. Validators: It is possible to declare functions to validate the formatting. We ensure that the first character is not a number in the sample code. The function’s name is unimportant, but the @validator decorator must receive the same name as the variable you want to approve. (like @validator(’words’)) It is worth noting that the field variable inside the validator function will be a list if you specify it as one.\n",
    "\n",
    "We will pass the created class to the PydanticOutputParser wrapper to make it a LangChain parser object. The next step is to prepare the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the specified target_word based the presented context.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format_prompt(\n",
    "\t\t\ttarget_word=\"behaviour\",\n",
    "\t\t\tcontext=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template variable is a string that can have named index placeholders using the following variable_name format, as was covered in earlier articles. The template describes what we anticipate from the model, including how the inputs and parser should be formatted. The template string containing information about the kind of each placeholder is sent to the prompt template. They might either be partial_variables that need to be initialised immediately or input_variables whose value is initialised later using the.format_prompt() method. \n",
    "\n",
    "\n",
    "The prompt can use LangChain's OpenAI wrapper to deliver the query to models like GPT. (Remember to set the environment variables OPENAI_API_KEY with your OpenAI API key.) To obtain the best results, we are employing the Davinci model, one of the most potent possibilities, and setting the temperature value to 0, which makes the results repeatable.\n",
    "\n",
    "**The temperature value might range from 0 to 1, with a higher number indicating a more imaginative model. If you work on projects that call for creative output, using larger value in production is a beneficial practise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)\n",
    "\n",
    "output = model(model_input.to_string())\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1Lc1aD0giWW"
   },
   "source": [
    "The parser object’s parse() function will convert the model’s string response to the format we specified. There is a list of words that you can index through and use in your applications.\n",
    "\n",
    "#### Multiple Outputs Example\n",
    "\n",
    "Here is some sample Python code that processes numerous outputs. It asks the model to provide a list of words and the justifications for each claim.\n",
    "\n",
    "To execute this example, substitute the template variable and Suggestion class with the following scripts. The model will be asked to explain its reasoning in response to template modifications, and the suggestion class specifies a new output called reasons. The validator function also modifies the output to guarantee that each line of reasoning concludes with a dot. The validator function may also be used to manipulate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBYOEFr3gtJq"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitute the specified target_word based on the presented context and the reasoning for each word.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRJre1bXgiEr"
   },
   "outputs": [],
   "source": [
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "    \n",
    "    @validator('words')\n",
    "    def not_start_with_number(cls, field):\n",
    "      for item in field:\n",
    "        if item[0].isnumeric():\n",
    "          raise ValueError(\"The word can not start with numbers!\")\n",
    "      return field\n",
    "    \n",
    "    @validator('reasons')\n",
    "    def end_with_dot(cls, field):\n",
    "      for idx, item in enumerate( field ):\n",
    "        if item[-1] != \".\":\n",
    "          field[idx] += \".\"\n",
    "      return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_Odwb2Vgq6i"
   },
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usHAhjsJg2YG"
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format_prompt(target_word=\"behaviour\", context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vfz6lxXOg6vq"
   },
   "outputs": [],
   "source": [
    "output = model(model_input.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1R-flPY0g8E-",
    "outputId": "23decf92-2ef0-48ab-d6e0-c016b58c3ba3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner', 'demeanor', 'comportment'], reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation.', 'refers to the way someone behaves in a particular situation.', 'refers to the way someone behaves in a particular situation.'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kX-OJMQZHyB"
   },
   "source": [
    "### CommaSeparatedListOutputParser\n",
    "\n",
    "This class's name makes it clear that it handles comma-separated outputs. It handles one particular situation: whenever you request a list of the model's outputs. Importing the relevant module will be the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9SXjmZkU9Rz"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngM4gJ2iZJ1P"
   },
   "source": [
    "The parser does not require a setting up step. Therefore it is less flexible. We can create the object by calling the class. The rest of the process for writing the prompt, initializing the model, and parsing the output is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_5y6jTTX0q2"
   },
   "outputs": [],
   "source": [
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_5lp0_cYGKG"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the word '{target_word}' based the presented the following text: {context}.\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MX9mnMoCX4IQ"
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format(\n",
    "  target_word=\"behaviour\",\n",
    "  context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8GOxGjjX4ks"
   },
   "outputs": [],
   "source": [
    "model_name = 'text-davinci-003'\n",
    "temperature = 0.0\n",
    "\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRyegkT1X7dn"
   },
   "outputs": [],
   "source": [
    "output = model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMU4iXnJfPBh",
    "outputId": "38c34572-06fb-4188-e277-b52a5f8ed96e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conduct',\n",
       " 'Actions',\n",
       " 'Demeanor',\n",
       " 'Mannerisms',\n",
       " 'Attitude',\n",
       " 'Performance',\n",
       " 'Reactions',\n",
       " 'Interactions',\n",
       " 'Habits',\n",
       " 'Repertoire',\n",
       " 'Disposition',\n",
       " 'Bearing',\n",
       " 'Posture',\n",
       " 'Deportment',\n",
       " 'Comportment']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two sections of the sample code might need attention, even though the majority of it was covered in the preceding subsection. In order to demonstrate multiple approaches to writing a prompt, we first tested a new format for the prompt's template. The second is that the model's input is produced using.format() rather than.format_prompt(). The key change from the code in the preceding part is that since the prompt is already of the string type, we no longer need to invoke the.to_string() object.\n",
    "\n",
    "As you can see, the result is a list of words with some overlaps and more variation than the PydanticOutputParser method. However, it is not possible to request extra reasoning data using the CommaSeparatedOutputParser class.\n",
    "\n",
    "### StructuredOutputParser\n",
    "\n",
    "The LangChain team has just implemented its first output parser. However, it only accepts texts and does not offer alternatives for other data kinds, such as lists or integers, while being able to process numerous outputs. When you only want one response from the model, you can utilise it. For instance, the thesaurus application should only have one alternative term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"words\", description=\"A substitue word based on context\"),\n",
    "    ResponseSchema(name=\"reasons\", description=\"the reasoning of why this word fits the context.\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvDFFFJg7owk"
   },
   "source": [
    "How to define a schema is shown in the code above. But we won't get into specifics here. The PydanticOutputParser class offers validation and more flexibility for more complicated activities, and the CommaSeparatedOutputParser option covers simpler applications, therefore this class has no advantage.\n",
    "\n",
    "## Fixing Errors\n",
    "\n",
    "The parsers are effective tools for dynamically extracting the data from the prompt and partially validating it. They cannot, however, promise a response. Imagine that once you've launched your application, the parser throws an error because the model's answer [to a user's request] isn't complete. It's not the best! We shall present two fail-safe classes in the subsections that follow. To help correct the inaccuracies, they build a layer on top of the model's answer.\n",
    "\n",
    "**The PydanticOutputParser class, which is the only one containing a validation method, is compatible with the following strategies.**\n",
    "\n",
    "### OutputFixingParser\n",
    "\n",
    "By examining the model's answer and the preceding parser, this approach seeks to correct the parsing error. The problem is resolved using a Large Language Model (LLM). To stay consistent with the rest of the tutorial, we'll use GPT-3, although you can pass any model that is currently supported. Let's begin by outlining the Pydantic data schema and then demonstrate a possible issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUOr9lNE7rNe"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMozaMOz-EEX"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8icANHJADeP"
   },
   "outputs": [],
   "source": [
    "model_name = 'text-davinci-003'\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoaaITzR-LSB"
   },
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaf084y9__G8"
   },
   "source": [
    "#### Example can fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BH3_H3BvBAuc"
   },
   "outputs": [],
   "source": [
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "auvd-X1gBAn5",
    "outputId": "3f406479-9cf3-4350-cade-66e0b4764ec9"
   },
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-5597a0ca70b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissformatted_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser properly spotted a mistake in our sample response (missformatted_output), as you can see in the error message, because we used the word reasoning instead of the anticipated reasons key. It would be simple to correct this problem using the OutputFixingParser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0U6zhY_BAfT"
   },
   "outputs": [],
   "source": [
    "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-S4HyELBuN6",
    "outputId": "b7525722-d220-4322-f719-ef7efb9d7f18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation.'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputfixing_parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro4gfYu5BwYo"
   },
   "source": [
    "#### Example can NOT fix\n",
    "\n",
    "The old parser and a language model are input arguments for the from_llm() method. It then creates a new parser for you that can correct output errors. In this instance, it was able to recognise the incorrect key and update it to the one we defined.\n",
    "\n",
    "However, it is not always possible to resolve the problems using this class. Here is an illustration of how to fix a missing key mistake using the OutputFixingParser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiVpYRA_-N_3"
   },
   "outputs": [],
   "source": [
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "DG7bQwWc-qeO",
    "outputId": "39f6e423-9c24-4f29-f2a8-279da7617612"
   },
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-5597a0ca70b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissformatted_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7U9CJr1_OD1"
   },
   "outputs": [],
   "source": [
    "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgycYLR7_gD9",
    "outputId": "14fef061-8ea8-4abb-9a57-e590df0c88d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=[\"The word 'conduct' implies a certain behavior or action, while 'manner' implies a polite or respectful way of behaving.\"])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputfixing_parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvknCvhpB9u4"
   },
   "source": [
    "It is clear from the output that the model understood the important factors that were absent from the response but was missing the context of the intended result. While we anticipate one rationale for each word, it produced a list with only one entry. We occasionally need to use the RetryOutputParser class because of this.\n",
    "\n",
    "### RetryOutputParser\n",
    "\n",
    "As shown in the last section, the parser occasionally needs access to both the output and the prompt in order to analyse the entire context. We must define the aforementioned variables first. The LLM model, parser, and prompt, which were previously described in greater depth, are initialised by the ensuing scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTpm-zyCCppn"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import RetryWithErrorOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95BsYWA-Cs19"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjoMnUDbCso2"
   },
   "outputs": [],
   "source": [
    "model_name = 'text-davinci-003'\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNbiNpSMB_Gx"
   },
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqZdjApXCWeR"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the specified target_word based the presented context and the reasoning for each word.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format_prompt(target_word=\"behaviour\", context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCBYLqS0CWW_"
   },
   "outputs": [],
   "source": [
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "-B02plpFCWUI",
    "outputId": "974b1b17-8a24-4e1a-ca00-1d2391f5d36e"
   },
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-5597a0ca70b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissformatted_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the RetryWithErrorOutputParser class, we can now correct the same missformatted_output. As we saw in the previous part, it is given the old parser and a model to declare the new parser object. The parse_with_prompt function, which requires both the output and the prompt, is in charge of resolving the parsing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cevr_NbACWPL"
   },
   "outputs": [],
   "source": [
    "retry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHZ1U-DFC3oa",
    "outputId": "c3c7305e-a916-468f-ecda-193d5f975415"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=[\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'conduct' is a suitable substitute.\", \"The students' behaviour was inappropriate, so 'manner' is a suitable substitute.\"])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retry_parser.parse_with_prompt(missformatted_output, model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results demonstrate that the RetryOuputParser can resolve problems that the OuputFixingParser was unable to. The model generated one rationale for each word thanks to the parser's accurate guidance.\n",
    "\n",
    "The try:... except:... method is the best way to use these strategies in production to capture the parsing mistake. It implies that utilising the aforementioned classes, we can attempt to remedy the issues that are caught in the except section. It will reduce the amount of API requests and prevent extra expenses that come with them.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We learned how to validate and extract the information in an easy-to-use format from the language models’ responses which are always a string. Additionally, we reviewed LangChain’s fail-safe procedures to guarantee the consistency of the output. Combining these approaches will help us write more reliable applications in production environments. In the next lesson, we will learn how to build a knowledge graph and capture useful information or entities from texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37k8ZOqMDULn"
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
