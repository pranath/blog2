{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b6367a18-9af3-4e6d-bf7d-13d1d39ae0ce",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-06-23'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - openai\n",
    "title: Checking Outputs of Large Language Models like ChatGPT\n",
    "description: In this article we will focus on checking outputs generated by an LLM before showing them to users - which can be important for ensuring the quality, relevance, and safety of the responses provided to them or used in automation flows\n",
    "image: https://github.com/pranath/blog/raw/master/images/chatgpt3.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8adb9f4-f9ed-4c9f-8495-049c16548003",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Large language models such as [ChatGPT](https://openai.com/blog/chatgpt) can generate text responses based on a given prompt or input. Writing prompts allow users to guide the language model's output by providing a specific context or topic for the response. This feature has many practical applications, such as generating creative writing prompts, assisting in content creation, and even aiding in customer service chatbots. \n",
    "\n",
    "In [earlier articles](/#category=openai) i've looked at how you can use ChatGPT to solve some of these tasks with simple prompts. But in many use cases, what is required is not just one prompt but a sequence of prompts where we need to also consider the outputs at each stage, before providing a final output - for example with a customer service chatbot.\n",
    "\n",
    "In this article, we'll focus on checking outputs \n",
    "generated by an LLM. Checking outputs before showing them to users can be \n",
    "important for ensuring the quality, relevance and safety \n",
    "of the responses provided to them or used in automation flows. \n",
    "We'll learn how to use the ChatGPT OpenAI moderation API, but \n",
    "this time for outputs, and how to use additional \n",
    "prompts to the model to evaluate output quality \n",
    "before displaying them.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Load the API key and relevant Python libaries.\n",
    "\n",
    "First we need to load certain python libs and connect the OpenAi api.\n",
    "\n",
    "The OpenAi api library needs to be configured with an account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
    " ```\n",
    " !export OPENAI_API_KEY='sk-...'\n",
    " ```\n",
    "\n",
    "Or, set `openai.api_key` to its value:\n",
    "\n",
    "```\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af3568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a858d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d3db3-2258-48c9-819e-cfd8780416e3",
   "metadata": {},
   "source": [
    "## Check output for potentially harmful content\n",
    "\n",
    "We've previously looked at the [moderation API in the context \n",
    "of evaluating inputs](https://livingdatalab.com/posts/2023-06-20-evaluating-moderation-inputs-large-language-models.html). Let's go over it once more in the context of examining outputs. \n",
    "The outputs produced by the system itself can also be filtered and moderated using the ChatGPT Moderation API. So, let me give you an example. Let's check to see if this output is flagged now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6164c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"categories\": {\n",
      "    \"hate\": false,\n",
      "    \"hate/threatening\": false,\n",
      "    \"self-harm\": false,\n",
      "    \"sexual\": false,\n",
      "    \"sexual/minors\": false,\n",
      "    \"violence\": false,\n",
      "    \"violence/graphic\": false\n",
      "  },\n",
      "  \"category_scores\": {\n",
      "    \"hate\": 4.313069e-07,\n",
      "    \"hate/threatening\": 5.590539e-10,\n",
      "    \"self-harm\": 2.91932e-10,\n",
      "    \"sexual\": 2.1767946e-06,\n",
      "    \"sexual/minors\": 1.2402804e-08,\n",
      "    \"violence\": 5.962453e-06,\n",
      "    \"violence/graphic\": 4.4420557e-07\n",
      "  },\n",
      "  \"flagged\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "final_response_to_customer = f\"\"\"\n",
    "The SmartX ProPhone has a 6.1-inch display, 128GB storage, \\\n",
    "12MP dual camera, and 5G. The FotoSnap DSLR Camera \\\n",
    "has a 24.2MP sensor, 1080p video, 3-inch LCD, and \\\n",
    "interchangeable lenses. We have a variety of TVs, including \\\n",
    "the CineView 4K TV with a 55-inch display, 4K resolution, \\\n",
    "HDR, and smart TV features. We also have the SoundMax \\\n",
    "Home Theater system with 5.1 channel, 1000W output, wireless \\\n",
    "subwoofer, and Bluetooth. Do you have any specific questions \\\n",
    "about these products or any other products we offer?\n",
    "\"\"\"\n",
    "response = openai.Moderation.create(\n",
    "    input=final_response_to_customer\n",
    ")\n",
    "moderation_output = response[\"results\"][0]\n",
    "print(moderation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde612c-383e-444a-96d6-51c4f42dfb51",
   "metadata": {},
   "source": [
    "You can see that this output is not flagged and gets extremely low ratings in every category, which is understandable given the content. Checking the outputs can be crucial generally speaking. \n",
    "Use lower criteria for output flagging, for instance, if you were developing a chatbot for sensitive audiences. \n",
    "Generally speaking, if the moderation output shows that the material has been marked, you can reply appropriately by providing a fallback response or by creating a new response. \n",
    "As the models get better, it should be noted that the likelihood that they would produce a negative result is decreasing. \n",
    "\n",
    "## Check if output is factually based on the provided product information\n",
    "\n",
    "Asking the model directly if the results were satisfactory and met the criteria you defined is another method for ensuring the quality of outputs. \n",
    "This can be achieved by giving the model the generated output as input and asking it to assess the output's quality. \n",
    "There are numerous methods you can accomplish this. let's look at an example. \n",
    "\n",
    "So, say our system message is:\n",
    "\n",
    ">\"You are an assistant that evaluates \n",
    "whether customer service agent responses sufficiently \n",
    "answer customer questions and also validates that all the \n",
    "facts the assistant cites from the product information are correct. \n",
    "The product information and user and customer service agent \n",
    "messages will be delivered by three backticks. Respond with a Y or \n",
    "N character with no punctuation. Y if the \n",
    "output sufficiently answers the question and \n",
    "the response correctly uses product information \n",
    "and N otherwise. Output a single letter only.\".\n",
    "\n",
    "And you could also use a chain of thought reasoning prompt for this. You could experiment with this since the model might find it challenging to validate both in one go. You could also provide other kinds of rules. \n",
    "You may provide a question or provide a rubric, such as one for marking an essay or an exam. \n",
    "If it's something that's really important to you, you could utilise that structure and ask whether the tone used here is in keeping with our brand rules. You could also express some of your brand guidelines. \n",
    "\n",
    "And now we'll define our comparison. So the customer message is \n",
    "the customer message, the product information, and \n",
    "then the agent response, which is the response to \n",
    "the customer that we have from this previous cell. \n",
    "So let's format this into a messages list and get \n",
    "the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6e394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n"
     ]
    }
   ],
   "source": [
    "system_message = f\"\"\"\n",
    "You are an assistant that evaluates whether \\\n",
    "customer service agent responses sufficiently \\\n",
    "answer customer questions, and also validates that \\\n",
    "all the facts the assistant cites from the product \\\n",
    "information are correct.\n",
    "The product information and user and customer \\\n",
    "service agent messages will be delimited by \\\n",
    "3 backticks, i.e. ```.\n",
    "Respond with a Y or N character, with no punctuation:\n",
    "Y - if the output sufficiently answers the question \\\n",
    "AND the response correctly uses product information\n",
    "N - otherwise\n",
    "\n",
    "Output a single letter only.\n",
    "\"\"\"\n",
    "customer_message = f\"\"\"\n",
    "tell me about the smartx pro phone and \\\n",
    "the fotosnap camera, the dslr one. \\\n",
    "Also tell me about your tvs\"\"\"\n",
    "product_information = \"\"\"{ \"name\": \"SmartX ProPhone\", \"category\": \"Smartphones and Accessories\", \"brand\": \"SmartX\", \"model_number\": \"SX-PP10\", \"warranty\": \"1 year\", \"rating\": 4.6, \"features\": [ \"6.1-inch display\", \"128GB storage\", \"12MP dual camera\", \"5G\" ], \"description\": \"A powerful smartphone with advanced camera features.\", \"price\": 899.99 } { \"name\": \"FotoSnap DSLR Camera\", \"category\": \"Cameras and Camcorders\", \"brand\": \"FotoSnap\", \"model_number\": \"FS-DSLR200\", \"warranty\": \"1 year\", \"rating\": 4.7, \"features\": [ \"24.2MP sensor\", \"1080p video\", \"3-inch LCD\", \"Interchangeable lenses\" ], \"description\": \"Capture stunning photos and videos with this versatile DSLR camera.\", \"price\": 599.99 } { \"name\": \"CineView 4K TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-4K55\", \"warranty\": \"2 years\", \"rating\": 4.8, \"features\": [ \"55-inch display\", \"4K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"A stunning 4K TV with vibrant colors and smart features.\", \"price\": 599.99 } { \"name\": \"SoundMax Home Theater\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"SoundMax\", \"model_number\": \"SM-HT100\", \"warranty\": \"1 year\", \"rating\": 4.4, \"features\": [ \"5.1 channel\", \"1000W output\", \"Wireless subwoofer\", \"Bluetooth\" ], \"description\": \"A powerful home theater system for an immersive audio experience.\", \"price\": 399.99 } { \"name\": \"CineView 8K TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-8K65\", \"warranty\": \"2 years\", \"rating\": 4.9, \"features\": [ \"65-inch display\", \"8K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"Experience the future of television with this stunning 8K TV.\", \"price\": 2999.99 } { \"name\": \"SoundMax Soundbar\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"SoundMax\", \"model_number\": \"SM-SB50\", \"warranty\": \"1 year\", \"rating\": 4.3, \"features\": [ \"2.1 channel\", \"300W output\", \"Wireless subwoofer\", \"Bluetooth\" ], \"description\": \"Upgrade your TV's audio with this sleek and powerful soundbar.\", \"price\": 199.99 } { \"name\": \"CineView OLED TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-OLED55\", \"warranty\": \"2 years\", \"rating\": 4.7, \"features\": [ \"55-inch display\", \"4K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"Experience true blacks and vibrant colors with this OLED TV.\", \"price\": 1499.99 }\"\"\"\n",
    "q_a_pair = f\"\"\"\n",
    "Customer message: ```{customer_message}```\n",
    "Product information: ```{product_information}```\n",
    "Agent response: ```{final_response_to_customer}```\n",
    "\n",
    "Does the response use the retrieved information correctly?\n",
    "Does the response sufficiently answer the question\n",
    "\n",
    "Output Y or N\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_message},\n",
    "    {'role': 'user', 'content': q_a_pair}\n",
    "]\n",
    "\n",
    "response = get_completion_from_messages(messages, max_tokens=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ee6aba-3b33-4cc8-9202-3a80ad63c21c",
   "metadata": {},
   "source": [
    "As a result, the model responds that the question has been adequately addressed and the product information is accurate. \n",
    "In general, it is preferable to utilise a more sophisticated model for these types of evaluation tasks because they are simply more logical. As a result, consider GPT-4. \n",
    "Let's use one more example. \n",
    "\n",
    "So say a example response is:\n",
    "\n",
    "> \"life is like a box of chocolates\". \n",
    "\n",
    "So let's add our message to do with the output checking. \n",
    "And the model has determined that this does not \n",
    "sufficiently answer the question or use \n",
    "the retrieved information. \n",
    "This question:\n",
    "\n",
    ">\"does it use the retrieved information correctly?\"\n",
    "\n",
    "**This is a good prompt to use if you want \n",
    "to make sure that the model isn't hallucinating, which \n",
    "is making up things that aren't true.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c33911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n"
     ]
    }
   ],
   "source": [
    "another_response = \"life is like a box of chocolates\"\n",
    "q_a_pair = f\"\"\"\n",
    "Customer message: ```{customer_message}```\n",
    "Product information: ```{product_information}```\n",
    "Agent response: ```{another_response}```\n",
    "\n",
    "Does the response use the retrieved information correctly?\n",
    "Does the response sufficiently answer the question?\n",
    "\n",
    "Output Y or N\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_message},\n",
    "    {'role': 'user', 'content': q_a_pair}\n",
    "]\n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4d168-9038-4e53-89c7-6ed08ab4d333",
   "metadata": {},
   "source": [
    "As you can see, the model can give feedback on the quality of an output that is generated, and you can use this feedback to determine whether to show the output to the user or to create a new answer. You could even try creating numerous model responses for each user inquiry and letting the model decide which one to present to the user. There are lots of different things you could try. \n",
    "\n",
    "**In general, checking outputs using the \n",
    "moderation API is good practice, while asking the model to \n",
    "evaluate its own output might be useful for immediate \n",
    "feedback to ensure the quality of responses in a very small number \n",
    "of cases.**\n",
    "\n",
    "It's probably unnecessary most of the time, especially \n",
    "if you're using a more advanced model like GPT-4. \n",
    " \n",
    "It's unlikely to be appropriate in production as it would also increase the latency \n",
    "and cost of your system, because you'd have to wait for an additional \n",
    "call for the model, and that's also additional tokens. If it's really \n",
    "important for your app or product that your error rate \n",
    "is 0.0000001%, then maybe you should try this approach. But overall, we wouldn't \n",
    "really recommend that you do this in practice.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [Building Systems with the ChatGPT API Course](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/) by DeepLearning.ai and OpenAI - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69130b4-9f24-45aa-8159-43976b76dc65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
