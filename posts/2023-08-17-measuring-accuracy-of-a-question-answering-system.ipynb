{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8e059cbe-7523-4419-85c0-29bc20500551",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-17'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - openai\n",
    " - llm-evaluation\n",
    "title: Measuring the Accuracy of an LLM based Question and Answering System\n",
    "description: Evaluating a question and response system can help you improve its system design as well as the prompt and model quality. We tend to improve what we can measure, therefore verifying for correctness is a key focus. In this post, we will utilise LangSmith to test the accuracy of a Q&A system against an example dataset.\n",
    "image: https://github.com/pranath/blog/raw/master/images/ai-eval2.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Evaluating a question and response system can help you improve its system architecture as well as the prompt and model quality. We tend to improve what we can measure, therefore verifying for correctness is a key focus. One difficulty in gauging accuracy is that the responses are unstructured text. A Q&A system can generate lengthy responses, rendering typical metrics like BLEU or ROUGE unreliable. In this case, employing a well-labeled dataset and llm-assisted assessors can help you rate the response quality of your system. This supplemented any human review and other measurements you may have already implemented.\n",
    "\n",
    "In an [earlier article](2023-08-16-langsmith-for-llm-application-evaluation.html) we introduced Langsmith and how it can help with LLM-based application evaluation.\n",
    "\n",
    "In this post, we will utilise LangSmith to validate a Q&A system against an example dataset. The main steps are as follows:\n",
    "\n",
    "1. Create a dataset of questions and answers.\n",
    "2. Define your question and answering system.\n",
    "3. Run evaluation using LangSmith.\n",
    "4. Iterate to improve the system.\n",
    "\n",
    "The test run will be saved in a project along with all its feedback and links to every evaluator run.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langsmith7.png\" width=\"800\"/>\n",
    "\n",
    "> **Note 1:** This walkthrough tests the end-to-end behavior of the system. Separately evaluating each component of system is still important! Many components, such as the retrievers, can be tested separately using standard retrieval metrics to complement this full integration test.\n",
    "\n",
    "> **Note 2:** If your knowledge base is changing, make sure your answers are still correct! You can avoid this through some combination of independent testing of chain components, freezing the knowledge source used during testing, and regularly updating your dataset.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial uses OpenAI for the model, ChromaDB to store documents, and LangChain to compose the chain. To make sure the tracing and evals are set up for [LangSmith](https://smith.langchain.com), please configure your API Key appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c788783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_API_KEY=<YOUR_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
    "Install the required packages. `lxml` and `html2text` are used by the document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U \"langchain[openai]\" > /dev/null\n",
    "# %pip install chromadb > /dev/null\n",
    "# %pip install lxml > /dev/null\n",
    "# %pip install html2text > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## Create a Dataset\n",
    "\n",
    "In our case, we will compare a Q&A system to the LangSmith documentation. To calculate aggregate accuracy, we'll need to compile a list of example question-answer paris. To demonstrate the procedure, we've hard-coded several examples below. In general, you'll need a lot more pairs (>100) to get relevant results. Drawing from actual inquiries can help to create a more accurate portrayal of the domain.\n",
    "\n",
    "Below, we have hard-coded several question-answer pairs to assess and created each example row using the client's 'create_example' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e98a80-bf37-457e-b31d-952292e76c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some hard-coded examples here.\n",
    "examples = [\n",
    "    (\"What is LangChain?\", \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\"),\n",
    "    (\"How might I query for all runs in a project?\", \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\"),\n",
    "    (\"What's a langsmith dataset?\", \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\"),\n",
    "    (\"How do I use a traceable decorator?\", \"\"\"The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,\\\n",
    "import the required function, decorate your function, and then call the function. Below is an example:\n",
    "```python\n",
    "from langsmith.run_helpers import traceable\n",
    "@traceable(run_type=\"chain\") # or \"llm\", etc.\n",
    "def my_function(input_param):\n",
    "    # Function logic goes here\n",
    "    return output\n",
    "result = my_function(input_param)\n",
    "```\"\"\"),\n",
    "    (\"Can I trace my Llama V2 llm?\", \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\"),\n",
    "    (\"Why do I have to set environment variables?\", \"Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith.\"\n",
    "     \" While there are other ways to connect, environment variables tend to be the simplest way to configure your application.\"),\n",
    "    (\"How do I move my project between organizations?\", \"LangSmith doesn't directly support moving projects between organizations.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5edb7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcd3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Retrieval QA Questions\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "for q, a in examples:\n",
    "    client.create_example(inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976437f",
   "metadata": {},
   "source": [
    "## Define RAG Q&A System\n",
    "\n",
    "Our Q&A system employs a straightforward retriever and an LLM response generator. To further simplify, the chain will consist of:\n",
    "\n",
    "1. A [VectorStoreRetriever](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.base.VectorStoreRetriever.html#langchain.vectorstores.base.VectorStoreRetriever) to retrieve documents. This uses:\n",
    "   - An embedding model to vectorize documents and user queries for retrieval. In this case, the [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) model.\n",
    "   - A vectorstore, in this case we will use [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma)\n",
    "2. A response generator. This uses:\n",
    "   - A [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html#langchain.prompts.chat.ChatPromptTemplate) to combine the query and documents. \n",
    "   - An LLM, in this case, the 16k token context window version of `gpt-3.5-turbo` via [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html#langchain.chat_models.openai.ChatOpenAI).\n",
    "\n",
    "We will combine them using LangChain's [expression syntax](https://python.langchain.com/docs/guides/expression_language/cookbook).\n",
    "\n",
    "First, load the documents to populate the vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95fab721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "documents = text_splitter.split_documents(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc4b79-4219-4446-a00c-beda55c2205a",
   "metadata": {},
   "source": [
    "With the documents prepared, create the vectorstore retriever. This is what will be used to provide context when generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057f0841-dd9f-4f75-8ff5-dbdda73f84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35f884-bb93-427d-a0ad-3858c449a1ea",
   "metadata": {},
   "source": [
    "Next up, we'll define the response generator. This responds to the user by injecting the retrieved documents and the user query into a prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f266e9-e4de-42ad-b41e-99ace4dc5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangSmith's documentation.\"\n",
    "            \" LangChain is a framework for building applications using large language models.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "            (\"system\", \"{context}\"),\n",
    "            (\"human\",\"{question}\")\n",
    "        ]\n",
    "    ).partial(time=str(datetime.now()))\n",
    "    \n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "response_generator = (\n",
    "    prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e63610-fdab-4d4e-80db-cdf38805040d",
   "metadata": {},
   "source": [
    "Finally, assemble the full chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4490e622-d865-44ee-b6f4-681b658dad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full chain looks like the following\n",
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    # The runnable map here routes the original inputs to a context and a question dictionary to pass to the response generator\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | response_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77b0954d-924b-4241-9c59-96adbd1c3ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To log user feedback to a run, you can use the LangSmith client or the REST API. Here's an example of how to log user feedback using the LangSmith client in Python:\n",
      "\n",
      "```python\n",
      "from langsmith import Client\n",
      "\n",
      "client = Client()\n",
      "feedback = client.create_feedback(\n",
      "    \"<run_id>\",\n",
      "    \"<feedback_key>\",\n",
      "    score=True,\n",
      "    comment=\"This is a positive feedback from the user.\"\n",
      ")\n",
      "```\n",
      "\n",
      "In this example, you need to replace `<run_id>` with the ID of the run you want to log feedback for, and `<feedback_key>` with a key that represents the type of feedback you want to log (e.g., \"positive\", \"negative\", \"accuracy\", etc.). You can also provide additional information such as a score, comment, and other optional fields.\n",
      "\n",
      "Make sure you have the necessary environment variables set up for the LangSmith client to authenticate with the LangSmith API.\n",
      "\n",
      "If you prefer to use the REST API directly, you can make a POST request to the `/feedback` endpoint with the necessary parameters.\n",
      "\n",
      "Remember that logging user feedback is an important step in improving your LLM application and ensuring a high-quality user experience."
     ]
    }
   ],
   "source": [
    "for tok in chain.stream({\"question\": \"How do I log user feedback to a run?\"}):\n",
    "    print(tok, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "## Evaluate the Chain\n",
    "\n",
    "We will use the off-the-shelf QA evaluator to measure the correctness of the retrieval Q&A responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dedaff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"qa\"],\n",
    "    # If you want to configure the eval LLM:\n",
    "    # eval_llm=ChatAnthropic(model=\"claude-2\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5e4f7-f5b4-4d6a-9dd7-1973d8e9c0f7",
   "metadata": {},
   "source": [
    "Run the evaluation. This makes predictions over the dataset and then uses the \"QA\" evaluator to check the correctness on each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f30ce874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '6ed4213fc4c54b3fbcfdd9cba14e87f0-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/da05d8be-995f-4ee7-8d1b-ce8943bb085e?eval=true\n"
     ]
    }
   ],
   "source": [
    "_ = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=lambda: chain,\n",
    "    evaluation=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fac17c-e0c7-4d85-bca1-18c4337fbcba",
   "metadata": {},
   "source": [
    "You can visit to the produced \"test run\" project to examine the chain's outputs, evaluator feedback, and connections to the evaluation traces as the test run progresses.\n",
    "\n",
    "You can filter the results depending on feedback metrics from the test project page. For example, in the filters section, click on \"Correctness==0\" to show the instances designated as inaccurate.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langsmith8.png\" width=\"800\"/>\n",
    "\n",
    "After you've filtered the results, you can view the traces and triage where the chain failed by clicking on the individual runs. You may see for yourself by clicking on the image below. By selecting the \"Feedback\" option, you may view the evaluation results for this run. \n",
    "\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langsmith9.png\" width=\"800\"/>\n",
    "\n",
    "To view the trace of the evaluator run, click the link underlined in red above. Because LLM-assisted evaluations are flawed, analysing their traces allows you to evaluate the feedback decisions and choose when and how to modify the prompt to your individual use case.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langsmith10.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15270951-84df-4ac8-84ef-23312fc16db0",
   "metadata": {},
   "source": [
    "This trace was marked as \"incorrect\". It looks like the chain is making up information, or \"hallucinating.\" If you click on the `ChatOpenAI` run in your own test project, you can open it in the playground to experiment with changes that may address this error.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langsmith11.png\" width=\"800\"/>\n",
    "\n",
    "Let's try tweaking the prompt to better instruct the model. We'll add an additional system message to remind the model to only respond based on the retrieved documents.\n",
    "Click \"Add Message\" and paste in the following text:\n",
    "\n",
    "> Respond as best as you can. If no documents are retrieved or if you do not see an answer in the retrieved documents, admit you do not know or that you don't see it being supported at the moment.\n",
    "\n",
    "Click \"Submit\" to view the results streamed to the message in the right column. If you haven't already added your OpenAI key, you can do so using the \"Secrets & API Keys\" button.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langsmith12.png\" width=\"800\"/>\n",
    "\n",
    "That seems to have the desired effect for this data point, but we want to be careful that we're not overfitting to a single example. We'll want to re-evaluate to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbd6a6-e3eb-4d3a-afe6-d5917d81c54d",
   "metadata": {},
   "source": [
    "## Iterate\n",
    "\n",
    "The chain performed well, and in the previous part, we were able to use the playground to generate a potential solution to the problem. Let's re-run the evaluation with the updated prompt to see how it does overall. \n",
    "We've duplicated the chain code below and added an additional system message to the chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25d189e9-ff07-48d9-9aef-7fc17b265e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangSmith's documentation.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "            (\"system\", \"{context}\"),\n",
    "            (\"human\",\"{question}\"),\n",
    "            # Add the new system message here:\n",
    "            (\"system\", \"Respond as best as you can. If no documents are retrieved or if you do not see an answer in the retrieved documents,\"\n",
    "             \" admit you do not know or that you don't see it being supported at the moment.\"),\n",
    "        ]\n",
    "    ).partial(time=lambda: str(datetime.now()))\n",
    "    \n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "response_generator_2 = (\n",
    "    prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain_2 = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | response_generator_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf882f9-c75b-43ac-956f-acd8f17ef800",
   "metadata": {},
   "source": [
    "Rerun the evaluation and check out the results as they become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbdfa2f4-fa96-42d9-94bc-bb5c227104da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'e42329ecc0c3492282fe02bead607ce9-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/e42be46d-a0e3-40f6-ac58-59bcb70c3e6a?eval=true\n"
     ]
    }
   ],
   "source": [
    "_ = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=lambda: chain_2,\n",
    "    evaluation=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5352e20-af87-4735-9061-caf09e86610c",
   "metadata": {},
   "source": [
    "We can now begin comparing findings. To see the aggregate feedback metrics for each test run, go to the \"Retrieval QA Questions\" dataset page. Click the datasets & testing icon on the left bar to view your datasets.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langsmith13.png\" width=\"800\"/>\n",
    "\n",
    "It appears that the new chain is now passing all of the cases. Remember that this illustrative dataset is too tiny to provide a thorough picture of the chain's performance. We may add more examples to the dataset as we continue to prototype this chain.\n",
    "\n",
    "You can view specific forecasts on each row in addition to the aggregate feedback metrics. To view each row in the dataset, select the \"Examples\" tab. When you click on a specific example, it will display the results of both test runs for that data point.\n",
    "You may rapidly compare forecasts across chain versions using the linked runs table to get an idea of the types of outputs you might expect. You can re-view the full traces by clicking on each connected run.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langsmith14.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3e141-30aa-4427-8b79-73f9535872ca",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've just completed a fast assessment of the accuracy of your Q&A system. You used LangSmith in this post to uncover problems in a RAG pipeline and make immediate changes to improve the chain's performance. You've also learnt about evaluator feedback and how to use it into your LLM app development process. This is an excellent place to start when it comes to enhancing the consistency of your LLM applications.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [Langsmith Cookbook Repo](https://github.com/langchain-ai/langsmith-cookbook/) and acknowledge the use of some images and other materials from this project in writing this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2069907-34ea-44b5-8244-ff144bae4474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
