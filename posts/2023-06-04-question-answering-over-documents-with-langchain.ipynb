{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6fc6cbf0-2d51-4a4b-97ed-fcc08fe72c56",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-06-04'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - openai\n",
    "title: Question and Answering for Documents using LangChain\n",
    "description: In this article we look at how LangChain can perform question answering over documents using embeddings and vector stores.\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain1.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. But using LLMs in isolation is often not enough in practice to create a truly powerful or useful business application - the real power comes when you are able to combine them with other sources of computation, services or knowledge. [LangChain](https://python.langchain.com/en/latest/index.html) is an intuitive open-source python framework created to simplify the development of useful applications using large language models (LLMs), such as OpenAI or Hugging Face. \n",
    "\n",
    "In [earlier articles](/#category=langchain) we introduced the LangChain library and key components.\n",
    "\n",
    "In this article, we look at how to use LangChain to peform question & answering over documents. This allows LLM's to be able to use more data then they were trained on, which allows them to be much more useful and specific for a given use case. We will also look at more advanced uses of memory such as embeddings and vector stores. An example application of this might be a tool that would allow you to query a product catalog for items of interest.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We will use OpenAI's ChatGPT LLM for our examples, so lets load in the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c1f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (0.0.188)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.21.5)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from pydantic<2,>=1->langchain) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.11)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eedc9b0-b18f-4071-b818-d87433e2fedc",
   "metadata": {},
   "source": [
    "We will also import some LangChain objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e520cb-10c9-44b8-a6a1-810c23f75932",
   "metadata": {},
   "source": [
    "## Creating a Q&A Chain and Vector Index Quickly\n",
    "\n",
    "We will import some sample data of a catalog of outdoor clothing to use. We can use the CSVLoader object to load this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7249846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc5281-6a29-4df2-a6c0-4c6ec446b870",
   "metadata": {},
   "source": [
    "We will also import VectorstoreIndexCreator which will help us create an index really easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfaba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5ab657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docarray\n",
      "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich>=13.1.0\n",
      "  Downloading rich-13.4.1-py3-none-any.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from docarray) (1.21.5)\n",
      "Collecting types-requests>=2.28.11.6\n",
      "  Downloading types_requests-2.31.0.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pydantic>=1.10.2 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from docarray) (1.10.8)\n",
      "Collecting orjson>=3.8.2\n",
      "  Downloading orjson-3.9.0-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.5/240.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-inspect>=0.8.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from pydantic>=1.10.2->docarray) (4.3.0)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting types-urllib3\n",
      "  Downloading types_urllib3-1.26.25.13-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect>=0.8.0->docarray) (0.4.3)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: types-urllib3, types-requests, pygments, orjson, mdurl, markdown-it-py, rich, docarray\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docarray-0.32.1 markdown-it-py-2.2.0 mdurl-0.1.2 orjson-3.9.0 pygments-2.15.1 rich-13.4.1 types-requests-2.31.0.1 types-urllib3-1.26.25.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "pip install docarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6076c70f-4d9a-4b22-ac24-2a38a02c0f2c",
   "metadata": {},
   "source": [
    "To create the vector store we are going to specify 2 things, the vector store class, and then use the from_loaders method to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e200726",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ec095-cb62-4072-8f4f-2c0a362cce2e",
   "metadata": {},
   "source": [
    "Now we are ready to query our data using text prompts already! Lets make an example query and submit it to the index (our data store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34562d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd0cc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae21f1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "| Name | Description |\n",
       "| --- | --- |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated, 100% polyester, wrinkle-resistant, front and back cape venting, two front bellows pockets |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ rated, 52% polyester and 48% nylon, machine washable and dryable, front and back cape venting, two front bellows pockets |\n",
       "| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ rated, 71% Nylon, 29% Polyester, 100% Polyester knit mesh, wrinkle resistant, front and back cape venting, two front bellows pockets |\n",
       "| Sun Shield Shirt by | UPF 50+ rated, 78% nylon, 22% Lycra Xtra Life fiber, wicks moisture, fits comfortably over swimsuit, abrasion resistant |\n",
       "\n",
       "All four shirts provide UPF 50+ sun protection, blocking 98% of the sun's harmful rays. The Men's Tropical Plaid Short-Sleeve Shirt is made of 100% polyester and is wrinkle-resistant. The Men's Plaid Trop"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8e666-8510-4773-be5d-07f62530e7a7",
   "metadata": {},
   "source": [
    "So we can see this has given us a nice table of results formatted in Markdown to our question.\n",
    "\n",
    "We also have a nice summary undeneath.\n",
    "\n",
    "## LLM's on Documents\n",
    "\n",
    "LLM's can only look at a few thousand words at a time from a document. So if we have really large documents, how do we get the language model to be able to respond appropriately to everything in a large document? \n",
    "\n",
    "Embeddings and vector storage can help with this issue.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "Embeddings create numerical representations for text. These numerical representations captures the semantic meaning of that text. Therefore, text with similar meaning will have similar numerical representations - or vectors. \n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langchain-embeddings.png\" width=\"800\"/>\n",
    "\n",
    "In the example above, we can see the first two sentances are about pets - whereas the third is about a car. If we look at the representation of these numerically we can see the first two have very similar numbers compared to the last one. This helps us figure out which bits of text are similar, which will be very useful for when deciding which pieces of text we want to pass to the language model say to answer a question.\n",
    "\n",
    "### Vector Databases\n",
    "\n",
    "A vector database is a way to store these numerical representations (or vectors) for each of our text pieces from our document or documents. When we get the text of a document we are doing to first break it up into smaller chunks, this creates bits of text that are smaller than the original document. This is useful as the document may be too large to pass in its entirety to the language model. \n",
    "By creating these smaller chunks, we can then pass only the most relevant parts of text to the language model. So we create embeddings for each of these chunks.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langchain-vectors1.png\" width=\"800\"/>\n",
    "\n",
    "Once we have created this index, we can use it at run time to find the most relevant chunks of text to an incoming query. We create an embedding vector for an incoming query, and find the most similar embedding vectors to it in our index. Cosine similarity for example is a method to find the nearest vectors to a given vector.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langchain-vectors2.png\" width=\"800\"/>\n",
    "\n",
    "These most relevant chunks can then be passed to the LLM in the prompt, to help provide the most useful and relevant context from the document for answering the query.\n",
    "\n",
    "## Creating a Q&A Chain and Vector Index Step by Step\n",
    "\n",
    "### Create the Vector Store\n",
    "\n",
    "We will now create a question and answer chain using a vector index as we did previously, but now step by step to go over more of the details.\n",
    "\n",
    "So as before we will use the CSVLoader to load the documents we want to do question and answering over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631396c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c2164b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a44a0-1cc5-4987-b3a6-3e279a73ebcd",
   "metadata": {},
   "source": [
    "If we look at one of the individual documents loaded, we can see that it corresponds to one of the product rows in the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a977f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d91581-788f-409e-9392-85bad0738ce1",
   "metadata": {},
   "source": [
    "Previously we talked about how useful it is to create document chunks. Because these particular documents are so small, we don't actually need to do any document chunking in this case, so we can create embeddings directly for each of these documents. Do create these embeddings we are doing to use LangChains wrapper class for OpenAI OpenAIEmbeddings.\n",
    "\n",
    "So if we want to see what these embeddings look like, lets take an example text and convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e875693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "779bec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(\"Hi my name is Harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699aaaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(len(embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d00d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.021900920197367668, 0.006746490020304918, -0.018175246194005013, -0.039119575172662735, -0.014097143895924091]\n"
     ]
    }
   ],
   "source": [
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2636f0-f880-4ec1-9e65-55344828345a",
   "metadata": {},
   "source": [
    "We can see that these embeddings have 1536 numbers, here are the first 5 numbers for our example emedding.\n",
    "\n",
    "So we want to create embeddings for all the text documents we loaded, and then store them in a vector store. We can do this using the from_documents method of the vector store object.\n",
    "\n",
    "This takes set of documents and an embedding object, and creates a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27ad0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbc14b-5945-439a-baf3-3337980b0910",
   "metadata": {},
   "source": [
    "We can now use this vector store to find the most similar document texts for an incoming query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0329bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7909c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43321853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eba90b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.', metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 255})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0483880-3c56-489d-a64e-b1a9281741b4",
   "metadata": {},
   "source": [
    "### Using the Vector Store to Answer Questions\n",
    "\n",
    "So how can we use this vector store to do question answering over all our documents? \n",
    "\n",
    "First we need to create a retriever from this vector store. A retriever is a generic method that takes a query and returns documents. Vector stores and embeddings are one way we can do this, but there are other methods.\n",
    "\n",
    "Next, because we want to do text generation and create a natural language response to our query - we need to import a language model - for our example we will use OpenAI's ChatGPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0c3596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0625f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ef5cc-530e-45fa-aa91-250b1cdf980a",
   "metadata": {},
   "source": [
    "If we were doing this by hand, we would combine the documents into a single piece of text into a variable, then pass this variable into a prompt as context for answering a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a573f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14682d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
    "shirts with sun protection in a table in markdown and summarize each one.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bba545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Name | Description |\n",
       "| --- | --- |\n",
       "| Sun Shield Shirt | High-performance sun shirt with UPF 50+ sun protection, moisture-wicking, and abrasion-resistant fabric. Recommended by The Skin Cancer Foundation. |\n",
       "| Men's Plaid Tropic Shirt | Ultracomfortable shirt with UPF 50+ sun protection, wrinkle-free fabric, and front/back cape venting. Made with 52% polyester and 48% nylon. |\n",
       "| Men's TropicVibe Shirt | Men's sun-protection shirt with built-in UPF 50+ and front/back cape venting. Made with 71% nylon and 29% polyester. |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | Lightest hot-weather shirt with UPF 50+ sun protection, front/back cape venting, and two front bellows pockets. Made with 100% polyester and is wrinkle-resistant. |\n",
       "\n",
       "All of these shirts provide UPF 50+ sun protection, blocking 98% of the sun's harmful rays. They are made with high-performance fabrics that are moisture-wicking, wrinkle-resistant, and abrasion-resistant. The Men's Plaid Tropic Shirt and Men's Tropical Plaid Short-Sleeve Shirt both have front/back cape venting for added breathability. The Sun Shield Shirt is recommended by The Skin Cancer Foundation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb1bcc-b6fc-48cf-aca3-4d8e692b4876",
   "metadata": {},
   "source": [
    "Alternatively we can incorporate all of these steps into a LangChain chain that does Retrieval then question and answering: RetrievalQA. We pass into this a language model to do text generation at the end, then we specify the chain type 'stuff' which will just stuffs all of the documents into the context for the prompt Finally we pass in a retriever object, which is just a object we used as before for fetching the most relevant documents to pass to the language model. \n",
    "\n",
    "Now we can create a query, and run this chain on that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32c94d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4769316",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fc3c2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fba1a5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Shirt Number | Name | Description |\n",
       "| --- | --- | --- |\n",
       "| 618 | Men's Tropical Plaid Short-Sleeve Shirt | This shirt is made of 100% polyester and is wrinkle-resistant. It has front and back cape venting that lets in cool breezes and two front bellows pockets. It is rated UPF 50+ for superior protection from the sun's UV rays. |\n",
       "| 374 | Men's Plaid Tropic Shirt, Short-Sleeve | This shirt is made with 52% polyester and 48% nylon. It is machine washable and dryable. It has front and back cape venting, two front bellows pockets, and is rated to UPF 50+. |\n",
       "| 535 | Men's TropicVibe Shirt, Short-Sleeve | This shirt is made of 71% Nylon and 29% Polyester. It has front and back cape venting that lets in cool breezes and two front bellows pockets. It is rated UPF 50+ for superior protection from the sun's UV rays. |\n",
       "| 255 | Sun Shield Shirt | This shirt is made of 78% nylon and 22% Lycra Xtra Life fiber. It is handwashable and line dry. It is rated UPF 50+ for superior protection from the sun's UV rays. It is abrasion-resistant and wicks moisture for quick-drying comfort. |\n",
       "\n",
       "The Men's Tropical Plaid Short-Sleeve Shirt is made of 100% polyester and is wrinkle-resistant. It has front and back cape venting that lets in cool breezes and two front bellows pockets. It is rated UPF 50+ for superior protection from the sun's UV rays.\n",
       "\n",
       "The Men's Plaid Tropic Shirt, Short-Sleeve is made with 52% polyester and 48% nylon. It has front and back cape venting, two front bellows pockets, and is rated to UPF 50+.\n",
       "\n",
       "The Men's TropicVibe Shirt, Short-Sleeve is made of 71% Nylon and 29% Polyester. It has front and back cape venting that lets in cool breezes and two front bellows pockets. It is rated UPF 50+ for superior protection from the sun's UV rays.\n",
       "\n",
       "The Sun Shield Shirt is made of 78% nylon and 22% Lycra Xtra Life fiber. It is abrasion-resistant and wicks moisture for quick-drying comfort. It is rated UPF 50+ for superior protection from the sun's UV rays."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbbc66-9012-40ff-a7f8-346cb7b321cd",
   "metadata": {},
   "source": [
    "So thats how you might do it in detail, but we can of course use the one line method as before. So thats the great thing about LangChain, you can use either a more concise or more detailed call to specify your chain. The more detailed calls of course allow you to customise more about the specifics going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "500ec062",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dce370-b04d-44f1-9180-42a519c04692",
   "metadata": {},
   "source": [
    "We can also customise the index when we create it. When we created it by hand we specified the ChatGPT embeddings, which gives us flexibility over how the embeddings are created and also allows us to use different types of vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cffb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57fcc98-f16e-4939-abe9-82d9b053dfe0",
   "metadata": {},
   "source": [
    "## Alternative Methods to Populate Prompt Context\n",
    "\n",
    "We used the stuff method previously to populate the prompt which is the simplest method but has various pros and cons and is not always the best solution. For example when we fetched the documents in our case each document was relatively small, but this might not work so well for bigger or multiple documents.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langchain-stuff.png\" width=\"800\"/>\n",
    "\n",
    "But what if you wanted to do the same kind of question answering over lots of different types of chunks? There are a few other methods we could use.\n",
    "\n",
    "**Map_reduce** takes all the chunks, passes them with the question to a language model, gets a response. Then uses another LLM call to summerise all the individual document responses into a final answer. This is really powerful as it can operate over any number of documents, and its also powerful as you could do all the individual questions in parallel. But it does take more calls so could be more expensive for a paid service like OpenAI, and it does treat all the documents as independant which may not be the most desired approach for a use case.\n",
    "\n",
    "**Refine** is also used to run over all the chunks, but it does so iteratively and builds upon the answer from the previous document. So this is really good for combining information and building up an answer over time. It will generally take longer to execute, and lead to longer answers. And this also takes as many calls as Map_reduce.\n",
    "\n",
    "**Map_rerank** is a more experimental method, where you do a single call to a language model for each document and you also ask it to return a score in the same call - and you use the highest score to select the best answer. But this relies on the model to know what the score should be. And like Map_reduce its relatively fast. But you are making a load of calls so it will be more expensive. \n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/langchain-stuff-alts.png\" width=\"800\"/>\n",
    "\n",
    "The most commonly method is actually the simple stuff method. The second most common method is Map_reduce. These methods can be used for many other chains beyond question answering, for example a common use case for Map_reduce is text summerisation where you have a really long document and you want to recursively summerise documents.\n",
    "\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain for LLM Application Development Course](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/) by DeepLearning.ai - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590b337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb587c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec249f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64f166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21322e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
