{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-07-28'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    "title: Large Language Models v Chat Models\n",
    "description: In LangChain LLMs and Chat Models are two different kinds of models that are used for various tasks involving natural language processing - the distinctions between LLMs and Chat Models as well as their distinctive applications and implementation strategies within LangChain will be covered in this article\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake1.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Large Language Models have significantly advanced Natural Language Processing (NLP), allowing AI systems to comprehend and produce prose that is human-like. Based on the Transformers architecture, ChatGPT is a well-known language model that can comprehend lengthy texts and determine the relationships between words or concepts. It excels in predicting linguistic patterns and word associations.\n",
    "\n",
    "In LangChain, LLMs and Chat Models are two different kinds of models that are used for various tasks involving natural language processing. The distinctions between LLMs and Chat Models, as well as their distinctive applications and implementation strategies within LangChain, will be covered in this article. \n",
    "\n",
    "## Import Libs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cnb6iwlq8F0k",
    "outputId": "87e9ea31-f3ba-4158-def3-f18762a27a04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuDPxI-x4Twy"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding LLMs and Chat Models\n",
    "\n",
    "### LLMs\n",
    "\n",
    "Text strings are inputted into LLMs like GPT-3, Bloom, PaLM, and Aurora genAI, and text strings are outputted in return. They can produce prose that is human-like, engage in complicated reasoning, and even write code since they have been taught on language modelling tasks. LLMs are strong and adaptable, able to produce text for a variety of jobs. They occasionally give out inaccurate or meaningless responses, and their API is less organised than Chat Models.\n",
    "\n",
    "By exposing these models to massive corpora and letting them anticipate the subsequent word, pre-training these models teaches them the links between words. LLMs can produce high-quality text through this learning process, which can be used for a variety of applications, including predictive text and automatic form filling.\n",
    "\n",
    "Some of these models are trained on a combination of general and domain-specific data, such Intel Aurora genAI, which is trained on general text, scientific texts, scientific data, and domain-related codes. The majority of these models are trained on general purpose training datasets. Increasing performance in a given area while still being able to complete the vast majority of jobs that general LLMs can handle is the aim of domain-specific LLMs.\n",
    "\n",
    "LLMs have the potential to permeate many facets of human existence, including as the humanities, sciences, and law. LLMs are a crucial technology to master as they continue to be developed and integrated into our academic, social, and professional lives.\n",
    "\n",
    "You can use a large language model (LLM) like GPT-3 in LangChain by following these instructions. Initialise the OpenAICopy wrapper with the desired model name and any other arguments by importing it from the langchain.llmsCopy module. Set a high temperature, for instance, to produce more random outcomes. After that, make a PromptTemplateCopy to format the model's input. \n",
    "\n",
    "To merge the model and prompt, define an LLMChainCopy. Run the chain while using the desired input.run()Copy. As previously noted, before running the following instructions, make sure to set your OpenAI key stored in the \"OPENAI_API_KEY\" environment variable. Keep in mind to use the following command to install the necessary packages: pip install deeplake openai tiktokenCopy langchain==0.0.208."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdJlDzgK8Cel"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNk81HaE8DWF"
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPLkpoZV8rHc"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VlzgOR_8xKF",
    "outputId": "e62644a9-33f4-4217-8cda-da78adf12246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wireless Audio Solutions\n"
     ]
    }
   ],
   "source": [
    "result = chain.run(\"wireless headphones\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the input for the chain is the string \"wireless headphones\". The chain processes the input and generates a result based on the product name.\n",
    "\n",
    "### Chat Models\n",
    "\n",
    "The most well-liked models in LangChain are chat models, such ChatGPT, which can have either GPT-3 or GPT-4 as its heart. Their capacity to learn from user comments and user-friendly chat interface have attracted a lot of attention.\n",
    "\n",
    "A list of messages is provided as input to chat models like ChatGPT, which return an AIMessageCopy. Their APIs are more formal, but their underlying technology is typically LLMs. Chat models are made to keep track of the user's previous conversations throughout a session and use that context to produce more pertinent responses. Additionally, they gain from reinforcement learning from human feedback, which helps them respond more effectively. They may still have logical constraints, though, and need to be handled carefully to prevent producing offensive content.\n",
    "\n",
    "#### Types of Chat Messages\n",
    "\n",
    "When dealing with chat models in LangChain, SystemMessageCopy, HumanMessageCopy, and AIMessageCopy are the three primary message types used.\n",
    "\n",
    "- **SystemMessage:** These messages give the AI model its first directives, context, or information. They determine the goals the AI should pursue and can aid in regulating the AI's behaviour. System messages are directives for the AI to follow rather than user input.\n",
    "- **HumanMessage:** The user's input is represented by these messages, which are forwarded to the AI model. It is anticipated that the AI model will react to these messages. To alter how the human input is displayed in LangChain, you can customise the human prefix (for example, \"User\") in the discussion summary.\n",
    "- **AIMessage:** During interactions with human users, these messages are sent from the AI's point of view. They serve as the AI's answers to input from humans. Similar to HumanMessage, the AI prefix (such as \"AI Assistant\" or \"AI\") in the discussion summary can be customised to alter how the AI's responses are shown.\n",
    "\n",
    "#### An illustration of how to use a HumanMessage with ChatOpenAI\n",
    "\n",
    "Here, we're attempting to build a chatbot that can translate a text using the LangChain library. Instead of depending on a single prompt, we'll use a variety of message kinds to distinguish between users' requests and system instructions. The model will better understand the requirements by using this method.\n",
    "\n",
    "We first generate a list of messages, beginning with a SystemMessageCopy that establishes the chatbot's context and informs it that its purpose is to serve as a helpful translator aid. The user's inquiry is then placed below it in a HumanMessageCopy, which is similar to an English sentence that needs to be translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AtRDt9i8x1i",
    "outputId": "3485eaa3-a3b4-4e63-f3cd-ba2e86703752"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dv1OWi-f9EIl",
    "outputId": "a9c1a156-ce9a-478b-f6ce-2395491da3cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we use the **chat()** method to send the chatbot a list of messages. After analysing the input messages and taking into account the context offered by the system message, the chatbot converts the given English sentence into French. \n",
    "\n",
    "**SystemMessage** is a representation of the messages that the system that wants to use the model generates, such as warnings, notifications, or errors. These messages are produced by the underlying system to offer context, instructions, or status updates and are not created by the human user or the AI chatbot.\n",
    "\n",
    "You may also generate completions for other groups of messages using the generate function. Every batch of messages can function independently and have an own SystemMessageCopy. The first set of messages in the code below displays the sentences from to, whereas the second set does the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5V8LECK9EF9",
    "outputId": "8e975be6-8262-4955-af8f-6d773ea2b021"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'aime la programmation.\", generation_info=None, message=AIMessage(content=\"J'aime la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text='I love programming.', generation_info=None, message=AIMessage(content='I love programming.', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 65, 'completion_tokens': 11, 'total_tokens': 76}, 'model_name': 'gpt-4'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates French to English.\"),\n",
    "        HumanMessage(content=\"Translate the following sentence: J'aime la programmation.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4RzkSTEKR-n"
   },
   "source": [
    "As a comparison, here's what LLM and Chat Model APIs look like in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdvIUD0FJOll"
   },
   "outputs": [],
   "source": [
    "llm_input = \"Translate the following text from English to French: Hello, how are you?\"\n",
    "llm_output = chain(llm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1X0gywLKSyP",
    "outputId": "503bff64-cd36-4bb6-fd83-603edf99a144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_output:  {'product': 'Translate the following text from English to French: Hello, how are you?', 'text': '\\n\\nBonjour, comment allez-vous?'}\n"
     ]
    }
   ],
   "source": [
    "print(\"llm_output: \", llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yJ-BV7yKcjj"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate the following sentence: Hello, how are you?\")\n",
    "]\n",
    "chat_output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvh33mPOKd8j",
    "outputId": "15eda530-4ef7-4546-aeb1-836c1e5c8585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_output:  content='Bonjour, comment Ã§a va ?' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(\"chat_output: \", chat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0H5fEZJKoea"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Both LLMs and chat models have benefits and drawbacks. LLMs are strong and adaptable, able to produce text for a variety of jobs. Their API is less organised than Chat Models', though.\n",
    "\n",
    "Conversely, Chat Models are better suited for conversational tasks and have a more organised API. Additionally, they have the ability to recall earlier talks with the user, which makes them better suited for having meaningful interactions. They also gain from reinforcement learning from human feedback, which helps them respond more effectively. They may need careful treatment to prevent hallucinations and the creation of inappropriate content because they still have certain limits in their reasoning.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
