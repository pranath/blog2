{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-08-05'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - activeloop\n",
    " - openai\n",
    " - prompt-engineering\n",
    "title: An Improved News Articles Summarizer\n",
    "description: Our goal in this post is to improve a news summarisers ability to extract the most important information from lengthy news items and display it in an easy-to-read bulleted list format\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain-deeplake1.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This article aims to improve our earlier News Article Summarizer implementation. Our goal is to improve our tool's ability to extract the most important information from lengthy news items and display it in an easy-to-read, bulleted list format. With this improvement, consumers will be able to quickly and clearly understand the essential ideas of an article, saving time and improving the reading experience.\n",
    "\n",
    "We will change our current summarizer to tell the underlying language model to produce summaries as bulleted lists in order to do this. We need to make a few adjustments to the way we give our cue to the model for this assignment, and the workflow below will walk you through them.\n",
    "\n",
    "## Workflow for Building a News Articles Summarizer with Bulleted Lists\n",
    "\n",
    "This is what we are going to doin this project.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/news-article-summariser2.png\" width=\"800\"/>\n",
    "\n",
    "We set up the environment and retrieved the news article.\n",
    "\n",
    "1. Install required libraries: The first step is to ensure that the necessary libraries, namely requests, newspaper3k, and LangChain, are installed.\n",
    "2. Scrape articles: We will use the requests library to scrape the content of the target news articles from their respective URLs.\n",
    "3. Extract titles and text: The newspaper library will be used to parse the scraped HTML, extracting the titles and text of the articles.\n",
    "4. Preprocess the text: The extracted texts need to be cleaned and preprocessed to make them suitable for input to LLM.\n",
    "\n",
    "The rest of the post will explore new possibilities to enhance the application’s performance further.\n",
    "\n",
    "5. Use Few-Shot Learning Technique: We use the few-shot learning technique in this step. This template will provide a few examples of the language model to guide it in generating the summaries in the desired format - a bulleted list.\n",
    "6. Generate summaries: With the modified prompt, we utilize the model to generate concise summaries of the extracted articles' text in the desired format.\n",
    "7. Use the Output Parsers: We employ the Output Parsers to interpret the output from the language model, ensuring it aligns with our desired structure and format.\n",
    "8. Output the results: Finally, we present the bulleted summaries along with the original titles, enabling users to quickly grasp the main points of each article in a structured manner.\n",
    "\n",
    "By following these instructions, you may create a robust programme that can summarise news items into digestible, bulleted summaries while also using OutputParsers to arrange the output according to a specified data structure and the FewShotLearning technique for increased precision. Let's get started!\n",
    "\n",
    "Technically, the first phases of the procedure are the same as in part 1 of this tutorial. Keep in mind to use the following command to install the necessary packages: pip install deeplake openai tiktoken langchain==0.0.208. Install the newspaper3k package as well, which was examined in this session with the 0.2.8 version.\n",
    "\n",
    "## Import Libs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQVIcL2LWULJ",
    "outputId": "16806fb7-df94-4347-9f01-e66c01e705f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install -q langchain==0.0.208 openai python-dotenv newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDuYoCMhWW_x",
    "outputId": "93c64e0e-207c-42a8-e42b-df4218d01bf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a summary, we used the URL of a news story. The code that follows uses a customised User-Agent header together with the requests library to fetch articles from a list of URLs. The title and content of each article are then extracted using the newspaper library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6rH09GpWyP_",
    "outputId": "d73b822b-7fac-4a01-b6db-ce4cf123665b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Meta claims its new AI supercomputer will set records\n",
      "Text: Ryan is a senior editor at TechForge Media with over a decade of experience covering the latest technology and interviewing leading industry figures. He can often be sighted at tech conferences with a strong coffee in one hand and a laptop in the other. If it's geeky, he’s probably into it. Find him on Twitter (@Gadget_Ry) or Mastodon (@gadgetry@techhub.social)\n",
      "\n",
      "Meta (formerly Facebook) has unveiled an AI supercomputer that it claims will be the world’s fastest.\n",
      "\n",
      "The supercomputer is called the AI Research SuperCluster (RSC) and is yet to be fully complete. However, Meta’s researchers have already begun using it for training large natural language processing (NLP) and computer vision models.\n",
      "\n",
      "RSC is set to be fully built in mid-2022. Meta says that it will be the fastest in the world once complete and the aim is for it to be capable of training models with trillions of parameters.\n",
      "\n",
      "“We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together,” wrote Meta in a blog post.\n",
      "\n",
      "“Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.”\n",
      "\n",
      "For production, Meta expects RSC will be 20x faster than Meta’s current V100-based clusters. RSC is also estimated to be 9x faster at running the NVIDIA Collective Communication Library (NCCL) and 3x faster at training large-scale NLP workflows.\n",
      "\n",
      "A model with tens of billions of parameters can finish training in three weeks compared with nine weeks prior to RSC.\n",
      "\n",
      "Meta says that its previous AI research infrastructure only leveraged open source and other publicly-available datasets. RSC was designed with the security and privacy controls in mind to allow Meta to use real-world examples from its production systems in production training.\n",
      "\n",
      "What this means in practice is that Meta can use RSC to advance research for vital tasks such as identifying harmful content on its platforms—using real data from them.\n",
      "\n",
      "“We believe this is the first time performance, reliability, security, and privacy have been tackled at such a scale,” says Meta.\n",
      "\n",
      "(Image Credit: Meta)\n",
      "\n",
      "Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022.\n",
      "\n",
      "Explore other upcoming enterprise technology events and webinars powered by TechForge here.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_url = \"https://www.artificialintelligence-news.com/2022/01/25/meta-claims-new-ai-supercomputer-will-set-records/\"\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "\n",
    "try:\n",
    "    response = session.get(article_url, headers=headers, timeout=10)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        print(f\"Title: {article.title}\")\n",
    "        print(f\"Text: {article.text}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch article at {url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while fetching article at {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting\n",
    "\n",
    "We learned how to utilise FewShotPromptTemplate in the previous posts; now, let's explore an other method of adding examples to a prompt that is slightly different but achieves the same effects. In this experiment, we provide a number of examples that direct the model's process of summarising to produce bullet lists.  As a result, it is anticipated that the model will provide a bulleted list that summarises the provided article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mrx20THKYkA4"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"\n",
    "As an advanced AI, you've been tasked to summarize online articles into bulleted points. Here are a few examples of how you've done this in the past:\n",
    "\n",
    "Example 1:\n",
    "Original Article: 'The Effects of Climate Change\n",
    "Summary:\n",
    "- Climate change is causing a rise in global temperatures.\n",
    "- This leads to melting ice caps and rising sea levels.\n",
    "- Resulting in more frequent and severe weather conditions.\n",
    "\n",
    "Example 2:\n",
    "Original Article: 'The Evolution of Artificial Intelligence\n",
    "Summary:\n",
    "- Artificial Intelligence (AI) has developed significantly over the past decade.\n",
    "- AI is now used in multiple fields such as healthcare, finance, and transportation.\n",
    "- The future of AI is promising but requires careful regulation.\n",
    "\n",
    "Now, here's the article you need to summarize:\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Please provide a summarized version of the article in a bulleted list format.\n",
    "\"\"\"\n",
    "\n",
    "# format prompt\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples help the model comprehend the type of responses we desire from it.  Here are a few significant elements:\n",
    "\n",
    "- Article data: The title and text of the article are obtained, which will be used as inputs to the model.\n",
    "\n",
    "- Template preparation: A template is prepared for the prompt. This template includes a few-shot learning style, where the model is provided with examples of how it has previously converted articles into a bulleted list format. The template also includes placeholders for the actual article title and text that will be summarized.  Then, the placeholders in the template ({article_title} and {article_text}) are replaced with the actual title and text of the article using the .format() method.\n",
    "\n",
    "The GPT-4 model is then loaded using the ChatOpenAI class in order to provide the summary. The prepared prompt is then given as input/prompt to the language model. A HumanMessage list is accepted as an input argument by the chat instance of the ChatOpenAI class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VomIzpn0uO_G"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCBiLvA-uO5X",
    "outputId": "bf7d8f47-ac51-4700-c670-ab197af7ec0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Meta (formerly Facebook) has unveiled an AI supercomputer called the AI Research SuperCluster (RSC).\n",
      "- The RSC is yet to be fully complete but is already being used for training large natural language processing (NLP) and computer vision models.\n",
      "- Meta claims that the RSC will be the fastest in the world once complete and capable of training models with trillions of parameters.\n",
      "- The aim is for the RSC to help build entirely new AI systems that can power real-time voice translations to large groups of people.\n",
      "- Meta expects the RSC to be 20x faster than its current V100-based clusters for production.\n",
      "- The RSC is estimated to be 9x faster at running the NVIDIA Collective Communication Library (NCCL) and 3x faster at training large-scale NLP workflows.\n",
      "- Meta says that its previous AI research infrastructure only leveraged open source and other publicly-available datasets.\n",
      "- RSC was designed with security and privacy controls in mind to allow Meta to use real-world examples from its production systems in production training.\n",
      "- Meta can use RSC to advance research for vital tasks such as identifying harmful content on its platforms using real data from them.\n"
     ]
    }
   ],
   "source": [
    "# generate summary\n",
    "summary = chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz2Et93TwvOs"
   },
   "source": [
    "The utilisation of a few-shot learning approach in the prompt is the main takeaway from this attempt. This gives the model examples of how to carry out the task, directing it to create a bulleted list that summarises the article. You can alter the output of the model to satisfy different needs and make sure it adheres to a specific format, tone, style, etc. by changing the prompt and the examples.\n",
    "\n",
    "## Output Parsers\n",
    "\n",
    "Let's now advance by utilising output parsers. The LangChain Pydantic output parser provides a flexible mechanism to shape language model outputs in accordance with pre-defined schemas.  It allows for more structured interactions with language models and makes it simpler to extract and use the data the model provides when used in conjunction with prompt templates.\n",
    "\n",
    "Our parser's format recommendations are included in the prompt template, which directs the language model to generate the output in the appropriate format. The goal is to show how, rather than receiving the output as a string, you might use the PydanticOutputParser class to receive it as a type List that contains each bullet point. A list's benefit is the ability to loop through the results or index a particular item.\n",
    "\n",
    "As previously indicated, a parser that will convert the output from the string into a data structure is made using the PydanticOutputParser wrapper. The model's output will be analysed using the custom ArticleSummary class, which derives from BaseModel in the Pydantic package.\n",
    "\n",
    "Using the Field object, we established the schema to display a title and a summary variable that contains a list of text. Each variable must represent something, and the description argument will explain this and assist the model in doing so. Additionally, a validator function is included in our own class to guarantee that the output is generated with at least three bullet points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afMNWcL9uOyG"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import validator\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# create output parser class\n",
    "class ArticleSummary(BaseModel):\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    summary: List[str] = Field(description=\"Bulleted list summary of the article\")\n",
    "\n",
    "    # validating whether the generated summary has at least three lines\n",
    "    @validator('summary')\n",
    "    def has_three_or_more_lines(cls, list_of_lines):\n",
    "        if len(list_of_lines) < 3:\n",
    "            raise ValueError(\"Generated summary has less than three bullet points!\")\n",
    "        return list_of_lines\n",
    "\n",
    "# set up output parser\n",
    "parser = PydanticOutputParser(pydantic_object=ArticleSummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to design a template for the input prompt that tells the language model how to bullet point the news story. The prompts that are provided to the language model are correctly formatted using a PromptTemplate object, which is created using this template. The.get_format_instructions() method of the PromptTemplate, which will also contain extra instructions on how the output should be structured, is used to format the prompt delivered to the language model using our unique parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xutVrJsauOtJ"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# create prompt template\n",
    "# notice that we are specifying the \"partial_variables\" parameter\n",
    "template = \"\"\"\n",
    "You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"article_title\", \"article_text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Format the prompt using the article title and text obtained from scraping\n",
    "formatted_prompt = prompt.format_prompt(article_title=article_title, article_text=article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the GPT-3 model is initialised with the temperature set to 0.0, meaning the output will be deterministic and favour the most likely result over unpredictability or innovation. Using the.parse() method, the parser object subsequently transforms the model's output text into a specified schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRJJc4r_uOlW"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# instantiate model class\n",
    "model = OpenAI(model_name=\"text-davinci-003\", temperature=0.0)\n",
    "\n",
    "# Use the model to generate a summary\n",
    "output = model(formatted_prompt.to_string())\n",
    "\n",
    "# Parse the output into the Pydantic model\n",
    "parsed_output = parser.parse(output)\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Geu-uFOGxlbQ",
    "outputId": "d94d02c9-ad8e-4f00-8b95-c71b3949716d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleSummary(title='Meta claims its new AI supercomputer will set records', summary=['Meta (formerly Facebook) has unveiled an AI supercomputer that it claims will be the world’s fastest.', 'The supercomputer is called the AI Research SuperCluster (RSC) and is yet to be fully complete.', 'Meta says that it will be the fastest in the world once complete and the aim is for it to be capable of training models with trillions of parameters.', 'For production, Meta expects RSC will be 20x faster than Meta’s current V100-based clusters.', 'Meta says that its previous AI research infrastructure only leveraged open source and other publicly-available datasets.', 'What this means in practice is that Meta can use RSC to advance research for vital tasks such as identifying harmful content on its platforms—using real data from them.'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potent technique for shaping and organising the output from language models is the Pydantic output parser. It establishes and enforces data schemas for the model's output using the Pydantic library, which is renowned for its data validation skills.\n",
    "\n",
    "Here's an overview of what we did:\n",
    "\n",
    "- We created the ArticleSummary Pydantic data structure. This model acts as a guide for the structure that the generated article summary should have. It has fields for the summary and title, each of which should contain a list of strings that correspond to bullet points. In order to preserve a particular amount of depth in the summarization, it is crucial that we include a validator within this model to make sure the summary has at least three points.\n",
    "- Next, we use our ArticleSummary class to create a parser object. This parser is essential in ensuring that the language model's output adheres to the specified structures of our unique schema.\n",
    "- We develop the prompt template to control the language model's output. By adding the parser object, the template informs the model to serve as a helper that summarises internet content.\n",
    "- So, output parsers make it simpler to extract useful information from model replies by allowing us to specify the intended format of the model's output.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this post, we've demonstrated the possibilities of prompt handling in LangChain by building our News Articles Summarizer using the potential of PromptTemplates and OutputParsers. A potent technique for shaping and organising the output from language models is the Pydantic output parser. It establishes and enforces data schemas for the model's output using the Pydantic library, which is renowned for its data validation skills.\n",
    "\n",
    "This is followed by the definition of the Pydantic model \"ArticleSummary.\" This model acts as a guide for the structure that the generated article summary should have. It has fields for the summary and title, each of which should contain a list of strings that correspond to bullet points. In order to preserve a particular amount of depth in the summarization, it is crucial that we include a validator within this model to make sure the summary has at least three points.\n",
    "\n",
    "The \"ArticleSummary\" model is then given a PydanticOutputParser that we just created. This parser is essential in ensuring that the language model's output adheres to the structure described in the \"Article Summary\" model.\n",
    "\n",
    "If you have a solid grasp of the subtleties involved in prompt and output design, you can adapt the model to deliver outcomes that address your unique needs\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [LangChain & Vector Databases in Production Course](https://learn.activeloop.ai/courses/langchain) by Activeloop - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
