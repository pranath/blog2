{
 "cells": [
  {
   "cell_type": "raw",
   "id": "89a2d929-6a5e-49f6-b96b-4b17d5e0b91d",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-11-04'\n",
    "categories:\n",
    " - natural-language-processing\n",
    " - deep-learning\n",
    " - langchain\n",
    " - openai\n",
    "title: An Introduction to LangChain Expression Language (LCEL)\n",
    "description: Here we introduce is a new syntax LangChain Expression Language (LCEL) that makes it much easier and more transparent to construct and work with different LLM chains and agents for LLM based applications\n",
    "image: https://github.com/pranath/blog/raw/master/images/langchain1.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc925562",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In the evolving world of large language models and AI, the need for more streamlined and efficient methods of chaining components and agents together has led to the development of LangChain, and more recently the LangChain Expression Language (LCEL). This new syntax simplifies the construction and management of these llm chains, enhancing transparency and usability, enabling the building of LLM application prototypes quicker and faster.\n",
    "\n",
    "## Understanding LCEL: A New Protocol for Chaining LangChain Components\n",
    "\n",
    "LCEL presents a new approach to composing various components of LangChain. It has a protocol that specifies a set of permissible input and output types, along with a suite of standard methods. These methods are designed to be universally applicable across all runables, allowing for uniform invocation. Moreover, LCEL facilitates the dynamic adjustment of parameters during runtime and the incorporation of additional options, such as fallback mechanisms.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/lcel-1.png\" width=\"800\"/>\n",
    "\n",
    "A large part of LangChain's power derives from combining chains of diverse components. LCEL, a runable protocol that defines these things, is a new way of doing this. It begins by defining an acceptable set of input types and a corresponding allowed set of output types. There are then a number of standard methods, which we will go over in detail, but these are standard ways that all runables use, so you can refer to them all in the same way. \n",
    "\n",
    "### The Interface of Runables in LCEL\n",
    "\n",
    "Every runable under LCEL is expected to adhere to a common interface, consisting of methods like 'invoke' for single inputs, 'stream' for streaming responses, and 'batch' for listed inputs. To cater to different operational needs, each of these synchronous methods is paired with an asynchronous counterpart, ensuring flexibility across various use cases.\n",
    "\n",
    "### Common Properties and Methods\n",
    "\n",
    "Runables share common properties, including input and output schemas that define the types they handle. This standardization is crucial for consistency and interoperability among different components within LangChain.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/lcel-2.png\" width=\"800\"/>\n",
    "\n",
    "## The Advantages of Utilizing LCEL\n",
    "\n",
    "LCEL offers several benefits to developers and users alike. It comes equipped with asynchronous operations, streaming, and batch processing capabilities, making the transition from development to production seamless. Its design inherently supports parallel processing, vital for time-intensive language model (LLM) calls. Built-in logging capabilities provide visibility into the complex sequences of steps within chains and agents, essential for robust application development.\n",
    "\n",
    "<img src=\"https://github.com/pranath/blog/raw/master/images/lcel-3.png\" width=\"800\"/>\n",
    "\n",
    "### Fallbacks, Parallelism, and Logging\n",
    "\n",
    "The ability to attach fallbacks to both individual LLMs and entire chains ensures reliability in unpredictable scenarios. Parallel execution of LLM calls via LCEL maximizes efficiency. As chains and agents grow in complexity, the integrated logging system within LCEL offers a critical tool for monitoring and debugging.\n",
    "\n",
    "## Practical Application of LCEL\n",
    "\n",
    "When it comes to practical usage, setting up an LCEL environment involves importing various components such as prompt templates, language models, and output parsers. These elements are then seamlessly chained together using LCEL's syntax, with the ability to pass additional arguments and modify prompts on the fly.\n",
    "\n",
    "Let's first import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044db1b7-c18c-45d2-be7a-29f027c901e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de9d2bd-18de-44d5-81ac-5918e2106367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd55c0a0-ca4e-4311-a33c-fcebeb7d8b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranathfernando/opt/anaconda3/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.4) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c432b-b2c9-497b-9912-c9ca4c1e3740",
   "metadata": {},
   "source": [
    "## Simple Chain\n",
    "\n",
    "### Setting Up and Executing Chains\n",
    "\n",
    "A simple chain might link a prompt template to a language model and then to an output parser. The integration process is straightforward, connecting components with specified types and utilizing the 'invoke' method for invocation, with inputs tailored to the prompt template's requirements. And we'll make one in which we ask the language model to give us a brief joke. The language model that will be used will then be initialised. Finally, we'll design the output parser that we'll use. \n",
    "\n",
    "All that remains is to connect them with the types and apps in order to form a chain. And we can now use the invoke method to call this chain. And the inputs here will be the prompt template's inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0be20d-0e00-478c-a844-017cad13af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aedf1c1e-b697-47ce-9d81-eaec9192243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df32028-d35f-4392-bb15-ddeec9ee09b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba33b5-2ae9-44d7-b023-18c903af571a",
   "metadata": {},
   "source": [
    "## More complex chain\n",
    "\n",
    "### Advanced Chaining Techniques\n",
    "\n",
    "Beyond basic chains, LCEL also supports more complex operations like retrieval-augmented generation. This involves setting up retrievers, creating prompts that incorporate context and user questions, and then constructing chains that process these elements in a streamlined fashion.\n",
    "\n",
    "In a previous post [we discussed retrieval augmented generation (RAG)](/posts/2023-07-25-chat-with-your-data-using-memory-and-langchain.html). So we'll use LCEL to mimic that process. \n",
    "\n",
    "First, we must configure our retrievers. We'll start by making a very simple vector storage. This will be accomplished by initialising it with two types. We'll then pass in the embedding model, open an embedding, and use vector store to generate a retriever. This includes retrievers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d036bb8e-8ca7-4dbd-8103-f50a3c8c3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8955cff7-f1a2-4f94-ab5b-fcdda0859702",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"pranath worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ff0ee-6681-45e0-ae61-c7811796ce3a",
   "metadata": {},
   "source": [
    "So we call it with \"where did Pranath work?\" And we get a list of documents back, with the first one visible. If we ask it something different, such as what do bears like to eat, we can see that the first answer is that bears like to eat honey. So this is a simplistic example because there are just two documents, but the main point is that you can do this on a bigger number of documents and it will return the most relevant document, which we will then utilise in our retriever order of creation pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df87934-1697-405c-b460-5e9bfd16c792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='pranath worked at kensho', metadata={}),\n",
       " Document(page_content='bears like to eat honey', metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"where did pranath work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "871cb26b-97b3-4f63-8bb3-523d3e6d117b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='bears like to eat honey', metadata={}),\n",
       " Document(page_content='pranath worked at kensho', metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what do bears like to eat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "127a7fb6-5821-4934-ab56-9e3300516c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de70e9-1502-4c88-b6ce-f6db74ea1440",
   "metadata": {},
   "source": [
    "To build this pipeline, we'll start with a prompt that basically asks the language model to answer the question based only on the subsequent context, and has two variables here, as context and question, so we'll be packing in two. We'll be packing in the question, which will be the user query, and then we'll be packing in the context, which will be the documents that we recover from the retriever. \n",
    "\n",
    "So, how do we plan on constructing this chain? The user question should be the first and only implicit chain. From there, we want to retrieve relevant context, which we will then pass into the prompt, which we will then pass into a model, which we will then give into the other person and transform the chat message into a string. So the first thing we'll do is build something that takes a single question and converts it into a dictionary comprising two elements: context and question. To accomplish so, we'll utilise a runnable map, which we can import like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ec01c56-731c-4e4f-a5f6-493fba953db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f6e8f-10cc-475e-b93c-760c0ca57d06",
   "metadata": {},
   "source": [
    "This runnable map will contain two elements: the first is context, and the context will call the retriever. We may create a lambda function that calls the retriever and accepts the query argument as a parameter. We also want to pass the original question parameter forward, which we can do with another simple lambda function. From here, we want to feed that into the prompt, then into the model, and finally into the output parser. \n",
    "\n",
    "So we can just put all of those together in a pipe, call it our chain, and we have a new runnable. And now we bring it up again with the inquiry, \"Where did Pranath work?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9ca6506-826f-4420-8f19-25dd4dbbc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "707d1319-8840-4ed5-b4a4-a2a128799db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pranath worked at Kensho.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"where did pranath work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ee5ec-91b0-461c-b53b-a7a20fd9bfa3",
   "metadata": {},
   "source": [
    "We'd like to take a closer look at what's going on in the scene. We can simply look at this runnable map. So let's make a list of variables, hoping, but, and then if we run this on the same thing, we can see that we receive back context, which is a listed document, and then the question, so there's both pieces there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05ec3727-4284-417e-9e23-eec0682eb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4216c7ab-6d1b-4f2a-98dc-5d2ace23e3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='pranath worked at kensho', metadata={}),\n",
       "  Document(page_content='bears like to eat honey', metadata={})],\n",
       " 'question': 'where did pranath work?'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.invoke({\"question\": \"where did pranath work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec59c3b-33e7-437f-9b8b-b4652bc3b863",
   "metadata": {},
   "source": [
    "This is then sent into the prompt; we've just finished constructing a prompt value, so we'll just send it into the model, which will return the chat message, pass the output person, and lastly return the string. Another thing we can do with runnables is tie useful functions to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3efed3b-6d4c-42a4-9692-0cc4596f530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8be4721-91d2-4ae6-8fdb-e91dc6ac1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e61b095d-9934-41b8-a794-a9dd57e9c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a638efeb-b5ce-4aa4-8377-3e86597a03ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'weather_search', 'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}'}}, example=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"what is the weather in sf\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a690bcc-0a03-4150-be87-15ac1fdf650c",
   "metadata": {},
   "source": [
    "So, given this collection of functions, we wish to invoke the language model using those functions. Normally, this would seem like a prompt followed by a model. But what about the functions? So, to insert the functions, we can perform **bind**, then functions = functions, and what this does under the hood is that when the model is called, it will send any parameters and bind along with the invitation. So, if we construct a runnable by executing prompt and then typing model, when we call the runnable, an input that should use the function, such as what is the weather, is passed. \n",
    "\n",
    "We can see that what we get back is a method with this extra parsing argument and the function call that we expect to return. So lets include another function, sport search. It will look for news about recent sporting events and will enter a team name. We now have two distinct functions here. We may simply update the model by adding additional functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a22faf5-ea24-48d2-b028-03733b548225",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb43b030-459f-47e8-a27d-96c2d70cdfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ff03e0d-d6c6-4b47-815e-d7ea5b248567",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03855fa3-5e2f-4ab2-aba0-c2cd5423239e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'sports_search', 'arguments': '{\\n  \"team_name\": \"patriots\"\\n}'}}, example=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc0c55-48c2-4105-90ec-7297553b8e6a",
   "metadata": {},
   "source": [
    "We can see that it is now aware of the sport search. \n",
    "\n",
    "## Implementing Fallbacks and Parallelism\n",
    "\n",
    "LCEL's power is further demonstrated in its ability to handle fallbacks for entire sequences and facilitate parallel executions. For instance, creating a retriever-augmented generation pipeline involves various steps, from fetching relevant documents to parsing the final output, all of which can be configured to run in parallel, enhancing performance.\n",
    "\n",
    "### Debugging and Modifying Chains\n",
    "\n",
    "Debugging is made easier with LCEL, as developers can inspect each step of the chain and the transformations it undergoes. Additionally, chains can be dynamically altered with new functions or parameters, showcasing the flexibility of the LCEL system.\n",
    "\n",
    "So, when we're attempting to get a language model to generate JSON, let's take a look at that. We'll utilise an older version of a language model from OpenAI that isn't quite as good - to create a circumstance that will fail. To accomplish this, we will import an OpenAI language model. This is distinct from chat models. Chat models are newer forms of models, although they're generally quite good at things like JSON output. Older models, on the other hand, aren't. Now we'll make a simple model and a simple chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa0b1ea2-7aef-4449-a553-426cb8c5aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17f90857-5625-4dba-bf3f-99432bd7f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = OpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1000, \n",
    "    model=\"text-davinci-001\"\n",
    ")\n",
    "simple_chain = simple_model | json.loads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5891872-4087-4ec8-a618-4c1142288ed2",
   "metadata": {},
   "source": [
    "In this simple model, we'll set the temperature to zero to get some determinism. This is an extremely early version of an OpenAI model. Our chain will simply call this language model and then pipe the output into json.loads. This will fail if the outcome of the language model is not correct. We'll ask it to write three poems in a JSON blob, with each containing a JSON blob with a title, author, and opening line. This is a difficult problem, and we want it to respond with a valid JSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "441928c5-8712-45c5-bfdf-6f51634198a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0739e85f-8497-4471-8ec9-17e958d80771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n[\"The Waste Land\",\"T.S. Eliot\",\"April is the cruelest month, breeding lilacs out of the dead land\"]\\n\\n[\"The Raven\",\"Edgar Allan Poe\",\"Once upon a midnight dreary, while I pondered, weak and weary\"]\\n\\n[\"Ode to a Nightingale\",\"John Keats\",\"Thou still unravish\\'d bride of quietness, Thou foster-child of silence and slow time\"]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20d15c-dc8a-4b6d-a423-a5f814425219",
   "metadata": {},
   "source": [
    "And so if we run the simple model on this, we can see that it responds with some structure format, and it's kind of close. But if we take this and try to decode it with JSON, we can see that we get a JSON decode error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a5e8492-0927-4a3a-b939-947826246330",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 5 column 1 (char 103)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nx/h0vy791n1cj763xzkg85h0940000gn/T/ipykernel_1284/3924921287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchallenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/schema/runnable/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m    779\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/schema/runnable/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     ) -> Output:\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/schema/runnable/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maccepts_run_manager_and_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 output = func(\n\u001b[0m\u001b[1;32m    280\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                     \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/schema/runnable/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m     ) -> Output:\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0;31m# If the output is a runnable, invoke it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 5 column 1 (char 103)"
     ]
    }
   ],
   "source": [
    "simple_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb502953-f7a1-4c09-8f15-9fdace4297c1",
   "metadata": {},
   "source": [
    "The latest OpenAI models are quite good at producing JSON. So we'll make a model called ChatOpenAI, and then we'll make a chain by piping that model with an output person, who will basically take the chat message from the model and convert it to a string, and then we'll pipe that string into json.loads. We can see that this model successfully outputs validation when we apply the same challenge to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6814143b-4a35-4d29-bd32-ba461274bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f55f04cf-0217-4106-b41f-0e0661d12c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'Whispers of the Wind',\n",
       "  'author': 'Emily Rivers',\n",
       "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
       " 'poem2': {'title': 'Silent Serenade',\n",
       "  'author': 'Jacob Moore',\n",
       "  'first_line': 'In the stillness of night, a silent serenade'},\n",
       " 'poem3': {'title': 'Dancing Shadows',\n",
       "  'author': 'Sophia Anderson',\n",
       "  'first_line': 'Shadows dance upon the moonlit floor'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f9c83-ee52-48bb-9fb4-83402e8f0a4d",
   "metadata": {},
   "source": [
    "We can see that when we call this chain on the challenge, it answers in the correct format. So it tried on the simple chain, failed, then went back to the good chain that sees in ChatOpenAI and succeeded, and that's what we're getting from this final chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5d3f035-b18d-4cba-854d-a43ef8554e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = simple_chain.with_fallbacks([chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a09fe6a-548c-412d-9468-9efe2b49f7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'Whispers of the Wind',\n",
       "  'author': 'Emily Rivers',\n",
       "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
       " 'poem2': {'title': 'Silent Serenade',\n",
       "  'author': 'Jacob Moore',\n",
       "  'first_line': 'In the stillness of night, a silent serenade'},\n",
       " 'poem3': {'title': 'Dancing Shadows',\n",
       "  'author': 'Sophia Anderson',\n",
       "  'first_line': 'Shadows dance upon the moonlit floor'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfdda0-3db2-4073-a647-f2d62c460349",
   "metadata": {},
   "source": [
    "## Interface\n",
    "\n",
    "Finally, lets consider the interface of these runnables. Let's go back to the previous chain and tell ourselves a joke. Let's have a look at some of the interface's different elements now. The one we were using from the start is invoke. This is a straightforward approach that relies on a single variable. There's also that, which will call it based on a set of inputs. So we call chain.batch after passing in two different inputs, one with bears and one with frogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33b3b27f-b5a0-4db5-a1b0-20754437a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48c8cabf-ea55-4448-b070-3ec22942c559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc58bdb4-a896-46ba-90c4-1b333245229a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\",\n",
       " 'Why did the frog take the bus to work?\\n\\nBecause his car got toad away!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b281ec-6161-4b20-9dbb-4b499505c8d5",
   "metadata": {},
   "source": [
    "And, under the hood, this is actually executing them in parallel to the greatest extent possible. We can also receive responses in real time. So chain.stream will return an iterable, and we can loop through it to print out the various elements. So, if we run this now, we can see the reaction to that streaming to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a069d61-0a67-4368-b7c4-367262267bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " don\n",
      "'t\n",
      " bears\n",
      " wear\n",
      " shoes\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " they\n",
      " already\n",
      " have\n",
      " bear\n",
      " feet\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2315b43f-c7e1-4f7b-9595-4cabdc019dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why don\\'t bears use cell phones? \\n\\nBecause they can\\'t find the \"paws\" button!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e3cf3-36b3-4d71-832c-5623df3b729e",
   "metadata": {},
   "source": [
    "## Conclusion: The Future of Chaining with LCEL\n",
    "\n",
    "LangChain Expression Language represents a significant advancement in the realm of language model chaining, offering a robust, flexible, and efficient framework. Its ability to handle asynchronous operations, fallbacks, parallel processing, and detailed logging paves the way for sophisticated LLM applications. With LCEL, developers are equipped to build, modify, and debug complex chains, ushering in a new era of AI-driven solutions.\n",
    "\n",
    "Find out more about LCEL in [this article](https://blog.langchain.dev/langchain-expression-language/) and this [documentation](https://python.langchain.com/docs/expression_language/).\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd like to express my thanks to the wonderful [Functions, Tools and Agents with LangChain](https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/) by DeepLearning.ai - which i completed, and acknowledge the use of some images and other materials from the course in this article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
