<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>LivingDataLab</title>
<link>http://livingdatalab.com/index.html</link>
<atom:link href="http://livingdatalab.com/index.xml" rel="self" type="application/rss+xml"/>
<description>LivingDataLab Data Science Blog</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Thu, 13 Jul 2023 23:00:00 GMT</lastBuildDate>
<item>
  <title>Fine-Tuning a Generative AI Model for Dialogue Summarization</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-14-finetune-generative-ai-model-summarisation.html</link>
  <description><![CDATA[ In this project I will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. We will use the <a href="https://huggingface.co/docs/transformers/model_doc/flan-t5">FLAN-T5</a> model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, we will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then we will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>aws</category>
  <category>hugging-face</category>
  <guid>http://livingdatalab.com/posts/2023-07-14-finetune-generative-ai-model-summarisation.html</guid>
  <pubDate>Thu, 13 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Parameter Efficient Fine-Tuning (PEFT) for Large Language Models</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-13-parameter-efficient-fine-tuning-of-llms.html</link>
  <description><![CDATA[ It takes a lot of computation to train LLMs. Memory is needed for complete fine-tuning not just to store the model but also a number of other training-related factors. You must be able to allocate memory for optimizer states, gradients, forward activations, and temporary memory throughout the training process even if your computer can hold the model weights, which are currently on the order of hundreds of terabytes for the largest models. These extra parts may be many times bigger than the model and can easily outgrow the capabilities of consumer hardware. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-13-parameter-efficient-fine-tuning-of-llms.html</guid>
  <pubDate>Wed, 12 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai1.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Evaluating Fine-Tuned Large Language Models</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-12-evaluating-fine-tuned-large-language-models.html</link>
  <description><![CDATA[ When looking at large language models such as ChatGPT and others we might customise and fine tune to improve, we might often describe it by saying the model demonstrated good performance on this task or this fine-tuned model showed a large improvement in performance over the base model. But what do statements like this mean? How can you formalize the improvement in performance of your fine-tuned model over the pre-trained model you started with? In this article we explore several metrics that are used by developers of large language models that you can use to assess the performance of your own models and compare to other models out in the world. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-12-evaluating-fine-tuned-large-language-models.html</guid>
  <pubDate>Tue, 11 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai4.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Multi-task Instruction Fine-Tuning for Large Language Models</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-11-multi-task-instruction-fine-tuning.html</link>
  <description><![CDATA[ In this post, we’ll look at techniques you might employ to make an existing large language model more effective for your particular use case using a method called instruction fine-tuning, and in particular see how this can be used to optimise for multiple tasks as the same time. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-11-multi-task-instruction-fine-tuning.html</guid>
  <pubDate>Mon, 10 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai3.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Improve Large Language Models with Instruction Fine-Tuning</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-10-fine-tuning-llms-with-instructions.html</link>
  <description><![CDATA[ In this post, we’ll look at techniques you might employ to make an existing large language model more effective for your particular use case using a method called instruction fine-tuning. We will also see how this differs from using prompts and in-context prompt learning. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-10-fine-tuning-llms-with-instructions.html</guid>
  <pubDate>Sun, 09 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Pre-training Large Language Models for Domain Adaptation</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-09-pretraining-llms-for-domain-adaptation.html</link>
  <description><![CDATA[ Here we will examine particular use cases where it might make sense to train a large language model from scratch. These use cases are often characterised by situations that use language in a very unique way such as legal or medical text. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-09-pretraining-llms-for-domain-adaptation.html</guid>
  <pubDate>Sat, 08 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai1.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Scaling Laws and Compute Optimal Large Language Models</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-08-scaling-laws-compute-optimal-llms.html</link>
  <description><![CDATA[ In this article we’ll look at research that has looked at the relationship between model size, training, configuration, and performance to try to pinpoint the optimal size for large language models. It’s important to keep in mind that the objective of pre-training is to maximise the model’s achievement of its learning objective, which is to minimise the loss while predicting tokens. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-08-scaling-laws-compute-optimal-llms.html</guid>
  <pubDate>Fri, 07 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai4.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Computational Challenges fo training LLMs</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-07-computational-challenges-of-training-llms.html</link>
  <description><![CDATA[ Running out of memory is one of the most frequent problems you still encounter when trying to train large language models. Compute Unified Device Architecture, or CUDA, is a group of tools and libraries created specifically for Nvidia GPUs. Libraries like PyTorch and TensorFlow make advantage of CUDA to improve performance on deep learning operations like metrics multiplication. Because most LLMs are large and need a lot of memory to store and train all of their parameters, you’ll run across these memory concerns. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-07-computational-challenges-of-training-llms.html</guid>
  <pubDate>Thu, 06 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai3.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Choosing a Pre-Trained Large Language Model</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-06-choosing-a-pretrained-llm.html</link>
  <description><![CDATA[ The project life cycle for generative AI was introduced in this <a href="../posts/2023-07-04-generative-ai-project-lifecycle.html">previous article</a>. There are a few tasks to complete before you can launch your generative AI app, as we saw there. Selecting a model to work with comes after you have defined your use case and chosen how the LLM will operate within your application. Working with an existing model or creating your own from scratch will be your first option. In some situations, it may be advantageous to build your own model from scratch. In most cases, though, you’ll use an existing foundation model to start the process of constructing your application. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>hugging-face</category>
  <guid>http://livingdatalab.com/posts/2023-07-06-choosing-a-pretrained-llm.html</guid>
  <pubDate>Wed, 05 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Summarising Dialogue using Generative AI</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-05-summarising-dialogue-using-generative-ai.html</link>
  <description><![CDATA[ In this article I will perform dialogue summarization using generative AI. We will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task we need. By comparing zero shot, one shot, and few shot inferences, we will take the first steps towards prompt engineering and see how it can enhance the generative output of Large Language Models. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>hugging-face</category>
  <guid>http://livingdatalab.com/posts/2023-07-05-summarising-dialogue-using-generative-ai.html</guid>
  <pubDate>Tue, 04 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai1.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>An Approach to the Generative AI Project Lifecyle</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-04-generative-ai-project-lifecycle.html</link>
  <description><![CDATA[ Recent advances in AI such as ChatGPT have demonstrated impressive abilities for performing a wide range of tasks previously only done by humans, these are known as Large Language Models (LLM’s). However, there are so many different options and methods for building applications with these models, and these are all new methods. It can seem overhelming and not obvious how to approach building Generative AI applications. In this article, I will present a high level project architecture for building Generative AI projects that could be applied to any project, which has been proposed by DeepLearning AI and AWS in their <a href="https://www.deeplearning.ai/courses/generative-ai-with-llms/">Generative AI with Large Language Models Course</a> which I recently completed. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-04-generative-ai-project-lifecycle.html</guid>
  <pubDate>Mon, 03 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai4.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Generative Configuration for Large Language Models</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-03-generative-config-for-large-language-models.html</link>
  <description><![CDATA[ Recent advances in AI such as ChatGPT have demonstrated impressive abilities for performing a wide range of tasks previously only done by humans, these are known as Large Language Models (LLM’s). Once you have chosen a model, you often have some configuration options that let you control the outputs. In this article we will take a high level non-technical view of what these generative configuration options allow you to do. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-03-generative-config-for-large-language-models.html</guid>
  <pubDate>Sun, 02 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai3.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A High Level Overview of Prompting and In-Context Learning for Large Language Models</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-02-high-level-prompting-prompt-engineering.html</link>
  <description><![CDATA[ Recent advances in AI such as ChatGPT have demonstrated impressive abilities for performing a wide range of tasks previously only done by humans, these are known as Large Language Models (LLM’s). One key aspect for using these LLM’s is called prompting - which is how to write text requests to get the outputs you want from these models. In <a href="../#category=natural-language-processing">previous articles</a> I’ve looked at detailed practical use cases for how to make these prompts. In this article we will take a high level non-technical view of what prompting is all about, and introduce what in-context learning is. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-02-high-level-prompting-prompt-engineering.html</guid>
  <pubDate>Sat, 01 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A High Level Overview of the Transformer Model - The Magic Behind Recent Advances in AI</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-07-01-high-level-overview-of-the-transformer-model-the-magic-behind-AI.html</link>
  <description><![CDATA[ Recent advances in AI such as ChatGPT have demonstrated impressive abilities for performing a wide range of tasks previously only done by humans. The key technology used in these models is called the Transformer Model. In <a href="../#category=natural-language-processing">previous articles</a> I’ve looked at the detailed theoretical underpinnings of this model as well as practical use cases. In this article we will take a high level non-technical view of key aspects of the Transformer Model that have enabled it to make the huge advances that it has made. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <guid>http://livingdatalab.com/posts/2023-07-01-high-level-overview-of-the-transformer-model-the-magic-behind-AI.html</guid>
  <pubDate>Fri, 30 Jun 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/genai1.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Evaluating the outputs of Large Language Model Applications for Ambiguous Criteria</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-06-26-evaluating-outputs-of-llm-applications-ambiguous-criteria.html</link>
  <description><![CDATA[ Large language models such as <a href="https://openai.com/blog/chatgpt">ChatGPT</a> can generate text responses based on a given prompt or input. Writing prompts allow users to guide the language model’s output by providing a specific context or topic for the response. This feature has many practical applications, such as generating creative writing prompts, assisting in content creation, and even aiding in customer service chatbots. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>openai</category>
  <guid>http://livingdatalab.com/posts/2023-06-26-evaluating-outputs-of-llm-applications-ambiguous-criteria.html</guid>
  <pubDate>Sun, 25 Jun 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/chatgpt3.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Evaluating the outputs of Large Language Model Applications for Clear Criteria</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-06-25-evaluating-outputs-of-llm-applications-clear-criteria.html</link>
  <description><![CDATA[ Large language models such as <a href="https://openai.com/blog/chatgpt">ChatGPT</a> can generate text responses based on a given prompt or input. Writing prompts allow users to guide the language model’s output by providing a specific context or topic for the response. This feature has many practical applications, such as generating creative writing prompts, assisting in content creation, and even aiding in customer service chatbots. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>openai</category>
  <guid>http://livingdatalab.com/posts/2023-06-25-evaluating-outputs-of-llm-applications-clear-criteria.html</guid>
  <pubDate>Sat, 24 Jun 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/chatgpt2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Creating Better Chatbots using Chained Prompts and Quality Checks</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-06-24-creating-better-chatbots-using-chained-prompts-and-quality-checks.html</link>
  <description><![CDATA[ Large language models such as <a href="https://openai.com/blog/chatgpt">ChatGPT</a> can generate text responses based on a given prompt or input. Writing prompts allow users to guide the language model’s output by providing a specific context or topic for the response. This feature has many practical applications, such as generating creative writing prompts, assisting in content creation, and even aiding in customer service chatbots. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>openai</category>
  <guid>http://livingdatalab.com/posts/2023-06-24-creating-better-chatbots-using-chained-prompts-and-quality-checks.html</guid>
  <pubDate>Fri, 23 Jun 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/chatgpt1.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Checking Outputs of Large Language Models like ChatGPT</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-06-23-checking-outputs-of-llms-like-chatgpt.html</link>
  <description><![CDATA[ Large language models such as <a href="https://openai.com/blog/chatgpt">ChatGPT</a> can generate text responses based on a given prompt or input. Writing prompts allow users to guide the language model’s output by providing a specific context or topic for the response. This feature has many practical applications, such as generating creative writing prompts, assisting in content creation, and even aiding in customer service chatbots. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>openai</category>
  <guid>http://livingdatalab.com/posts/2023-06-23-checking-outputs-of-llms-like-chatgpt.html</guid>
  <pubDate>Thu, 22 Jun 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/chatgpt3.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Chaining Multiple Prompts together using ChatGPT for Better Task Execution</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-06-22-chaining-multiple-prompts-together-using-chatgpt.html</link>
  <description><![CDATA[ Large language models such as <a href="https://openai.com/blog/chatgpt">ChatGPT</a> can generate text responses based on a given prompt or input. Writing prompts allow users to guide the language model’s output by providing a specific context or topic for the response. This feature has many practical applications, such as generating creative writing prompts, assisting in content creation, and even aiding in customer service chatbots. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>openai</category>
  <guid>http://livingdatalab.com/posts/2023-06-22-chaining-multiple-prompts-together-using-chatgpt.html</guid>
  <pubDate>Wed, 21 Jun 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/chatgpt2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Using Chain of Thought Reasoning with ChatGPT</title>
  <dc:creator>Pranath Fernando</dc:creator>
  <link>http://livingdatalab.com/posts/2023-06-21-chain-of-thought-reasoning-with-chatgpt.html</link>
  <description><![CDATA[ Large language models such as <a href="https://openai.com/blog/chatgpt">ChatGPT</a> can generate text responses based on a given prompt or input. Writing prompts allow users to guide the language model’s output by providing a specific context or topic for the response. This feature has many practical applications, such as generating creative writing prompts, assisting in content creation, and even aiding in customer service chatbots. ]]></description>
  <category>natural-language-processing</category>
  <category>deep-learning</category>
  <category>openai</category>
  <guid>http://livingdatalab.com/posts/2023-06-21-chain-of-thought-reasoning-with-chatgpt.html</guid>
  <pubDate>Tue, 20 Jun 2023 23:00:00 GMT</pubDate>
  <media:content url="https://github.com/pranath/blog/raw/master/images/chatgpt1.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
