<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Pranath Fernando">
<meta name="dcterms.date" content="2023-07-01">
<meta name="description" content="In this article we will take a high level non-technical view of key aspects of the Transformer Model - the technology behind recent advances in AI">

<title>LivingDataLab - A High Level Overview of the Transformer Model - The Magic Behind Recent Advances in AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-91568149-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<link rel="stylesheet" href="../css/styles.css">
<meta property="og:title" content="LivingDataLab - A High Level Overview of the Transformer Model - The Magic Behind Recent Advances in AI">
<meta property="og:description" content="In this article we will take a high level non-technical view of key aspects of the Transformer Model - the technology behind recent advances in AI">
<meta property="og:image" content="https://github.com/pranath/blog/raw/master/images/genai1.jpg">
<meta property="og:site-name" content="LivingDataLab">
<meta name="twitter:title" content="LivingDataLab - A High Level Overview of the Transformer Model - The Magic Behind Recent Advances in AI">
<meta name="twitter:description" content="In this article we will take a high level non-technical view of key aspects of the Transformer Model - the technology behind recent advances in AI">
<meta name="twitter:image" content="https://github.com/pranath/blog/raw/master/images/genai1.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">LivingDataLab</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../projects.html" aria-current="page">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/pranath-fernando/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/LivingDataLab"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pranath"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">A High Level Overview of the Transformer Model - The Magic Behind Recent Advances in AI</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">A High Level Overview of the Transformer Model - The Magic Behind Recent Advances in AI</h1>
                  <div>
        <div class="description">
          In this article we will take a high level non-technical view of key aspects of the Transformer Model - the technology behind recent advances in AI
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">natural-language-processing</div>
                <div class="quarto-category">deep-learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Pranath Fernando </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 1, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects.html" class="sidebar-item-text sidebar-link">Projects Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Projects</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/doc-chat.html" class="sidebar-item-text sidebar-link">Document Chat</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/doc-summarisation.html" class="sidebar-item-text sidebar-link">Document Summarisation</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/web-page-chat.html" class="sidebar-item-text sidebar-link">Web Page Chat</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/web-page-summarisation.html" class="sidebar-item-text sidebar-link">Web Page Summarisation</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/youtube-chat.html" class="sidebar-item-text sidebar-link">YouTube Chat</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/youtube-summarisation.html" class="sidebar-item-text sidebar-link">YouTube Summarisation</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
    /* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
    <form action="https://livingdatalab.us8.list-manage.com/subscribe/post?u=e2d57b0d6e43b4f6bff927a55&amp;id=a30bdff125&amp;f_id=009d05e0f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2>Subscribe</h2>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e2d57b0d6e43b4f6bff927a55_a30bdff125" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
            </div>
        </div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script>
<!--End mc_embed_signup-->

</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#languge-models-before-transformers" id="toc-languge-models-before-transformers" class="nav-link" data-scroll-target="#languge-models-before-transformers"><span class="toc-section-number">2</span>  Languge Models before Transformers</a></li>
  <li><a href="#languge-models-after-transformers" id="toc-languge-models-after-transformers" class="nav-link" data-scroll-target="#languge-models-after-transformers"><span class="toc-section-number">3</span>  Languge Models after Transformers</a>
  <ul class="collapse">
  <li><a href="#embedding-layers" id="toc-embedding-layers" class="nav-link" data-scroll-target="#embedding-layers"><span class="toc-section-number">3.1</span>  Embedding Layers</a></li>
  <li><a href="#self-attention-layers" id="toc-self-attention-layers" class="nav-link" data-scroll-target="#self-attention-layers"><span class="toc-section-number">3.2</span>  Self-Attention Layers</a></li>
  <li><a href="#feed-forward-layers-final-output" id="toc-feed-forward-layers-final-output" class="nav-link" data-scroll-target="#feed-forward-layers-final-output"><span class="toc-section-number">3.3</span>  Feed-Forward Layers &amp; Final Output</a></li>
  </ul></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements"><span class="toc-section-number">4</span>  Acknowledgements</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Recent advances in AI such as ChatGPT have demonstrated impressive abilities for performing a wide range of tasks previously only done by humans. The key technology used in these models is called the Transformer Model. In <a href="../#category=natural-language-processing">previous articles</a> I’ve looked at the detailed theoretical underpinnings of this model as well as practical use cases. In this article we will take a high level non-technical view of key aspects of the Transformer Model that have enabled it to make the huge advances that it has made.</p>
</section>
<section id="languge-models-before-transformers" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="languge-models-before-transformers"><span class="header-section-number">2</span> Languge Models before Transformers</h2>
<p>I have covered language models in <a href="../#category=natural-language-processing">previous articles</a> - but some of the earliest and simplest language models essentially just predicted the next word in a sequence.</p>
<p>Language models are not brand-new. Recurrent neural networks, or RNNs, are a type of architecture that was used by earlier generations of language models. Although extremely powerful for their time, RNNs were constrained by the amount of memory and compute required to excel in generative tasks. Let’s examine an illustration of an RNN performing a straightforward next-word prediction generating task.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai1.png" width="800"></p>
<p>The model can’t make an accurate forecast with only one previous word observed by it. The resources that the model utilises must be greatly scaled when you scale the RNN implementation to be able to see more of the words that come before them in the text.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai2.png" width="800"></p>
<p>The model still hasn’t seen enough input, regardless of how much you scale it, to make a reliable prediction. Models require much more information than just the last few words in order to accurately predict the following word. Models must comprehend the entirety of the sentence, if not the entire document. Language complexity is the issue in this situation. One word can mean many different things in many different languages. They are homophones for example. It is possible for words to contain syntactic ambiguity or ambiguity inside sentence structures.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai3.png" width="800"></p>
<p>Take for example this sentence, “The teacher taught the students with the book.” Did the teacher teach using the book or did the student have the book, or was it both? How can an algorithm make sense of human language if sometimes we can’t?</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai4.png" width="800"></p>
<p>Everything changed in 2017, though, following the publishing of the paper ‘Attention is All You Need’ by Google and the University of Toronto. The architecture of the transformer had arrived. The advancement in generative AI that we now witness was made possible by this new model. It can be efficiently scaled to employ multi-core GPUs, process input data in parallel while using much bigger training datasets, and, most importantly, learn to pay attention to the meaning of the words it’s processing. And all you require is attention, as the paper title says!</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai5.png" width="800"></p>
</section>
<section id="languge-models-after-transformers" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="languge-models-after-transformers"><span class="header-section-number">3</span> Languge Models after Transformers</h2>
<p>Large language models built using the transformer architecture performed significantly better on natural language tasks than the preceding generation of RNNs, which resulted in a huge increase in regeneration power. The transformer architecture’s strength comes in its capacity to comprehend the significance and context of each word in a phrase. Not simply to each word next to its neighbour as you can see below, but to every other word in the phrase. Applying attention weights to those associations will help the model understand how each word relates to the others, regardless of where it appears in the input.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai6.png" width="800"></p>
<p>This enables the algorithm to discover who owns the book, who might own the book, and whether the book is even pertinent to the document’s larger context using attention weights. These attention weights are acquired via LLM training. The attention weights between each word and each other can be demonstrated using this graphic, known as an attention map. You can see that the word book is closely related to or paying attention to the words student and teacher in this example. This process of learning a relationships between words throughout the entire input is known as <strong>self-attention</strong>, and it dramatically enhances the model’s capacity to encode language.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai7.png" width="800"></p>
<p>Let’s look at the transfomer model’s functionality at a high level. The transformer architecture is divided into two separate components: the encoder and the decoder. All of these parts cooperate with one another and have a lot in common. Also, keep in mind that the diagram you are looking at is a modification of the original paper; nothing else is required. The model’s inputs are at the bottom and its outputs are at the top. Machine-learning models today only function with numbers, not language, and are essentially just large statistical calculators.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai8.png" width="800"></p>
<section id="embedding-layers" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="embedding-layers"><span class="header-section-number">3.1</span> Embedding Layers</h3>
<p>So, you must tokenize the terms before giving them to the model for processing. To put it simply, this changes the words into numbers, with each number denoting a particular location in a dictionary of all the words that the model could be able to use. There are numerous tokenization techniques available. For instance, utilising token IDs to represent word fragments or matching two whole words. as demonstrated here. It’s crucial to utilise the same tokenizer when creating text after choosing one to use while training the model. After this step, you send your input to the <strong>embedding layer</strong> as it is represented as a set of numbers.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai9.png" width="800"></p>
<p>This layer is a high-dimensional, trainable vector embedding space where each token is represented as a vector and has a specific place in the space. The idea is that these multi-dimensional vectors, which are matched to each token ID in the vocabulary, gradually learn to capture the meaning and context of specific tokens in the input sequence. Natural language processing has long made use of embedding vector spaces; Word2vec and other older language algorithms do so.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai10.png" width="800"></p>
<p>Looking at the example sequence again, you can see that in this simple scenario, each word has been matched to a token ID, and each token has been mapped into a vector. The vector size in the original transformer paper was 512, which is far larger than what we can fit onto this graphic. For the sake of simplicity, if you consider a vector size of just three, you may plot the words into a three-dimensional space and observe the correlations between those words. Now that you’ve seen how to associate words that are close to one another in the embedding space and how to calculate the distance between the words as an angle, you can see how the model is able to comprehend language mathematically.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai11.png" width="800"></p>
<p><strong>Positional encoding</strong> is added at the same time as the token vectors are added to the encoder or decoder’s base. The model performs parallel processing on each input token. Therefore, by including positional encoding, you maintain the information about word order and ensure that the position of the word in the sentence remains relevant.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai12.png" width="800"></p>
</section>
<section id="self-attention-layers" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="self-attention-layers"><span class="header-section-number">3.2</span> Self-Attention Layers</h3>
<p>The <strong>self-attention layer</strong> receives the output vectors after the input tokens and positional encodings have been added up. The model examines the connections between the tokens in your input sequence in this case. As you just saw, this enables the model to focus on various elements of the input sequence in order to more accurately represent how the words are related contextually. Each word’s relevance to the other words in the input sequence is reflected in the self-attention weights that are learned during training and stored in these layers.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai13.png" width="800"></p>
<p>However, this doesn’t simply happen once; the transformer architecture actually includes multiple heads for self-attention.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai14.png" width="800"></p>
<p>As a result, numerous sets of self-attention weights or heads are simultaneously and independently learnt. Although the number of attention heads contained in the attention layer varies from model to model, it typically ranges between 12 and 100. The underlying assumption is that each self-attention head will pick up on a unique component of language. One head, for instance, would understand how the human entities in our statement relate to one another. While another person’s attention might be drawn to the sentence’s activities. Another mind might be more interested in other characteristics, like whether the words rhyme.</p>
<p>It’s significant to remember that you cannot decide in advance which linguistic concepts the attention heads will learn. Each head’s weights are randomly initialised, and given enough training data and time, they will each learn certain facets of language. While certain attention maps, are simple to understand, others might not be.</p>
</section>
<section id="feed-forward-layers-final-output" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="feed-forward-layers-final-output"><span class="header-section-number">3.3</span> Feed-Forward Layers &amp; Final Output</h3>
<p>Once all of the attention weights have been applied to the input data, a fully linked feed-forward network processes the output. A vector of logits proportional to the likelihood score for each and every token in the tokenizer dictionary is the layer’s output.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai15.png" width="800"></p>
<p>The final softmax layer can then receive these logits and normalise them to get a likelihood score for each word. There are probably hundreds of scores in this output because it includes probabilities for each word in the lexicon.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai16.png" width="800"></p>
<p>There will be one token with a greater score than the others, this will be the token that will be output next. The ultimate choice from this vector of probabilities can be changed, though, using a variety of techniques.</p>
</section>
</section>
<section id="acknowledgements" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="acknowledgements"><span class="header-section-number">4</span> Acknowledgements</h2>
<p>I’d like to express my thanks to the wonderful <a href="https://www.deeplearning.ai/courses/generative-ai-with-llms/">Generative AI with Large Language Models Course</a> by DeepLearning.ai and AWS - which i completed, and acknowledge the use of some images and other materials from the course in this article.</p>


</section>

<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css"><div id="mc_embed_signup">
    <form action="https://livingdatalab.us8.list-manage.com/subscribe/post?u=e2d57b0d6e43b4f6bff927a55&amp;id=a30bdff125&amp;f_id=009d05e0f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2 class="anchored">Subscribe</h2>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e2d57b0d6e43b4f6bff927a55_a30bdff125" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
            </div>
        </div>
    </div>
</form>
</div><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/http:\/\/livingdatalab\.com/);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">LivingDataLab Data Science &amp; AI Blog</div>
  </div>
</footer>



</body></html>