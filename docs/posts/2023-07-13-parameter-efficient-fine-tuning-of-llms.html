<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Pranath Fernando">
<meta name="dcterms.date" content="2023-07-13">
<meta name="description" content="Training large language models can be computationally and financially expensive. Parameter efficient fine tuning techniques only modify a restricted number of parameters and can result in drastically reduce costs and training time.">

<title>LivingDataLab - Parameter Efficient Fine-Tuning (PEFT) for Large Language Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-91568149-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<link rel="stylesheet" href="../css/styles.css">
<meta property="og:title" content="LivingDataLab - Parameter Efficient Fine-Tuning (PEFT) for Large Language Models">
<meta property="og:description" content="Training large language models can be computationally and financially expensive.">
<meta property="og:image" content="https://github.com/pranath/blog/raw/master/images/genai1.jpg">
<meta property="og:site-name" content="LivingDataLab">
<meta name="twitter:title" content="LivingDataLab - Parameter Efficient Fine-Tuning (PEFT) for Large Language Models">
<meta name="twitter:description" content="Training large language models can be computationally and financially expensive.">
<meta name="twitter:image" content="https://github.com/pranath/blog/raw/master/images/genai1.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">LivingDataLab</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../projects.html" aria-current="page">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/pranath-fernando/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/LivingDataLab"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pranath"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Parameter Efficient Fine-Tuning (PEFT) for Large Language Models</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Parameter Efficient Fine-Tuning (PEFT) for Large Language Models</h1>
                  <div>
        <div class="description">
          Training large language models can be computationally and financially expensive. Parameter efficient fine tuning techniques only modify a restricted number of parameters and can result in drastically reduce costs and training time.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">natural-language-processing</div>
                <div class="quarto-category">deep-learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Pranath Fernando </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 13, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects.html" class="sidebar-item-text sidebar-link">Projects Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Projects</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/doc-chat.html" class="sidebar-item-text sidebar-link">Document Chat</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/doc-summarisation.html" class="sidebar-item-text sidebar-link">Document Summarisation</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/web-page-chat.html" class="sidebar-item-text sidebar-link">Web Page Chat</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/web-page-summarisation.html" class="sidebar-item-text sidebar-link">Web Page Summarisation</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/youtube-chat.html" class="sidebar-item-text sidebar-link">YouTube Chat</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/youtube-summarisation.html" class="sidebar-item-text sidebar-link">YouTube Summarisation</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
    /* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
    <form action="https://livingdatalab.us8.list-manage.com/subscribe/post?u=e2d57b0d6e43b4f6bff927a55&amp;id=a30bdff125&amp;f_id=009d05e0f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2>Subscribe</h2>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e2d57b0d6e43b4f6bff927a55_a30bdff125" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
            </div>
        </div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script>
<!--End mc_embed_signup-->

</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#parameter-efficient-fine-tuning-peft" id="toc-parameter-efficient-fine-tuning-peft" class="nav-link" data-scroll-target="#parameter-efficient-fine-tuning-peft"><span class="toc-section-number">2</span>  Parameter Efficient Fine Tuning (PEFT)</a></li>
  <li><a href="#peft-method-1-lora" id="toc-peft-method-1-lora" class="nav-link" data-scroll-target="#peft-method-1-lora"><span class="toc-section-number">3</span>  PEFT Method 1: LoRA</a></li>
  <li><a href="#peft-method-2-soft-prompts" id="toc-peft-method-2-soft-prompts" class="nav-link" data-scroll-target="#peft-method-2-soft-prompts"><span class="toc-section-number">4</span>  PEFT Method 2: Soft Prompts</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements"><span class="toc-section-number">5</span>  Acknowledgements</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>It takes a lot of computation to train LLMs. Memory is needed for complete fine-tuning not just to store the model but also a number of other training-related factors. You must be able to allocate memory for optimizer states, gradients, forward activations, and temporary memory throughout the training process even if your computer can hold the model weights, which are currently on the order of hundreds of terabytes for the largest models. These extra parts may be many times bigger than the model and can easily outgrow the capabilities of consumer hardware.</p>
<p>Parameter efficient fine tuning techniques only modify a restricted number of parameters, as opposed to full fine-tuning, which modifies every model weight during supervised learning.</p>
</section>
<section id="parameter-efficient-fine-tuning-peft" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="parameter-efficient-fine-tuning-peft"><span class="header-section-number">2</span> Parameter Efficient Fine Tuning (PEFT)</h2>
<p>Some PEFT strategies freeze the majority of the model weights and concentrate on fine-tuning a portion of the already-existing model parameters, such as specific layers or components. Other methods just add a few new parameters or layers and fine-tune them, leaving the existing model weights untouched. Most of the LLM weights, if not all of them, are kept frozen with PEFT. As a result, there are far fewer trained parameters than there were in the original LLM. Occasionally, just 15–25% of the LLM weights from the beginning. As a result, the memory needs for training become much more manageable.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai119.png" width="800"></p>
<p>In reality, PEFT can frequently be completed on a single GPU. Additionally, PEFT is less vulnerable to the catastrophic forgetting issues of full fine-tuning because the original LLM is only marginally changed or left unchanged. Every task you train on generates a new version of the model after full fine-tuning. Because they are all the same size as the original model, if you are fine-tuning for several activities, it might become a costly storage issue. Let’s look at how PEFT can help to make things better. With parameter efficient fine-tuning, you train fewer weights overall, resulting in a considerably lower footprint overall—depending on the workload, this footprint can be as small as a few gigabytes.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai120.png" width="800"></p>
<p>The original LLM weights and the additional parameters are merged for inference. The original model may be efficiently adapted to many tasks since the PEFT weights are trained for each task and are simple to swap out for inference. For parameter efficient fine-tuning, there are a number of approaches that you may apply, but each has trade-offs in terms of parameter efficiency, memory efficiency, training speed, model quality, and inference costs. Let’s examine the three primary categories of PEFT approaches. Selective techniques focus on adjusting just a portion of the initial LLM parameters.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai121.png" width="800"></p>
<p>You can choose from a number of methods to determine the parameters you wish to alter. You can choose to train only a portion of the model, a set of layers, or even a single kind of parameter.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai122.png" width="800"></p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai123.png" width="800"></p>
<p>Researchers have discovered that there are noticeable trade-offs between parameter efficiency and computation efficiency, and that these approaches perform inconsistently. Although reparameterization techniques also use the original LLM parameters, they do so by generating fresh low rank transformations of the initial network weights.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai124.png" width="800"></p>
<p>LoRA is one such approach that is frequently employed. Last but not least, additive techniques do fine-tuning by leaving all of the initial LLM weights frozen and adding new trainable components. Two basic strategies are included here. Typically located in the encoder or decoder components following the attention or feed-forward layers, adapter methods extend the architecture of the model by adding new trainable layers.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai125.png" width="800"></p>
<p>On the other hand, soft prompt approaches maintain a fixed and frozen model architecture and concentrate on modifying the input to enhance performance. This can be accomplished by either maintaining the input constant and retraining the embedding weights, or by adding trainable parameters to the prompt embeddings.</p>
</section>
<section id="peft-method-1-lora" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="peft-method-1-lora"><span class="header-section-number">3</span> PEFT Method 1: LoRA</h2>
<p>Low-rank Adaptation, or LoRA for short, is a re-parameterization method for fine-tuning that is parameter-efficient. Let’s examine its operation. Here is the diagram of the transformer architecture. Tokens created from the input prompt are then transformed into embedding vectors and sent to the encoder and/or decoder sections of the transformer. There are two different types of neural networks—self-attention and feedforward networks—in both of these components. These networks’ weights are taught during the pre-training phase.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai126.png" width="800"></p>
<p>The self-attention layers receive the produced embedding vectors and use a number of weights to determine the attention scores. Every parameter in these levels is updated during thorough fine-tuning. By freezing all of the initial model parameters and then infusing two rank decomposition matrices with the original weights, the LoRA method lowers the number of parameters that must be trained during fine-tuning. The smaller matrices’ size are chosen so that the final matrix has the same dimensions as the weights they are changing.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai127.png" width="800"></p>
<p>The smaller matrices are trained using a supervised learning procedure while maintaining the original weights of the LLM. For inference, a matrix with the same dimensions as the frozen weights is produced by multiplying the two low-rank matrices together. The old weights are then combined with this, and the model is then updated with these new values. You now possess a LoRA model that has been optimised to perform your particular purpose. Inference latency is hardly affected because this model has the same amount of parameters as the original. In order to fine-tune for a task and improve performance, researchers have discovered that applying LoRA to just the self-attention layers of the model achieves good results.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai128.png" width="800"></p>
<p>However, in theory, LoRA can also be used on other parts, such as feed-forward layers. However, applying LoRA to these weight matrices results in the highest reductions in trainable parameters because the majority of the LLMs’ parameters are in the attention layers. The transformer architecture described in the Attention is All You Need paper will be used ifor an example to illustrate. The size of the transformer weights, according to the original paper, are 512 by 64. This indicates that there are 32,768 trainable parameters for each weights matrix. Alternatively, we would train two tiny rank decomposition matrices with an eight-dimensional short dimension if we utilise LoRA as a fine-tuning strategy.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai129.png" width="800"></p>
<p>This implies that Matrix A will have 512 total parameters and 8 by 64 dimensions. The size of Matrix B will be 512 by 8, or 4,096 trainable parameters. You can reduce the number of training parameters by 86% by updating the weights of these new low-rank matrices rather than the original weights. With LoRA, you can drastically minimise the amount of trainable parameters, thus you don’t always require a distributed cluster of GPUs to carry out this kind of parameter efficient fine tuning. Since each task may be fine-tuned using a separate set of rank-decomposition matrices, you can switch between them at inference time by adjusting the weights.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai130.png" width="800"></p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai131.png" width="800"></p>
<p>Let’s say you train two LoRA matrices for the purpose of performing Task A. You would combine both matrices together and then add the resulting matrix to the initial frozen weights to do inference on this task. Then, using this updated weights matrix, you can swap out the old weights wherever they occur in your model.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai132.png" width="800"></p>
<p>After that, you can perform inference on Task A using this model. Instead, if you want to complete a different task, let’s say Task B, you only need to take the LoRA matrices you trained for it, figure out what their product is, add it to the initial weights, and update the model once more. These LoRA matrices only need a tiny amount of memory to be stored.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai133.png" width="800"></p>
<p>So in theory, LoRA can be used to train for a variety of tasks. To avoid having to store numerous full-size versions of the LLM, simply swap out the weights as needed. How reliable are these models? Let’s use the ROUGE measure to assess how well a LoRA fine-tune model performs in comparison to both the original base model and a fully fine-tuned version. Let’s concentrate on optimising the FLAN-T5 for dialogue synthesis. Just to refresh your memory, a substantial instruction data set was used for the initial set of comprehensive fine-tuning on the FLAN-T5-base model.</p>
<p>For the FLAN-T5 base model and the summarization data set we previously described, let’s first establish a baseline score. The ROUGE scores for the base model are shown below, with higher values indicating greater performance. For this discussion, you should concentrate on the ROUGE 1 score, however you can compare any of these scores. The scores are fairly low as you can see. Check the results for a model that has had extra complete dialogue summarization fine-tuning. Remember that even though the FLAN-T5 is a good model, some additional task-specific fine-tuning may be beneficial. When using comprehensive fine-tuning, supervised learning is used to update every aspect of the model.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai134.png" width="800"></p>
<p>As you can see, this causes the ROUGE 1 score to increase significantly above the baseline FLAN-T5 model by 0.19. The model’s performance on the summarising task has significantly improved thanks to the second round of fine-tuning. Let’s now examine the results for the LoRA fine-tune model. As you can see, this procedure also significantly improved performance. From the starting point, the ROUGE 1 score has increased by 0.17. This is only little less than full fine-tuning. Nevertheless, employing LoRA for fine-tuning learned a lot fewer parameters than full fine-tuning using a lot less computing, so this minor performance trade-off may very well be worthwhile.</p>
<p>You may be asking how to select the LoRA matrices’ rank. It’s a good question, and the field of study is still active. In general, there are fewer trainable parameters and greater compute savings the lower the rank is. There are, however, a few model performance-related considerations to take into account. Researchers from Microsoft looked into how different rank selections affected the model’s performance on language generation tasks in the study that first introduced LoRA. The table here is a summary of the findings.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai135.png" width="800"></p>
<p>The table displays the ultimate loss value of the model, the rank of the LoRA matrices in the first column, and the scores for other metrics, including BLEU and ROUGE. The best results for each statistic are represented by the values in bold. For ranks higher than 16, the loss value reached a plateau, according to the authors. In other words, performance wasn’t enhanced by employing larger LoRA matrices. The lesson learned from this is that ranks between 4 and 32 can offer you a good trade-off between lowering trainable characteristics and maintaining performance.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai136.png" width="800"></p>
<p>As more practitioners employ LoRA, there may be an evolution in best practises for optimising the selection of rank. LoRA is an effective fine-tuning technique that produces excellent performance. The method’s guiding concepts apply to training models across domains, not just LLMs.</p>
</section>
<section id="peft-method-2-soft-prompts" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="peft-method-2-soft-prompts"><span class="header-section-number">4</span> PEFT Method 2: Soft Prompts</h2>
<p>By using LoRA, we were able to update the model’s weights effectively without having to retrain any of the parameters. PEFT also includes additive techniques that try to enhance model performance without altering the weights in any way. You’ll learn about soft prompts aka prompt tuning, a second technique for parameter-efficient fine tuning here. Though they sound similar, prompt tuning and prompt engineering are very distinct from one another. Prompt engineering is modifying the language of your prompt to get the desired completion. Changing the words or phrases you use could be as simple as that, or it could be more difficult like giving examples of one-shot or few-shot inference.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai137.png" width="800"></p>
<p>The objective is to aid the model’s comprehension of the nature of the work you are asking it to perform and to improve the completion. Prompt engineering has significant drawbacks, though, in that creating and testing various prompts can be labor-intensive. The length of the context window is another restriction, and sometimes you can still not get the performance you require for your operation using this approach.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai138.png" width="800"></p>
<p>Prompt tuning involves adding more trainable tokens to your prompt and letting the supervised learning procedure decide what their ideal values should be. A soft prompt is a collection of trainable tokens that is prepended to embedding vectors that reflect the text in your input.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai139.png" width="800"></p>
<p>The soft prompt vectors are the same size as the language token embedding vectors. And for good performance, between 20 and 100 virtual tokens may be sufficient. Since each token corresponds to a specific place in the embedding vector space, the tokens used to represent natural language are fixed to specific words. The soft prompts, on the other hand, are not set, definite terms of natural language. As an alternative, consider them to be virtual tokens that can have any value in the continuous multidimensional embedding space. The model also learns the values for these virtual tokens that maximise performance for a particular task using supervised learning.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai140.png" width="800"></p>
<p>The training data set includes input prompts and output completions or labels in complete fine tuning. During supervised learning, the large language model’s weights are updated. Contary to prompt tuning the large language model’s weights are fixed, and the underlying model is not changed. Instead, to improve the model’s completion of the prompt, the embedding vectors of the soft prompt are changed over time.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai141.png" width="800"></p>
<p>Given that only a small number of parameters are being leanred, prompt tuning is a relatively parameter-efficient method compared to the millions to billions of parameters used for full fine tuning, as we observed with LoRA.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai142.png" width="800"></p>
<p>For each job, you can train a separate set of soft prompts, and when it comes time for inference, you can switch them out. For one job, you can teach one set of soft prompts, and another set for a different task. To move to another task, you just modify the soft prompt. To utilise them for inference, you prepend your input prompt with the learnt tokens. Since soft prompts take up very little space on disc, this type of fine tweaking is very effective and versatile. You’ll see that the LLM is the same for all tasks; all you have to do is change the soft prompts when it comes time for inference. So how effective is prompt tuning? Brian Lester and colleagues at Google looked at this in the original paper.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai143.png" width="800"></p>
<p>For a variety of model sizes, the authors contrasted prompt tuning with a few alternative techniques. You can see the Model size on the X axis and the SuperGLUE score on the Y axis in this paper’s figure. This is the evaluation benchmark that grades use to evaluate performance on a variety of different language tasks, as you learned about earlier this week. The results of models that underwent exhaustive fine tuning on a single job are represented by the red line. The score for models developed utilising multitask fine tuning is represented by the orange line. The performance of prompt tuning is displayed on the green line, while only scores for prompt engineering are displayed on the blue line.</p>
<p>As you can see, prompt tuning is less effective for smaller LLMs than full fine tuning. However, prompt tuning’s effectiveness decreases with model size. Additionally, prompt tuning, which provides a considerable performance improvement over prompt engineering alone, can be just as successful as full fine tuning once models have around 10 billion parameters. The interpretability of learned virtual tokens is one potential problem to take into account. Please keep this in mind as the soft prompt tokens might have any value in the continuous embedding vector space. No known token, word, or phrase in the LLM’s vocabulary corresponds to the training tokens.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai144.png" width="800"></p>
<p>However, a closer look at the tokens that are closest to the soft prompt location reveals that they organise into compact semantic clusters. In other words, the words with meanings most comparable to the soft prompt tokens are nearest to them. The fact that the words are frequently tied to the activity in some way suggests that the prompts are teaching word-like representations. In this session, you looked at two PEFT techniques, including LoRA, which effectively updates the model parameters using rank decomposition matrices. Additionally, prompt tuning adds trainable tokens while leaving the model weights alone.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai145.png" width="800"></p>
<p>Both techniques let you fine-tune models with the ability to do your jobs more effectively while utilising a lot less computing power than full fine-tuning techniques. Due to its performance being on par with full fine tuning for a wide range of jobs and data sets, LoRA is widely employed in practise.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai146.png" width="800"></p>
</section>
<section id="acknowledgements" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="acknowledgements"><span class="header-section-number">5</span> Acknowledgements</h2>
<p>I’d like to express my thanks to the wonderful <a href="https://www.deeplearning.ai/courses/generative-ai-with-llms/">Generative AI with Large Language Models Course</a> by DeepLearning.ai and AWS - which i completed, and acknowledge the use of some images and other materials from the course in this article.</p>


</section>

<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css"><div id="mc_embed_signup">
    <form action="https://livingdatalab.us8.list-manage.com/subscribe/post?u=e2d57b0d6e43b4f6bff927a55&amp;id=a30bdff125&amp;f_id=009d05e0f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2 class="anchored">Subscribe</h2>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e2d57b0d6e43b4f6bff927a55_a30bdff125" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
            </div>
        </div>
    </div>
</form>
</div><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/http:\/\/livingdatalab\.com/);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">LivingDataLab Data Science &amp; AI Blog</div>
  </div>
</footer>



</body></html>