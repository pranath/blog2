<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Pranath Fernando">
<meta name="dcterms.date" content="2023-06-14">
<meta name="description" content="Creating useful applications with AI &amp; Large Language Models involves many aspects, here I highlight key considerations when building these applications &amp; describe how I built &amp; deployed 6 LLM applications with LangChain to summarise or chat with documents, web pages or youtube videos">

<title>LivingDataLab - Key Considerations when Creating Practical Applications using Large Language Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-91568149-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<link rel="stylesheet" href="../css/styles.css">
<meta property="og:title" content="LivingDataLab - Key Considerations when Creating Practical Applications using Large Language Models">
<meta property="og:description" content="Creating useful applications with AI &amp; Large Language Models involves many aspects, here I highlight key considerations when building these applications &amp; describe how I built &amp; deployed 6 LLM applications with LangChain to summarise or chat with documents, web pages or youtube videos">
<meta property="og:image" content="https://github.com/pranath/blog/raw/master/images/ai_applications.png">
<meta property="og:site-name" content="LivingDataLab">
<meta name="twitter:title" content="LivingDataLab - Key Considerations when Creating Practical Applications using Large Language Models">
<meta name="twitter:description" content="Creating useful applications with AI &amp; Large Language Models involves many aspects, here I highlight key considerations when building these applications &amp; describe how I built &amp; deployed 6 LLM applications with LangChain to summarise or chat with documents, web pages or youtube videos">
<meta name="twitter:image" content="https://github.com/pranath/blog/raw/master/images/ai_applications.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">LivingDataLab</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://thefuturai.substack.com/" rel="" target=""><i class="bi bi-substack" role="img">
</i> 
 <span class="menu-text">Newsletter</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../projects.html" rel="" target="" aria-current="page">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/pranath-fernando/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/LivingDataLab" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pranath" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Key Considerations when Creating Practical Applications using Large Language Models</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Key Considerations when Creating Practical Applications using Large Language Models</h1>
                  <div>
        <div class="description">
          Creating useful applications with AI &amp; Large Language Models involves many aspects, here I highlight key considerations when building these applications &amp; describe how I built &amp; deployed 6 LLM applications with LangChain to summarise or chat with documents, web pages or youtube videos
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">natural-language-processing</div>
                <div class="quarto-category">deep-learning</div>
                <div class="quarto-category">langchain</div>
                <div class="quarto-category">hugging-face</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Pranath Fernando </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 14, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projects Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Projects</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/doc-chat.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Document Chat</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/doc-summarisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Document Summarisation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/web-page-chat.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Web Page Chat</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/web-page-summarisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Web Page Summarisation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/youtube-chat.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">YouTube Chat</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/youtube-summarisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">YouTube Summarisation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<!-- Begin Mailchimp Signup Form -->
<a href="https://thefuturai.substack.com/"><h2>Subscribe</h2></a>
<!--End mc_embed_signup-->

</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#choosing-a-large-language-model" id="toc-choosing-a-large-language-model" class="nav-link" data-scroll-target="#choosing-a-large-language-model"><span class="header-section-number">2</span> Choosing a Large Language Model</a>
  <ul class="collapse">
  <li><a href="#which-llms-could-be-used" id="toc-which-llms-could-be-used" class="nav-link" data-scroll-target="#which-llms-could-be-used"><span class="header-section-number">2.1</span> Which LLM’s could be used ?</a></li>
  <li><a href="#which-llms-give-a-good-enough-response-or-better" id="toc-which-llms-give-a-good-enough-response-or-better" class="nav-link" data-scroll-target="#which-llms-give-a-good-enough-response-or-better"><span class="header-section-number">2.2</span> Which LLM’s give a ‘good enough’ response or better ?</a></li>
  <li><a href="#is-response-time-or-latency-important" id="toc-is-response-time-or-latency-important" class="nav-link" data-scroll-target="#is-response-time-or-latency-important"><span class="header-section-number">2.3</span> Is response time or latency important ?</a></li>
  <li><a href="#how-big-is-my-budget" id="toc-how-big-is-my-budget" class="nav-link" data-scroll-target="#how-big-is-my-budget"><span class="header-section-number">2.4</span> How big is my budget ?</a></li>
  <li><a href="#how-do-i-need-to-adapt-the-model" id="toc-how-do-i-need-to-adapt-the-model" class="nav-link" data-scroll-target="#how-do-i-need-to-adapt-the-model"><span class="header-section-number">2.5</span> How do I need to adapt the model ?</a></li>
  <li><a href="#am-i-dealing-with-sensitive-data" id="toc-am-i-dealing-with-sensitive-data" class="nav-link" data-scroll-target="#am-i-dealing-with-sensitive-data"><span class="header-section-number">2.6</span> Am I dealing with sensitive data ?</a></li>
  <li><a href="#am-i-comfortable-being-dependant-on-an-external-service" id="toc-am-i-comfortable-being-dependant-on-an-external-service" class="nav-link" data-scroll-target="#am-i-comfortable-being-dependant-on-an-external-service"><span class="header-section-number">2.7</span> Am I comfortable being dependant on an external service ?</a></li>
  <li><a href="#am-i-ok-maintaining-the-llm" id="toc-am-i-ok-maintaining-the-llm" class="nav-link" data-scroll-target="#am-i-ok-maintaining-the-llm"><span class="header-section-number">2.8</span> Am I ok maintaining the LLM ?</a></li>
  <li><a href="#so-which-llm-did-i-choose-and-why" id="toc-so-which-llm-did-i-choose-and-why" class="nav-link" data-scroll-target="#so-which-llm-did-i-choose-and-why"><span class="header-section-number">2.9</span> So which LLM did I choose and why?</a></li>
  </ul></li>
  <li><a href="#choosing-a-llm-framework" id="toc-choosing-a-llm-framework" class="nav-link" data-scroll-target="#choosing-a-llm-framework"><span class="header-section-number">3</span> Choosing a LLM framework</a></li>
  <li><a href="#choosing-a-web-application-framework" id="toc-choosing-a-web-application-framework" class="nav-link" data-scroll-target="#choosing-a-web-application-framework"><span class="header-section-number">4</span> Choosing a Web application framework</a></li>
  <li><a href="#choosing-a-hosting-solution" id="toc-choosing-a-hosting-solution" class="nav-link" data-scroll-target="#choosing-a-hosting-solution"><span class="header-section-number">5</span> Choosing a Hosting Solution</a></li>
  <li><a href="#key-python-modules-and-functions" id="toc-key-python-modules-and-functions" class="nav-link" data-scroll-target="#key-python-modules-and-functions"><span class="header-section-number">6</span> Key python modules and functions</a>
  <ul class="collapse">
  <li><a href="#streamlit-functions" id="toc-streamlit-functions" class="nav-link" data-scroll-target="#streamlit-functions"><span class="header-section-number">6.1</span> Streamlit functions</a></li>
  <li><a href="#langchain-functions" id="toc-langchain-functions" class="nav-link" data-scroll-target="#langchain-functions"><span class="header-section-number">6.2</span> Langchain functions</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">7</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>AI and Large Language Models such as ChatGPT have brought some dramatic developments in recent years. But trying to actually build useful applications using these involves many challenges and considerations. However, the potential number of useful applications using LLM’s is huge.</p>
<p>For example, using the LLM application framework <a href="https://python.langchain.com/en/latest/index.html">LangChain</a> which <a href="#category=langchain">I have been using for a little while now</a> provides ready to go built in templates for common useful LLM usecases:</p>
<ul>
<li><p><strong>Autonomous Agents:</strong> Autonomous agents are long-running agents that take many steps in an attempt to accomplish an objective. Examples include AutoGPT and BabyAGI.</p></li>
<li><p><strong>Agent Simulations:</strong> Putting agents in a sandbox and observing how they interact with each other and react to events can be an effective way to evaluate their long-range reasoning and planning abilities.</p></li>
<li><p><strong>Personal Assistants:</strong> One of the primary LangChain use cases. Personal assistants need to take actions, remember interactions, and have knowledge about your data.</p></li>
<li><p><strong>Question Answering:</strong> Another common LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.</p></li>
<li><p><strong>Chatbots:</strong> Language models love to chat, making this a very natural use of them.</p></li>
<li><p><strong>Querying Tabular Data:</strong> Recommended reading if you want to use language models to query structured data (CSVs, SQL, dataframes, etc).</p></li>
<li><p><strong>Code Understanding:</strong> Recommended reading if you want to use language models to analyze code.</p></li>
<li><p><strong>Interacting with APIs:</strong> Enabling language models to interact with APIs is extremely powerful. It gives them access to up-to-date information and allows them to take actions.</p></li>
<li><p><strong>Extraction:</strong> Extract structured information from text.</p></li>
<li><p><strong>Summarization:</strong> Compressing longer documents. A type of Data-Augmented Generation.</p></li>
<li><p><strong>Evaluation:</strong> Generative models are hard to evaluate with traditional metrics. One promising approach is to use language models themselves to do the evaluation.</p></li>
</ul>
<p>In this article I describe 6 new AI applications I have built using LangChain and explore the details of the choices I made in order to deploy them.</p>
<p>The 6 LLM Applications I have recently built are:</p>
<ul>
<li><strong>Document Summarisation:</strong> Write a summary of any PDF document</li>
<li><strong>Document Chat:</strong> Chat with a PDF document, ask any questions about its content</li>
<li><strong>Web Page Summarisation:</strong> Write a summary of any web page</li>
<li><strong>Web Page Chat:</strong> Chat with a web page, ask any questions about its content</li>
<li><strong>YouTube Summarisation:</strong> Write a summary of any YouTube video</li>
<li><strong>YouTube Chat:</strong> Chat with a YouTube video, ask any questions about its content</li>
</ul>
<p>These are all live, and you can try out any of these applications in my <a href="../projects.html">projects section</a>.</p>
<p>The code behind all of these apps can be found in this <a href="https://github.com/pranath/streamlit">github repo</a>.</p>
<p>I will now cover some of the key considerations that could be good to think about when building LLM applications.</p>
</section>
<section id="choosing-a-large-language-model" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="choosing-a-large-language-model"><span class="header-section-number">2</span> Choosing a Large Language Model</h2>
<p>Before we start using LLM’s we need to consider what applications they will be used for. There is a huge range of LLM’s beyond just ChatGPT, and they all have their own unique characteristics and differences than make them more or less suitable for particular tasks.</p>
<p>Two broad categories of LLM’s would be:</p>
<ul>
<li><strong>Paid for service LLMs:</strong> These include services like OpenAI’s <a href="https://openai.com/blog/chatgpt">ChatGPT</a> and others where to build a product using it you would need to pay per usage and you cannot see or directly access the LLM</li>
<li><strong>Open Source LLMs:</strong> The most well known of these would be <a href="https://huggingface.co/">HuggingFace</a> where to build an application you can freely download these LLM’s or use them hosted totally free of charge</li>
</ul>
<p>With this in mind though there are various key considerations that should guide our choice of LLM for an application or project.</p>
<section id="which-llms-could-be-used" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="which-llms-could-be-used"><span class="header-section-number">2.1</span> Which LLM’s could be used ?</h3>
<p>Not all LLM’s can be used for all applications. For example only certain LLM’s can be used to engage in conversational responses, or for text classification, as we can see from the model task types for <a href="https://huggingface.co/models">HuggingFace</a>. In another use case, we may find a great model that gives perfect responses, but is actually so big it can’t actually fit on any commercially available servers.</p>
</section>
<section id="which-llms-give-a-good-enough-response-or-better" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="which-llms-give-a-good-enough-response-or-better"><span class="header-section-number">2.2</span> Which LLM’s give a ‘good enough’ response or better ?</h3>
<p>Once we have narrowed down which LLM’s could be used, we need to consider which models give the minimum viable product response needed or better. While of course everyone would ideally like to use the model that gives the best responses, these models often have other disadvantages that could be important i.e.&nbsp;if it costs too much to use to make it viable. So its worth having a broad range of potential models one could consider from the minimum viable product response to the best, so we can consider various pros and cons beyond simply the model that gives the best outputs. However its also fair to say the generally bigger models seem to give better responses and smaller models.</p>
</section>
<section id="is-response-time-or-latency-important" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="is-response-time-or-latency-important"><span class="header-section-number">2.3</span> Is response time or latency important ?</h3>
<p>For some project use cases the LLM will only need to give a few responses, and where having a quick response without having to wait too long doesnt matter so much - for example doing a text summary perhaps. In other use cases having quicker responses might be more important - for example a customer service chatbot used by thousands of users simultaneously. In this use case it might be more important to get quicker responses to questions as well as being able to scale this and be able to deal with multiple requests for multiple people all at the same time and quickly. Bigger LLM’s generally tend to give slower responses than smaller ones.</p>
</section>
<section id="how-big-is-my-budget" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="how-big-is-my-budget"><span class="header-section-number">2.4</span> How big is my budget ?</h3>
<p>Do you have a significant budget or not much at all? this can also be quite an important consideration. While currently, its fair to say paid models like ChatGPT give some of the best responses, you do need to pay. While the cost for each query to the model is small, this can quickly rise if you’re model gets used a lot. Are you making an application not as a product but just to demonstrate general caperbilities? If its publically available but gets used a lot, potentially this could rack up significant cost. Perhaps a free open source model is the right choice here.</p>
<p>Are the responses of the paid ChatGPT only slightly better for a particular task than a free open source model e.g.&nbsp;for text summarisation? Maybe its worth choosing the free open source model for the cost savings that could then be better spent elsewhere.</p>
</section>
<section id="how-do-i-need-to-adapt-the-model" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="how-do-i-need-to-adapt-the-model"><span class="header-section-number">2.5</span> How do I need to adapt the model ?</h3>
<p>There are basically 4 ways we can use and customise a LLM for a specific task application.</p>
<ol type="1">
<li><strong>Use as is:</strong> Here we simply use the model as it is for our task, we don’t change how it responds to queries. <em>Cost:</em> Zero</li>
<li><strong>Prompt Engineering:</strong> Here we <a href="https://livingdatalab.com/posts/2023-05-02-iterative-prompt-development-for-large-language-models.html">iteratively develop our queries for the LLM</a> until we get the best responses possible for a given model. <em>Cost:</em> Minimal, as this involves more human effort to find the best queries/prompts to ask.</li>
<li><strong>Fine Tune:</strong> Here we do some limited further training to <a href="https://livingdatalab.com/posts/2023-04-23-fine-tuning-a-sentiment-analysis-model-with-huggingface.html">fine tune the model</a> using a specific new dataset to improve the responses for a given use case. <em>Cost:</em> Depends, but can start to accumulate from hundreds to thousands of dollars, depending on the model size, dataset etc.</li>
<li><strong>Train from scratch:</strong> Here we train the LLM from scratch on a given dataset. <em>Cost:</em> not on option for only a few companies given it can cost <a href="https://www.cnbc.com/2023/03/13/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html">millions of dollars</a> to train a large model from scratch</li>
</ol>
<p>So we can see that the cost can increase depending on which option we use, and of course each option will probably produce different degrees of quality of responses. Furthermore, certain models you might not even have some of these options. For example with paid for services like ChatGPT you can use option 1 &amp; 2 only, wheras with open source models you could do options 1-3.</p>
</section>
<section id="am-i-dealing-with-sensitive-data" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="am-i-dealing-with-sensitive-data"><span class="header-section-number">2.6</span> Am I dealing with sensitive data ?</h3>
<p>Are you dealing with sensitive data such as personal data? This could have an impact on the LLM you choose. For example, using a service like ChatGPT means you are sending all your requests and data to an external party like OpenAI - do you really know what they are doing with this data? Does that matter to you? Using an open source model for example one provided by <a href="https://huggingface.co/">HuggingFace</a> means you can download and run this model on your own servers, which means when you make requests to these models all the data stays secure and totally under your control not seen by any other parties.</p>
</section>
<section id="am-i-comfortable-being-dependant-on-an-external-service" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="am-i-comfortable-being-dependant-on-an-external-service"><span class="header-section-number">2.7</span> Am I comfortable being dependant on an external service ?</h3>
<p>Arguably, one of the best LLM’s available in terms of quality of outputs currently is still OpenAI’s ChatGPT, getting very good responses to your queries and automating significant tasks by using this can seem very seductive and persusaive from a business perspective. And yet, not only is this a paid service so if you build a product around this you are building in a cost to your product that scales with usage, you are also making your LLM application dependant on a third party service. What happens if OpenAI decides to withdraw or change the terms of its ChatGPT service? what happens it they start to increase the price of it?</p>
<p>This dependancy on an external service can create long term risks to your LLM application, so this is a trade off against other aspects worth serious considation when building an LLM based solution. Of course there might be various ways to choose the trade off beyond simply using a paid service like ChatGPT or simply using an open source model like HuggingFace. For example, you could use a hybrid approach: perhaps using open source LLM’s for tasks where there is less difference in quality of output to ChatGPT for example for sentiment analysis, and you could use ChatGPT for tasks where the quality of output is much better for ChatGPT for example a chatbot.</p>
<p>This way, you’re not entirely dependant on an external service for all your LLM applications, and your costs also do not scale so directly with LLM usage, and yet you can still get the best outputs where it matters most to you.</p>
</section>
<section id="am-i-ok-maintaining-the-llm" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="am-i-ok-maintaining-the-llm"><span class="header-section-number">2.8</span> Am I ok maintaining the LLM ?</h3>
<p>Using an open source LLM has many advantages, one disadvantage is you are more responsible for its maintenance and performance. You could of course use an open source model that is hosted on HuggingFace for free for example, but these are also used by many others, and the maintenance of these models and ther service hosted there is going to vary. Open source models hosted publically may break, or be overloaded with requests - they are free for everyone to use after all!</p>
<p>So for most serious business applications using open source models where you can have more reliable performance and control - a common approach is to download the LLM to your own servers, in which case you are then responsible for maintaining them, making sure they work etc. As they say, with great power and freedom comes great responsibility! Furthermore, when your responsible for maintaining your own models this way - its up to you to keep up to date with the latest developments.</p>
<p>Newer and better open source models are being developed at a rapid rate, choosing a good model for now is fine, but you probably want to be keeping upto date with the latest developments and potentially testing then adding new and better open source models for your use case. Thats quite a bit of work, and work you don’t really need to do if you are using an external service like ChatGPT - yes you pay for the service, but they do all the work to maintain the models, and update them with better models. So again this is an important trade off to consider.</p>
</section>
<section id="so-which-llm-did-i-choose-and-why" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="so-which-llm-did-i-choose-and-why"><span class="header-section-number">2.9</span> So which LLM did I choose and why?</h3>
<p>So for my 6 LLM applications I chose the HuggingFace <a href="https://huggingface.co/IAJw/declare-flan-alpaca-large-18378">declare-flan-alpaca-large-18378</a> LLM. My use case if you recall is to develop these 6 LLM applications:</p>
<ul>
<li><strong>Document Summarisation:</strong> Write a summary of any PDF document</li>
<li><strong>Document Chat:</strong> Chat with a PDF document, ask any questions about its content</li>
<li><strong>Web Page Summarisation:</strong> Write a summary of any web page</li>
<li><strong>Web Page Chat:</strong> Chat with a web page, ask any questions about its content</li>
<li><strong>YouTube Summarisation:</strong> Write a summary of any YouTube video</li>
<li><strong>YouTube Chat:</strong> Chat with a YouTube video, ask any questions about its content</li>
</ul>
<p>These are also not production applications, these are demonstration applications hosted on my data science blog - not likely to be used by thousands of users a day!</p>
<p>So these are the reasons I chose this model:</p>
<ul>
<li>The hugging face open source models while not having the best quality responses currently (ChatGPT would have better responses) are ‘good enough’ to demonstrate the types of functionality possible with LLM’s which is the main objective of these applications</li>
<li>The response time/latency of the models is not very important, again its not a commercial service, and I am not expecting thousands of users! The latency of these open source models is good enough</li>
<li>Using an open source model from hugging face means, while its not the best model available it costs me absolutely nothing. This means not only can I continue to build more and more of these demonstration apps without worrying about cost, in the unlikely event some of my LLM apps start to get very popular, i’m not going to get hit by huge costs that I would have by using ChatGPT.</li>
<li>This open source model is good enough to use for all my current 6 applications, and has been relatively easy to adapt for each use case with very minimal prompt engineering</li>
<li>There are no issues with sensitive data</li>
<li>I am comfortable with being dependant on the open source model being hosted on hugging face, even though it means i’m dependant on their service which many others are using so the response time varies, the convenience compared to setting up my own server to host and maintain is very helpful</li>
<li>I like supporting the open source movement where I can for many moral, practical and safety reasons. While some have expressed concern about AI and LLM’s, some have also argued one of the <a href="https://www.flyingpenguin.com/?p=47431">best ways to help with AI safety is by using open source models</a> which are by definition open to being tested, looked at, and scruitinised by anyone. This is different to closed source LLM’s such as by ironically named OpenAI who actually do not open up ChatGPT to independant scrutiny. <a href="https://www.lunasec.io/docs/blog/openai-not-so-open/">OpenAI and ChatGPT is not open source</a>. More widely <a href="https://www.mend.io/resources/blog/3-reasons-why-open-source-is-safer-than-commercial-software/">open source code and models</a> are generally considered by experts as much more reliable, better understood and safer because of this.</li>
</ul>
</section>
</section>
<section id="choosing-a-llm-framework" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="choosing-a-llm-framework"><span class="header-section-number">3</span> Choosing a LLM framework</h2>
<p>So we’ve chosen our LLM, what next? Thats a great step - but we are far from done. An LLM can only do so much by itself. The real usefulness and power of using LLM’s comes with combining it with other elements. These often include:</p>
<ul>
<li><strong>Some kind of memory</strong>: This could be to remember the history of everything that has been said with a chatbot conversation so the LLM can use this context to help answer questions</li>
<li><strong>A vector embedding database:</strong> While anyone can easily chat with ChatGPT, one of the things that starts to make LLM useful for business applications is the ability to give a context to the model so it can ask specific questions say about a specific document. Furthermore, LLM’s can only consider a limited size context, so for example not a big amount of text like a whole book. A vector embedding database is a way of solving both of these problems together, providing a way to help an LLM answer specific questions about some content that could be as big as you like, and do this by providing only the most relevant context to the model.</li>
<li><strong>The ability to connect with third party services:</strong> This might be services like a search engine, and api like twitter or google drive as alternative sources of text data.</li>
<li><strong>Prompt managment:</strong> Prompts (aka questions) are key to using LLM’s, but we then need to manage and automate these prompts to actually make useful applications</li>
</ul>
<p>Thats just a list of a few things needed, there are often many other things needed to make an LLM useful. This would mean writing quite a considerable amount of supporting code around the LLM, to actually turn it into a useful business appliction as I showed in a <a href="https://livingdatalab.com/posts/2023-06-01-using-langchain-for-llm-application-develoment.html#using-openai-without-langchain">previous article</a>.</p>
<p>But what if there was a ready built framework that does most of this for you and makes it much easier? thats what an LLM framework does. This is also very new concept, so there are not many of these frameworks out there, but probably one of the most popular LLM frameworks out there is <a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a>.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/langchain-overview.png" width="800"></p>
<p>The key modules of LangChain are (from least to most complex):</p>
<ul>
<li><p><strong>Models:</strong> Supported model types and integrations.</p></li>
<li><p><strong>Prompts:</strong> Prompt management, optimization, and serialization.</p></li>
<li><p><strong>Memory:</strong> Memory refers to state that is persisted between calls of a chain/agent.</p></li>
<li><p><strong>Indexes:</strong> Language models become much more powerful when combined with application-specific data - this module contains interfaces and integrations for loading, querying and updating external data.</p></li>
<li><p><strong>Chains:</strong> Chains are structured sequences of calls (to an LLM or to a different utility).</p></li>
<li><p><strong>Agents:</strong> An agent is a Chain in which an LLM, given a high-level directive and a set of tools, repeatedly decides an action, executes the action and observes the outcome until the high-level directive is complete.</p></li>
<li><p><strong>Callbacks:</strong> Callbacks let you log and stream the intermediate steps of any chain, making it easy to observe, debug, and evaluate the internals of an application.</p></li>
</ul>
<p><a href="https://livingdatalab.com/posts/2023-06-01-using-langchain-for-llm-application-develoment.html#using-openai-without-langchain">This earlier article</a> introduces LangChain in more detail.</p>
</section>
<section id="choosing-a-web-application-framework" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="choosing-a-web-application-framework"><span class="header-section-number">4</span> Choosing a Web application framework</h2>
<p>So now we need to think about how users are going to interact with your LLM application - are you building a Whatsapp Chatbot for example? In my case, i just want to provide an easy way anyone can try out chatting or summarising a document, web page or youtube video. For this I want to make a simple web application.</p>
<p>Before I was a Data Scientist, <a href="https://www.linkedin.com/in/pranath-fernando/">I was a Web Developer for many years</a>. So i’m no stranger to building custom websites from scratch of all kinds, and can happily write Html, Css, Javascript, Linux, PHP, MySQL and Java all day. But the thing is, I’m no longer a web developer, I’m a Data Scientist, so while I can do web development, its a considerable job in itself. I would much prefer to be spending my time learning and using the latest data science and AI techniques.</p>
<p>So what I need is a web application framework - something that will do most of the web dev work for me to give me a good basic web page, and something that lets me stay completely using my data science language of choice Python to also create my web application.</p>
<p>There are several python web application frameworks out there including Flask, Dash, Django and many others. However one relatively new python web application framework I’ve become aware of recently is <a href="https://streamlit.io/">Streamlit</a>, and interestingly this is a popular choice of people who are also building LLM applcations.</p>
<p>Some of the reasons streamlit <a href="https://blog.streamlit.io/generative-ai-and-streamlit-a-perfect-match/">can be a good web application framework for LLM applications</a> include:</p>
<ul>
<li>Good support for LLM application modules such as LangChain, HuggingFace etc</li>
<li>Great feature support for LLM type web components such as chat interfaces and inputs and responses</li>
</ul>
<p>This is why I chose to use streamlit, and i certainly found it a joy to use - to very easily and quickly create my LLM applications.</p>
<p>You can see my streamlit code for these in this <a href="https://github.com/pranath/streamlit">github repo</a> see my live LLM streamlit applications <a href="../projects.html">here</a>.</p>
</section>
<section id="choosing-a-hosting-solution" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="choosing-a-hosting-solution"><span class="header-section-number">5</span> Choosing a Hosting Solution</h2>
<p>Finally we need to find a home for our LLM application, we need a hosting solution. If we were creating a high usage customer facing commercial LLM application, we would probably want to be considering a cloud hosting solution such as DataBricks or <a href="../#category=aws">AWS</a>.</p>
<p>In particular, AWS seems to have done <a href="https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/">considerable work</a> writing <a href="https://aws.amazon.com/blogs/machine-learning/build-a-powerful-question-answering-bot-with-amazon-sagemaker-amazon-opensearch-service-streamlit-and-langchain/">articles and developing solutions</a> for <a href="https://aws.amazon.com/blogs/machine-learning/exploring-generative-ai-in-conversational-experiences-an-introduction-with-amazon-lex-langchain-and-sagemaker-jumpstart/">LLM application development use cases that also use LangChain</a>, and would probably by my LLM application cloud hosting solution of choice.</p>
<p>However, my use case is just to create some demo LLM applications. Heroku has been previously a popular service for demo applications, which I have used myself in <a href="https://github.com/pranath/disaster_response">earlier projects a few years ago</a>. However Heroku has become far less attractive for people wanting to create demo applications since they <a href="https://www.techtarget.com/searchsoftwarequality/news/252524336/Heroku-to-end-free-tiers-creating-platform-void-for-devs">removed their free tier last year</a>.</p>
<p>Here again, the shiny new kid on the block Streamlit comes to the rescue with a new service <a href="https://streamlit.io/cloud">Streamlit Community Cloud</a> which allows you to host unlimited web applications for free! Small caveats: the unlimited web applications need to be from files hosted in a public github repo, and the virtual machines used to host your applications have a reasonable but limited spec.</p>
<p>But this seems like a small price to pay for building unlimited free demo LLM applications. I found this service extremely easy to use, and plan to use this for hosting future LLM demo applications.</p>
</section>
<section id="key-python-modules-and-functions" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="key-python-modules-and-functions"><span class="header-section-number">6</span> Key python modules and functions</h2>
<p>I’d like to end by explaining some of the key python modules and functions I used to build these LLM applications. Some of these are very new state of the art python tools that provide powerful functionality to make building LLM applications quicker, easier, more robust and more powerful. The code for these applications can be found in this <a href="https://github.com/pranath/streamlit/">github repo</a>.</p>
<section id="streamlit-functions" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="streamlit-functions"><span class="header-section-number">6.1</span> Streamlit functions</h3>
<p>Streamlit is as you recall our web application framework. The streamlit documentation show how easy it is to use for example <a href="https://docs.streamlit.io/library/get-started/create-an-app">showing you how you can effectively create a classic basic ‘hello world’ one page website with just one line of code</a> which i’d say is pretty neat.</p>
<p>The <strong>set_page_config()</strong> function was particularly easy to use in allowing me to customise the default web page menu, as was <strong>file_uploader()</strong> in generating a web interface for uploading documents.</p>
</section>
<section id="langchain-functions" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="langchain-functions"><span class="header-section-number">6.2</span> Langchain functions</h3>
<p>Langchain is of course our LLM application framework, and the heart of this application.</p>
<p>A common use case for LLM applications involves using a specific context to answer questions like a document, as mine do. As mentioned earlier, LLM’s can only consider a limited size context. So for example not a big amount of text like a whole book. A vector embedding database is a way of solving both of these problems together, providing a way to help an LLM answer specific questions about some content that could be as big as you like, and do this by providing only the most relevant context to the model.</p>
<p>More details on embeddings and vector databases for LLMs can be found in <a href="../posts/2023-06-04-question-answering-over-documents-with-langchain.html#llms-on-documents">this earlier article</a>.</p>
<p>LangChain offers many different options for creating vector stores aka embedding databases. <a href="https://faiss.ai/">FAISS</a> is a embedding vector store created by FaceBook that you can use within LangChain. I found it very good for helping me pull the relevant parts of the context (document, youtube video transcript or web page text) to send to the LLM with either a question or a request for a summary. I used the <em><a href="https://python.langchain.com/docs/modules/data_connection/vectorstores/">similarity_search()</a></em> function from FAISS to actually pull the most relevant parts of the text to the query from the vector store.</p>
<p>For the queries to the language model to summarise text, I used <em><a href="https://python.langchain.com/docs/use_cases/summarization.html">load_summarize_chain()</a></em> which has a nice default prompt for summarising text that works well with many LLMs. Given an LLM can’t actually take a whole large text in one go to summarise, this chain bascially takes one chunk of text at a time, and sumamrises that. Then once all the chunks of text are sumamrised, it then runs the chain again on those summaries, until you end up with your final summary.</p>
<p>For the queries to the language model to answer questions about some text I used <em><a href="https://python.langchain.com/docs/use_cases/question_answering/">load_qa_chain()</a></em>. This function passes the user question to the LLM along with the reteived context from the FAISS vector store, and returns the LLM answer.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">7</span> Conclusion</h2>
<p>In this article we have looked at key considerations you should think about when building LLM applications. I described how I built &amp; deployed 6 LLM applications with LangChain to summarise or chat with documents, web pages or youtube videos, as well as covering other options available when building your own LLM applications.</p>


</section>

<a href="https://thefuturai.substack.com/"><h2 class="anchored">Subscribe</h2></a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("http:\/\/livingdatalab\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">LivingDataLab AI Technical Blog</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>