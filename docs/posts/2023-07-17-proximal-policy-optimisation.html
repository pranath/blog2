<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Pranath Fernando">
<meta name="dcterms.date" content="2023-07-17">
<meta name="description" content="In this post we will look at Proximal Policy Optimization which is a powerful algorithm for solving reinforcement learning problems">

<title>LivingDataLab - Reinforcement learning from human feedback (RLHF) using Proximal Policy Optimisation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-91568149-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<link rel="stylesheet" href="../css/styles.css">
<meta property="og:title" content="LivingDataLab - Reinforcement learning from human feedback (RLHF) using Proximal Policy Optimisation">
<meta property="og:description" content="In this post we will look at Proximal Policy Optimization which is a powerful algorithm for solving reinforcement learning problems">
<meta property="og:image" content="https://github.com/pranath/blog/raw/master/images/genai1.jpg">
<meta property="og:site-name" content="LivingDataLab">
<meta name="twitter:title" content="LivingDataLab - Reinforcement learning from human feedback (RLHF) using Proximal Policy Optimisation">
<meta name="twitter:description" content="In this post we will look at Proximal Policy Optimization which is a powerful algorithm for solving reinforcement learning problems">
<meta name="twitter:image" content="https://github.com/pranath/blog/raw/master/images/genai1.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">LivingDataLab</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://thefuturai.substack.com/" rel="" target=""><i class="bi bi-substack" role="img">
</i> 
 <span class="menu-text">Newsletter</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../projects.html" rel="" target="" aria-current="page">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/pranath-fernando/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/LivingDataLab" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pranath" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Reinforcement learning from human feedback (RLHF) using Proximal Policy Optimisation</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement learning from human feedback (RLHF) using Proximal Policy Optimisation</h1>
                  <div>
        <div class="description">
          In this post we will look at Proximal Policy Optimization which is a powerful algorithm for solving reinforcement learning problems
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">natural-language-processing</div>
                <div class="quarto-category">deep-learning</div>
                <div class="quarto-category">fine-tuning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Pranath Fernando </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 17, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projects Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Projects</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/doc-chat.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Document Chat</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/doc-summarisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Document Summarisation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/web-page-chat.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Web Page Chat</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/web-page-summarisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Web Page Summarisation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/youtube-chat.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">YouTube Chat</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../projects/youtube-summarisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">YouTube Summarisation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<!-- Begin Mailchimp Signup Form -->
<a href="https://thefuturai.substack.com/"><h2>Subscribe</h2></a>
<!--End mc_embed_signup-->

</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#proximal-policy-optimisation-ppo" id="toc-proximal-policy-optimisation-ppo" class="nav-link" data-scroll-target="#proximal-policy-optimisation-ppo"><span class="header-section-number">2</span> Proximal Policy Optimisation (PPO)</a>
  <ul class="collapse">
  <li><a href="#phase-1" id="toc-phase-1" class="nav-link" data-scroll-target="#phase-1"><span class="header-section-number">2.1</span> Phase 1</a></li>
  <li><a href="#phase-2" id="toc-phase-2" class="nav-link" data-scroll-target="#phase-2"><span class="header-section-number">2.2</span> Phase 2</a></li>
  <li><a href="#iterate-to-produce-human-aligned-llm" id="toc-iterate-to-produce-human-aligned-llm" class="nav-link" data-scroll-target="#iterate-to-produce-human-aligned-llm"><span class="header-section-number">2.3</span> Iterate to produce Human-Aligned LLM</a></li>
  </ul></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements"><span class="header-section-number">3</span> Acknowledgements</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>In an <a href="https://livingdatalab.com/posts/2023-07-15-reinforcement-learning-with-human-feedback-1.html">earlier articles</a> we introduced Reinforcement learning from human feedback (RLHF) which is an important method used in modern large language models to help improve the performance and alignment of large language models.</p>
<p>In this post we will look at Proximal Policy Optimization which is a powerful algorithm for solving reinforcement learning problems.</p>
</section>
<section id="proximal-policy-optimisation-ppo" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="proximal-policy-optimisation-ppo"><span class="header-section-number">2</span> Proximal Policy Optimisation (PPO)</h2>
<p>Proximal Policy Optimisation, or PPO, is a powerful approach for dealing with reinforcement learning issues. As the name implies, PPO tweaks a policy - in this example, the LLM: to better suit people’s preferences. The LLM is updated by PPO over a number of iterations. Proximal Policy Optimisation gets its name from the fact that the modifications are small and contained inside a restricted region, producing an updated LLM that is nearly identical to the original. A more stable learning occurs when the changes are contained inside this constrained area. The policy should be updated in order to maximise the incentive.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai188.png" width="800"></p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai189.png" width="800"></p>
<section id="phase-1" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="phase-1"><span class="header-section-number">2.1</span> Phase 1</h3>
<p>Through the value function, a different LLM head, we estimate this amount. Let’s examine the value function and the value loss in more detail. Assume that several prompts are provided. The reward for prompt completions is determined using the reward model once the LLM responses to the prompts are first generated.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai190.png" width="800"></p>
<p>For instance, the incentive for the first prompt completion shown above might be 1.87. The award for the following person may be -1.24, and so forth. There are a number of prompt completions that come with associated incentives. For a specific State S, the value function calculates the anticipated overall reward.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai191.png" width="800"></p>
<p>In other words, you want to estimate the entire future reward based on the present sequence of tokens as the LLM generates each completion’s token. Consider this a baseline from which to compare the quality of completions to your alignment requirements. Let’s assume that the expected future total prize is 0.34 at this point in the process of completion.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai192.png" width="800"></p>
<p>The predicted future total reward rises to 1.23 for the token that is generated after that. The objective is to reduce the value loss, which is the difference between the hypothetical future total reward of 1.87 and its approximation to the value function of 1.23, as measured by this example. Estimates for potential rewards are more accurate because of the value loss.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai193.png" width="800"></p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai194.png" width="800"></p>
<p>The advantage estimation process in Phase 2, which we shall explain shortly, uses the value function after that. This is comparable to when you begin writing a piece and have a general concept of how it will turn out even before you start writing it. You indicated that the weights are updated in Phase 2 using the losses and rewards established in Phase 1 to produce an updated LLM.</p>
</section>
<section id="phase-2" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="phase-2"><span class="header-section-number">2.2</span> Phase 2</h3>
<p>In Phase 2, you make a few small changes to the model and assess how those changes affect your model’s alignment objective. The prompt completion, losses, and incentives serve as a reference for updating the model weights. Additionally, PPO makes sure to maintain model updates within a certain small region called the trust region.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai195.png" width="800"></p>
<p>The proximal component of PPO is used in this situation. This succession of minor adjustments ought to steer the model in the direction of more returns. The primary component of this strategy is the PPO policy objective. Keep in mind that the goal is to identify a policy with a high expected benefit. In other words, you are attempting to modify the LLM weights to produce completions that are more in line with human tastes and, hence, yield higher rewards. The PPO algorithm’s primary goal during training is to minimise the policy loss.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai196.png" width="800"></p>
<p>Although the maths of this equation seems difficult, it is actually quite straightforward. Let’s dissect it step by step. Prioritise the most crucial phrase for the time being and disregard the others. Pi of A_t given S_t is the likelihood that the next token A_t will appear given the current prompt S_t in this instance of an LLM. The state S_t is the completed prompt up to the token t, and the action A_t is the subsequent token. The likelihood that the following token will use the first, frozen version of the LLM is the denominator. Through the updated LLM, which we may modify for a greater payout, the numerator represents the odds of the subsequent token. The estimated advantage term of a specific course of action is denoted by the symbol A-hat_t.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai197.png" width="800"></p>
<p>The benefit term calculates how much better or worse the current action is in comparison to all other actions that could be taken at the present data condition. We estimate how favourable this completion is compared to the others by taking a look at the anticipated future rewards of a completion that comes after the new token. This amount can be estimated using a recursive calculation based on the value function we previously mentioned. Here, we emphasise intuitive comprehension. The figure’s several paths in the coloured chart below serve as examples of the various ways you can fulfil the question S. The advantage term reveals how superior or inferior the current token A_t is in comparison to all other tokens.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai198.png" width="800"></p>
<p>The top path in this visualisation leads to better completion and a higher reward. The worst completion is the bottom path, which descends.</p>
<p>Why, then, does extending this duration result in greater rewards? Let’s think about the scenario where the recommended token has a benefit. A benefit indicates how much better than average the suggested token is. As a result, raising the likelihood of the present token seems like a wise move that produces greater benefits. This translates to making the most of this expression. The benefit will be negative if the proposed token performs worse than average. Once more, depromoting the token by maximising the expression is the right course of action. In light of this, the general conclusion is that increasing this expression leads to a more aligned LLM.</p>
<p>Therefore, let’s simply maximise this expression. Since our computations are accurate under the supposition that our benefit estimations are reliable, directly maximising the expression might result in issues. Only when the old and new policies are closely related to one another are the advantage estimations accurate. The remaining terms are used in this context. In this case, choosing the smaller of the two terms is what happens after taking a step back and reviewing the entire equation. both the first modified version we just talked about and this second one. It’s important to note that this second expression designates a location where two policies are close to one another.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai199.png" width="800"></p>
<p>These additional words serve as guardrails, identifying a zone close to the LLM where our predictions have minimal inaccuracy. The trust region is where this occurs. We are unlikely to depart from the trust area thanks to these additional conditions. The PPO policy aim can be optimised to produce a superior LLM without overshooting into unreliable zones.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai200.png" width="800"></p>
<p>In addition, there is entropy loss. Entropy permits the model to keep innovation as the policy loss drives it towards the alignment objective. If entropy was kept to a minimum, you might end up constantly answering the prompt in the manner that is illustrated above. The LLM is guided towards more creativity by higher entropy. This is similar to the temperature setting of LLM.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai201.png" width="800"></p>
<p>The difference is that although entropy influences model creativity during training, temperature influences model creativity at the time of inference. We obtain our PPO objective by weighing the sum of all words, which updates the model steadily in the direction of human preference. The main PPO goal is to achieve this. These coefficients, C1 and C2, are hyperparameters. The PPO goal does back propagation across a number of steps to update the model weights. PPO begins a new cycle after updating the model weights.</p>
</section>
<section id="iterate-to-produce-human-aligned-llm" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="iterate-to-produce-human-aligned-llm"><span class="header-section-number">2.3</span> Iterate to produce Human-Aligned LLM</h3>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai202.png" width="800"></p>
<p>The revised LLM is used for the subsequent iteration, and a fresh PPO cycle is initiated. You eventually reach the LLM that is human-aligned after several cycles. Are there any alternative RLHF reinforcement learning techniques? Yes.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai203.png" width="800"></p>
<p>For instance, Q-learning is a different approach for optimising LLMs through RL, but PPO is currently the most common approach. PPO, is well-liked because it strikes the ideal mix between complexity and effectiveness. In spite of this, there is ongoing study into optimising LLMs through feedback from humans or artificial intelligence.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/gai204.png" width="800"></p>
<p>In the near future, there will likely be a lot more developments in this field. As an easier substitute for RLHF, Stanford researchers recently published a paper describing a method termed direct preference optimisation. It will take more research to fully comprehend the advantages of new approaches like these, but this is a really intriguing field of study.</p>
</section>
</section>
<section id="acknowledgements" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="acknowledgements"><span class="header-section-number">3</span> Acknowledgements</h2>
<p>I’d like to express my thanks to the wonderful <a href="https://www.deeplearning.ai/courses/generative-ai-with-llms/">Generative AI with Large Language Models Course</a> by DeepLearning.ai and AWS - which i completed, and acknowledge the use of some images and other materials from the course in this article.</p>


</section>

<a href="https://thefuturai.substack.com/"><h2 class="anchored">Subscribe</h2></a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("http:\/\/livingdatalab\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">LivingDataLab AI Technical Blog</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>