<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Pranath Fernando">
<meta name="dcterms.date" content="2021-06-11">
<meta name="description" content="In this article we are going to look at building a convolutional neural network from scratch, using Pytorch as well as one-cycle training and batch normalisation.">

<title>LivingDataLab - Building a Convolutional Image Model from scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">LivingDataLab</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pranath"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/LivingDataLab"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Building a Convolutional Image Model from scratch</h1>
                  <div>
        <div class="description">
          In this article we are going to look at building a convolutional neural network from scratch, using Pytorch as well as one-cycle training and batch normalisation.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep-learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Pranath Fernando </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 11, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#convolutions-in-pytorch" id="toc-convolutions-in-pytorch" class="nav-link" data-scroll-target="#convolutions-in-pytorch"><span class="toc-section-number">2</span>  Convolutions in PyTorch</a></li>
  <li><a href="#model-1---a-basic-convolutional-neural-network-to-predict-2-digits" id="toc-model-1---a-basic-convolutional-neural-network-to-predict-2-digits" class="nav-link" data-scroll-target="#model-1---a-basic-convolutional-neural-network-to-predict-2-digits"><span class="toc-section-number">3</span>  Model 1 - A basic Convolutional Neural Network to predict 2 digits</a></li>
  <li><a href="#convolutional-arithmetic" id="toc-convolutional-arithmetic" class="nav-link" data-scroll-target="#convolutional-arithmetic"><span class="toc-section-number">4</span>  Convolutional arithmetic</a></li>
  <li><a href="#model-2---a-convolutional-neural-network-to-predict-10-digits" id="toc-model-2---a-convolutional-neural-network-to-predict-10-digits" class="nav-link" data-scroll-target="#model-2---a-convolutional-neural-network-to-predict-10-digits"><span class="toc-section-number">5</span>  Model 2 - A Convolutional Neural Network to predict 10 digits</a></li>
  <li><a href="#one-cycle-training" id="toc-one-cycle-training" class="nav-link" data-scroll-target="#one-cycle-training"><span class="toc-section-number">6</span>  One cycle training</a></li>
  <li><a href="#batch-normalisation" id="toc-batch-normalisation" class="nav-link" data-scroll-target="#batch-normalisation"><span class="toc-section-number">7</span>  Batch Normalisation</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">8</span>  Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>In this article we are going to look at building a convolutional neural network from scratch, using Pytorch. We are also going to look at other techniques than help train models better, such as one-cycle training and batch normalisation.</p>
<p>This article is based on the content from the <a href="https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb">fastai deep learning course chapter 13</a>.</p>
</section>
<section id="convolutions-in-pytorch" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="convolutions-in-pytorch"><span class="header-section-number">2</span> Convolutions in PyTorch</h2>
<p>Pytorch defines a convolutional layer using the method F.conv2d. This uses two rank 4 tensors.</p>
<ul>
<li>Input tensor (minibatch, in_channels, iH, iW)</li>
<li>Weight tensor (out_channels, in_channels, kH, kW)</li>
</ul>
<p>Where iH, iW, kH, kW are the image and kernal widths and heights respectively. Pytorch expects rank 4 tensors as it will process an entire mini-batch of images in one go, as well as applying multiple kernals in one go - which is more efficient to do on a GPU.</p>
<p>Lets try this out by creating a tensor of multiple kernals.</p>
<div class="cell" data-outputid="4d0aefe0-f027-46c8-a860-55ece3f6a61b" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>top_edge <span class="op">=</span> tensor([[<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                   [ <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                   [ <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]]).<span class="bu">float</span>()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>left_edge <span class="op">=</span> tensor([[<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                    [<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                    [<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>]]).<span class="bu">float</span>()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>diag1_edge <span class="op">=</span> tensor([[ <span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                     [<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                     [ <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>]]).<span class="bu">float</span>()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>diag2_edge <span class="op">=</span> tensor([[ <span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                     [ <span class="dv">0</span>, <span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                     [ <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]]).<span class="bu">float</span>()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>edge_kernels <span class="op">=</span> torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>edge_kernels.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>torch.Size([4, 3, 3])</code></pre>
</div>
</div>
<p>We can also create a data loader, and extract one minibatch to test.</p>
<div class="cell" data-outputid="5aa8390e-447a-4a3c-9605-8be08b5cc341" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> DataBlock((ImageBlock(cls<span class="op">=</span>PILImageBW), CategoryBlock), </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                  get_items<span class="op">=</span>get_image_files, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                  splitter<span class="op">=</span>GrandparentSplitter(),</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                  get_y<span class="op">=</span>parent_label)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> mnist.dataloaders(path)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> first(dls.valid)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> to_cpu(xb),to_cpu(yb)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>xb.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>torch.Size([64, 1, 28, 28])</code></pre>
</div>
</div>
<p>So we are not quite there, because currently our composite kernal is a rank 3 tensor, and it needs to be rank 4. So in this case we need to define an axis for the number of input channels which will be one (because our greyscale images have one channel). So we can insert an extra axis of 1 in the right place using the <strong>unsqueeze</strong> method.</p>
<div class="cell" data-outputid="3832563f-d69a-409d-9544-d2710d7e45ef" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>edge_kernels.shape,edge_kernels.unsqueeze(<span class="dv">1</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3]))</code></pre>
</div>
</div>
<p>We can now pass the kernals to the convolutional layer along with the data and process the image.</p>
<div class="cell" data-outputid="503a9353-dd3e-4d7b-ba0d-af29a7f14e2d" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>edge_kernels <span class="op">=</span> edge_kernels.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>batch_features <span class="op">=</span> F.conv2d(xb, edge_kernels)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>batch_features.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>torch.Size([64, 4, 26, 26])</code></pre>
</div>
</div>
<p>This gives us a tensor of a batch of 64 images, with 4 kernals and 26x26 image (we lost one pixel from each side by convolutions of 28x28). Lets look at one of the images on one channel to see the applied convolution.</p>
<div class="cell" data-outputid="2f271ac6-4e6c-404e-f5ea-64c0ce5b99f5" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>show_image(batch_features[<span class="dv">0</span>,<span class="dv">0</span>])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2021-06-11-convolutional-image-model-from-scratch_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So with pure convolutions we lost parts of the image, which become a bit smaller. We can get around this by using padding. We can also use stride to end up with a smaller sampled image.</p>
</section>
<section id="model-1---a-basic-convolutional-neural-network-to-predict-2-digits" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="model-1---a-basic-convolutional-neural-network-to-predict-2-digits"><span class="header-section-number">3</span> Model 1 - A basic Convolutional Neural Network to predict 2 digits</h2>
<p>We are going to build a simple model to predict 2 digits a 3 or 7, as a multi-class classification problem so we will expect probabilities for each image for the likelihood of it being either 3 or 7.</p>
<p>So we can use gradient descent to actually learn the best values for each of the kernals.</p>
<p>nn.Conv2d is a better method to use when creating a network as it automatically creates a weight matrix. Lets try a very simple model.</p>
<div class="cell" data-outputid="ff95825f-db0e-48ba-f1ae-dbbeb6ee4649" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>broken_cnn <span class="op">=</span> sequential(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(<span class="dv">1</span>,<span class="dv">30</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(<span class="dv">30</span>,<span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>broken_cnn(xb).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>torch.Size([64, 1, 28, 28])</code></pre>
</div>
</div>
<p>Note we didn’t need to specify an input size as we do with normal linear layers, as a convolution is applied to every pixel whatever the size of the image. The weights of a convolutional layer are to do with the number of input and output channels and the kernal size.</p>
<p>Putting our test batch through this, we can see it produces an output of 28x28 activations which is not ideal for classification. We could use a series of layers with strides, to reduce down the output activations.</p>
<div class="cell" data-outputid="bf13e024-d8a1-4be4-e2e7-9282c245f428" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Stride 2 convolutional layer which will downsample our image</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, act<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> nn.Conv2d(ni, nf, stride<span class="op">=</span><span class="dv">2</span>, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: res <span class="op">=</span> nn.Sequential(res, nn.ReLU())</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>simple_cnn <span class="op">=</span> sequential(</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    conv(<span class="dv">1</span> ,<span class="dv">4</span>),            <span class="co">#14x14</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    conv(<span class="dv">4</span> ,<span class="dv">8</span>),            <span class="co">#7x7</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    conv(<span class="dv">8</span> ,<span class="dv">16</span>),           <span class="co">#4x4</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    conv(<span class="dv">16</span>,<span class="dv">32</span>),           <span class="co">#2x2</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    conv(<span class="dv">32</span>,<span class="dv">2</span>, act<span class="op">=</span><span class="va">False</span>), <span class="co">#1x1</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    Flatten(),</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>simple_cnn(xb).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>torch.Size([64, 2])</code></pre>
</div>
</div>
<p>Create a learner object with this.</p>
<div class="cell" data-outputid="f2525c8e-9caa-44f1-a0c9-a0daaf108ae1" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, simple_cnn, loss_func<span class="op">=</span>F.cross_entropy, metrics<span class="op">=</span>accuracy)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>learn.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>Sequential (Input shape: 64)
============================================================================
Layer (type)         Output Shape         Param #    Trainable 
============================================================================
                     64 x 4 x 14 x 14    
Conv2d                                    40         True      
ReLU                                                           
____________________________________________________________________________
                     64 x 8 x 7 x 7      
Conv2d                                    296        True      
ReLU                                                           
____________________________________________________________________________
                     64 x 16 x 4 x 4     
Conv2d                                    1168       True      
ReLU                                                           
____________________________________________________________________________
                     64 x 32 x 2 x 2     
Conv2d                                    4640       True      
ReLU                                                           
____________________________________________________________________________
                     64 x 2 x 1 x 1      
Conv2d                                    578        True      
____________________________________________________________________________
                     []                  
Flatten                                                        
____________________________________________________________________________

Total params: 6,722
Total trainable params: 6,722
Total non-trainable params: 0

Optimizer used: &lt;function Adam at 0x7ff1128d58c0&gt;
Loss function: &lt;function cross_entropy at 0x7ff13e6b55f0&gt;

Callbacks:
  - TrainEvalCallback
  - Recorder
  - ProgressCallback</code></pre>
</div>
</div>
<p>Note that the final conv layer output is 64x2x1x1 and flatten removes these unit axes, which is basically the <strong>squeeze</strong> function but as a network layer.</p>
<p>Let’s try training this model.</p>
<div class="cell" data-outputid="782d48f2-2eea-4657-973c-1bc29bdf7644" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">2</span>, <span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.080185</td>
      <td>0.035895</td>
      <td>0.988714</td>
      <td>00:13</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.024726</td>
      <td>0.029886</td>
      <td>0.990186</td>
      <td>00:13</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</section>
<section id="convolutional-arithmetic" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="convolutional-arithmetic"><span class="header-section-number">4</span> Convolutional arithmetic</h2>
<p>So we can see in this example the input size is 64x1x28x28, and these axes are batch, channel, height, width. This is often represented in Pytorch as NCHW (where N is batch size).</p>
<p><strong>When we use a stride-2 convolution, we often increase the number of features because we’re decreasing the number of activations in the activation map by a factor of 4; we don’t want to decrease the capacity of a layer by too much at a time.</strong></p>
<p>We also have one bias weight for each channel. So in this example, our stide 2 convolutions halve the grid size - but we also double the number of filters at each layer. This means we get the same amount of computation done.</p>
<p>A <em>receptive field</em> is the area of an image involved in the calculation of a layer. As we go deeper through the layers, a larger area of the original image layers progressively contribute to smaller areas of later layers that have smaller grid sizes.</p>
</section>
<section id="model-2---a-convolutional-neural-network-to-predict-10-digits" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="model-2---a-convolutional-neural-network-to-predict-10-digits"><span class="header-section-number">5</span> Model 2 - A Convolutional Neural Network to predict 10 digits</h2>
<p>As our previous model did well on predicting 2 digits, we will now try to build a model that predicts all 10 digits, using the full MNIST dataset.</p>
<div class="cell" data-outputid="ffcddcde-7b18-44ad-9f31-fbd8f232706a" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.MNIST)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
</div>
<p>The images are in 2 folders training and testing, so we can use the GrandparentSplitter but need to tell it explictly as by default it expects train and valid.</p>
<p>We will define a function to make it easy to define different dataloaders with different batch sizes.</p>
<div class="cell" data-outputid="cd55f26f-a9b8-4e9a-f917-f7c79a061155" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dls(bs<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> DataBlock(</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        blocks<span class="op">=</span>(ImageBlock(cls<span class="op">=</span>PILImageBW), CategoryBlock), </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        get_items<span class="op">=</span>get_image_files, </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        splitter<span class="op">=</span>GrandparentSplitter(<span class="st">'training'</span>,<span class="st">'testing'</span>),</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        get_y<span class="op">=</span>parent_label,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        batch_tfms<span class="op">=</span>Normalize()</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    ).dataloaders(path, bs<span class="op">=</span>bs)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls()</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>dls.show_batch(max_n<span class="op">=</span><span class="dv">9</span>, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2021-06-11-convolutional-image-model-from-scratch_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So we will try and improve our previous model with one with more activations, and we will probably need more filters to handle more numbers, so we could double them for each layer.</p>
<p>But there is a potential problem, if we add more filters we are producing an image of a similar size to our input, which does not force the network to learn useful features. If we use a larger kernal in the first layer, such as 5x5 instead of 3x3, this will force the network to find more useful features from this more limited information.</p>
<div class="cell" data-outputid="7aedb8a2-d7e9-455c-bbf6-e44c725b0b50" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.callback.hook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_cnn():</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sequential(</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">1</span> ,<span class="dv">8</span>, ks<span class="op">=</span><span class="dv">5</span>),        <span class="co">#14x14</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">8</span> ,<span class="dv">16</span>),             <span class="co">#7x7</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">16</span>,<span class="dv">32</span>),             <span class="co">#4x4</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">32</span>,<span class="dv">64</span>),             <span class="co">#2x2</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">64</span>,<span class="dv">10</span>, act<span class="op">=</span><span class="va">False</span>),  <span class="co">#1x1</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        Flatten(),</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(epochs<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    learn <span class="op">=</span> Learner(dls, simple_cnn(), loss_func<span class="op">=</span>F.cross_entropy,</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>                    metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ActivationStats(with_hist<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    learn.fit(epochs, <span class="fl">0.06</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> learn</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> fit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this
  warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")</code></pre>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.694318</td>
      <td>0.672285</td>
      <td>0.793600</td>
      <td>01:06</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>So we trained for one epoch, but that did’nt do well. We can use callbacks to investigate why right after training. The <em>ActivationStats</em> callback use some useful plots, for example we can plot the mean and std dev of the activations of a layer you give the index for, along with the percentage of activations which are zero.</p>
<div class="cell" data-outputid="8b17bf49-8ce8-4044-80a1-32b2d3312aba" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>learn.activation_stats.plot_layer_stats(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2021-06-11-convolutional-image-model-from-scratch_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So ideally we want our model to have a smooth mean and std dev during training. Activations near zero are not helpful, as it gives gradient descent little to work with. If we have little to zero activations in earlier layers, this gets even worse in later layers.</p>
<div class="cell" data-outputid="50ebcb48-7083-420c-ef57-fa29af33ca6c" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>learn.activation_stats.plot_layer_stats(<span class="op">-</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2021-06-11-convolutional-image-model-from-scratch_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We could try to make training more stable by increasing the batch size with better info for gradients, but less often updated due to larger batch sizes.</p>
<p>Lets try larger batch size.</p>
<div class="cell" data-outputid="8ea8f22b-137f-4662-94e4-cbccd51f8fab" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(<span class="dv">512</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> fit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this
  warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")</code></pre>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>2.327641</td>
      <td>2.302224</td>
      <td>0.113500</td>
      <td>00:56</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell" data-outputid="7ee6b508-5e6e-4b27-be5c-f2865e1c496f" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>learn.activation_stats.plot_layer_stats(<span class="op">-</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2021-06-11-convolutional-image-model-from-scratch_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This has’nt helped much with the activations, lets see what else we can do.</p>
</section>
<section id="one-cycle-training" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="one-cycle-training"><span class="header-section-number">6</span> One cycle training</h2>
<p>We have been training our model at the same learning rate, but it may be more helpful to vary the learning rate at different points - for example to have it higher when we are far in the loss landscape from the minimum, and have it lower when we are in the minimum area. In one cycle training, we start at a lower learning rate, building up gradually to a maximum, then gradually reducing the learning rate again.</p>
<div class="cell" data-outputid="e0202d5c-27b6-4015-92fd-6e86e69c8c50" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(epochs<span class="op">=</span><span class="dv">1</span>, lr<span class="op">=</span><span class="fl">0.06</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    learn <span class="op">=</span> Learner(dls, simple_cnn(), loss_func<span class="op">=</span>F.cross_entropy,</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                    metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ActivationStats(with_hist<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    learn.fit_one_cycle(epochs, lr)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> learn</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> fit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this
  warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")</code></pre>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.197716</td>
      <td>0.083136</td>
      <td>0.975800</td>
      <td>00:55</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell" data-outputid="5345b882-0c07-4d8b-eed9-551ca4ea2986" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>learn.recorder.plot_sched()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2021-06-11-convolutional-image-model-from-scratch_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Once cycle training also involves varying the momentum with the opposite pattern to the learning rate.</p>
<p>Looking at our layer stats again, we can see there is some improvement but still a large percentage of zero weights.</p>
<p>We can use the <strong>colour_dim</strong> module to show how the activations of a layer changes through the training accross time.</p>
<div class="cell" data-outputid="0b7bc328-c4eb-4ee8-ceb6-df3306f4ac43" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>learn.activation_stats.color_dim(<span class="op">-</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2021-06-11-convolutional-image-model-from-scratch_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Here for example we can see on the far left mostly white is with most of the activations at zero, then as time passes from left to right, we can see an expontential build up of activations, which then collapses into zero activations (white bands). Eventually the bands go and you get more consistant activations for most of the model.</p>
<p>Ideally if we can avoid this crashing of activations this can result in better training.</p>
</section>
<section id="batch-normalisation" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="batch-normalisation"><span class="header-section-number">7</span> Batch Normalisation</h2>
<p>Batch norm is a method we can use to stablise training to try and avoid extreme rises and crashes in activations. Essentially batch norm normalises the activations of each layer using the mean and std dev of the activations. Batch norm also uses extra 2 learnable parameters per layer <strong>gamma</strong> and <strong>beta</strong> which are addative and multiplicative factors that can then scale the activations of a layer.</p>
<p>Batchnorm layer output = (Normalised Activations * gamma) + beta</p>
<p>So each layer has its own normalisation and scaling with batchnorm layers. The normalisation is different during training vs validation: in training we use the mean and std dev of a batch to normalise, in validation we use the running mean and std dev calculated during training.</p>
<p>Lets add batchnorm to our layer definition.</p>
<div class="cell" data-outputid="02333b1a-2481-4d96-99aa-a4cb7b4ad01d" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, act<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [nn.Conv2d(ni, nf, stride<span class="op">=</span><span class="dv">2</span>, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: layers.append(nn.ReLU())</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    layers.append(nn.BatchNorm2d(nf))</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> fit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this
  warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")</code></pre>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.129701</td>
      <td>0.057382</td>
      <td>0.985700</td>
      <td>00:58</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell" data-outputid="f68095ed-06b7-47c0-9983-8779fffdb86f" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>learn.activation_stats.color_dim(<span class="op">-</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2021-06-11-convolutional-image-model-from-scratch_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This has given us more gradual training without the crashes in activations. Now we are using batchnorm in our layers it should make it easier to learn at a higher learning rate.</p>
<div class="cell" data-outputid="91e0f4fa-f43c-4aac-803a-7a22b8aef158" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> fit(<span class="dv">5</span>, lr<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this
  warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")</code></pre>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.180499</td>
      <td>0.142405</td>
      <td>0.957800</td>
      <td>00:58</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.078111</td>
      <td>0.064472</td>
      <td>0.979600</td>
      <td>00:58</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.051010</td>
      <td>0.052857</td>
      <td>0.983600</td>
      <td>00:58</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.031543</td>
      <td>0.030566</td>
      <td>0.990000</td>
      <td>00:58</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.015607</td>
      <td>0.024703</td>
      <td>0.991900</td>
      <td>00:58</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</section>
<section id="conclusion" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">8</span> Conclusion</h2>
<p>In this article we look at how we can build a convolutional neural network using Pytorch, as well as useful extra techniques to help with model training such as one-cycle training and batch normalisation.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>